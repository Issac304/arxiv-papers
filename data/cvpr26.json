[
  {
    "id": "2602.23361v1",
    "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale",
    "authors": [
      "Sven Elflein",
      "Ruilong Li",
      "Sérgio Agostinho",
      "Zan Gojcic",
      "Laura Leal-Taixé",
      "Qunjie Zhou",
      "Aljosa Osep"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23361v1",
    "pdf": "http://arxiv.org/pdf/2602.23361v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.",
    "summary_cn": "研究问题：解决离线前馈方法在计算和内存需求上随输入图像数量呈二次增长的问题。方法：基于关键洞察，即瓶颈源于输入图像数量的变化。核心创新点：提出可扩展的3D重建模型。实验结果：模型有效降低了计算和内存需求。"
  },
  {
    "id": "2602.23359v1",
    "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation",
    "authors": [
      "Vaibhav Agrawal",
      "Rishubh Parihar",
      "Pradhaan Bhat",
      "Ravi Kiran Sarvadevabhatla",
      "R. Venkatesh Babu"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23359v1",
    "pdf": "http://arxiv.org/pdf/2602.23359v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.",
    "summary_cn": "研究问题：识别遮挡推理作为3D布局条件生成的基础但被忽视的方面。方法：强调生成部分遮挡物体时深度一致几何和比例的重要性。核心创新点：提出遮挡推理方法。实验结果：生成具有深度一致几何和比例的物体。"
  },
  {
    "id": "2602.23191v1",
    "title": "Uni-Animator: Towards Unified Visual Colorization",
    "authors": [
      "Xinyuan Chen",
      "Yao Xu",
      "Shaowen Wang",
      "Pengjie Song",
      "Bowen Deng"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23191v1",
    "pdf": "http://arxiv.org/pdf/2602.23191v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.",
    "summary_cn": "研究问题：统一图像和视频草图着色。方法：提出基于扩散Transformer（DiT）的Uni-Animator框架。核心创新点：实现图像和视频任务的统一着色。实验结果：提高了着色精度和保真度。"
  },
  {
    "id": "2602.23153v1",
    "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
    "authors": [
      "Guofeng Mei",
      "Wei Lin",
      "Luigi Riz",
      "Yujiao Wu",
      "Yiming Wang",
      "Fabio Poiesi"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23153v1",
    "pdf": "http://arxiv.org/pdf/2602.23153v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.",
    "summary_cn": "研究问题：将2D大型多模态模型（LMMs）扩展到3D数据。方法：提出基于自编码器提取几何特征的方法。核心创新点：解决3D数据无序性的挑战。实验结果：提高了3D数据的处理效率和可扩展性。"
  },
  {
    "id": "2602.23120v1",
    "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement",
    "authors": [
      "Arian Sabaghi",
      "José Oramas"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23120v1",
    "pdf": "http://arxiv.org/pdf/2602.23120v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.",
    "summary_cn": "研究问题：降低弱监督目标定位（WSOL）的训练成本。方法：提出一种新的WSOL方法，减少多阶段管道和大型骨干网络的依赖。核心创新点：降低训练成本。实验结果：提高了WSOL的定位精度。"
  },
  {
    "id": "2602.23058v1",
    "title": "GeoWorld: Geometric World Models",
    "authors": [
      "Zeyu Zhang",
      "Danning Li",
      "Ian Reid",
      "Richard Hartley"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23058v1",
    "pdf": "http://arxiv.org/pdf/2602.23058v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "summary": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
    "summary_cn": "研究问题：解决基于能量预测的视觉规划中的挑战。方法：提出一种新的方法，在欧几里得空间之外学习潜在表示。核心创新点：改进了潜在表示的学习。实验结果：提高了视觉规划的准确性。"
  },
  {
    "id": "2602.23040v1",
    "title": "PackUV: Packed Gaussian UV Maps for 4D Volumetric Video",
    "authors": [
      "Aashish Rai",
      "Angela Xing",
      "Anushka Agarwal",
      "Xiaoyan Cong",
      "Zekun Li",
      "Tao Lu",
      "Aayush Prakash",
      "Srinath Sridhar"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23040v1",
    "pdf": "http://arxiv.org/pdf/2602.23040v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications. We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure. To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.",
    "summary_cn": "研究问题：解决体量视频的重建、存储和流式传输问题。方法：提出基于体素视频的改进方法。核心创新点：提高重建质量，解决长期序列、时间不一致性和大运动下的问题。实验结果：提高了体量视频的重建质量。"
  },
  {
    "id": "2602.23022v1",
    "title": "DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis",
    "authors": [
      "Xinglong Luo",
      "Ao Luo",
      "Zhengning Wang",
      "Yueqi Yang",
      "Chaoyu Feng",
      "Lei Lei",
      "Bing Zeng",
      "Shuaicheng Liu"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23022v1",
    "pdf": "http://arxiv.org/pdf/2602.23022v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.",
    "summary_cn": "研究问题：提高图像对齐的准确性。方法：提出一种基于深度学习的图像对齐方法。核心创新点：解决遮挡和光照变化等问题。实验结果：提高了图像对齐的视觉效果。"
  },
  {
    "id": "2602.23013v1",
    "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling",
    "authors": [
      "Camile Lendering",
      "Erkut Akdag",
      "Egor Bondarev"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23013v1",
    "pdf": "http://arxiv.org/pdf/2602.23013v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "summary": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.",
    "summary_cn": "研究问题：降低工业检测中视觉异常检测的样本数量。方法：提出一种基于基础模型特征的少样本方法。核心创新点：降低训练成本。实验结果：提高了异常检测的准确性。"
  },
  {
    "id": "2602.22949v1",
    "title": "OpenFS: Multi-Hand-Capable Fingerspelling Recognition with Implicit Signing-Hand Detection and Frame-Wise Letter-Conditioned Synthesis",
    "authors": [
      "Junuk Cha",
      "Jihyeon Kim",
      "Han-Mu Park"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22949v1",
    "pdf": "http://arxiv.org/pdf/2602.22949v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Fingerspelling is a component of sign languages in which words are spelled out letter by letter using specific hand poses. Automatic fingerspelling recognition plays a crucial role in bridging the communication gap between Deaf and hearing communities, yet it remains challenging due to the signing-hand ambiguity issue, the lack of appropriate training losses, and the out-of-vocabulary (OOV) problem. Prior fingerspelling recognition methods rely on explicit signing-hand detection, which often leads to recognition failures, and on a connectionist temporal classification (CTC) loss, which exhibits the peaky behavior problem. To address these issues, we develop OpenFS, an open-source approach for fingerspelling recognition and synthesis. We propose a multi-hand-capable fingerspelling recognizer that supports both single- and multi-hand inputs and performs implicit signing-hand detection by incorporating a dual-level positional encoding and a signing-hand focus (SF) loss. The SF loss encourages cross-attention to focus on the signing hand, enabling implicit signing-hand detection during recognition. Furthermore, without relying on the CTC loss, we introduce a monotonic alignment (MA) loss that enforces the output letter sequence to follow the temporal order of the input pose sequence through cross-attention regularization. In addition, we propose a frame-wise letter-conditioned generator that synthesizes realistic fingerspelling pose sequences for OOV words. This generator enables the construction of a new synthetic benchmark, called FSNeo. Through comprehensive experiments, we demonstrate that our approach achieves state-of-the-art performance in recognition and validate the effectiveness of the proposed recognizer and generator. Codes and data are available in: https://github.com/JunukCha/OpenFS.",
    "summary_cn": "研究问题：自动识别手语中的手指拼写。方法：提出一种基于深度学习的手指拼写识别方法。核心创新点：解决手语识别的挑战。实验结果：提高了手指拼写识别的准确性。"
  },
  {
    "id": "2602.22917v1",
    "title": "Towards Multimodal Domain Generalization with Few Labels",
    "authors": [
      "Hongzhao Li",
      "Hao Dong",
      "Hualei Wan",
      "Shupan Li",
      "Mingliang Xu",
      "Muhammad Haris Khan"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22917v1",
    "pdf": "http://arxiv.org/pdf/2602.22917v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.",
    "summary_cn": "研究问题：如何从多源数据中学习鲁棒的多模态模型，同时保持数据效率以降低标注成本。方法：提出半监督多模态域泛化（SSMDG）问题。创新点：学习跨域泛化的多模态模型。结果：实验表明，该方法在多个数据集上取得了良好的泛化性能。"
  },
  {
    "id": "2602.22862v1",
    "title": "GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion",
    "authors": [
      "Enda Xiang",
      "Haoxiang Ma",
      "Xinzhu Ma",
      "Zicheng Liu",
      "Di Huang"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22862v1",
    "pdf": "http://arxiv.org/pdf/2602.22862v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "summary": "This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.",
    "summary_cn": "研究问题：如何提高通过模仿学习学习的操作策略的抓取精度和泛化能力。方法：采用基于扩散的政策学习方法。创新点：针对抓取任务进行优化。结果：实验证明，该方法在抓取精度和泛化能力方面均有显著提升。"
  },
  {
    "id": "2602.22819v1",
    "title": "Face Time Traveller : Travel Through Ages Without Losing Identity",
    "authors": [
      "Purbayan Kar",
      "Ayush Ghadiya",
      "Vishal Chudasama",
      "Pankaj Wasnik",
      "C. V. Jawahar"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22819v1",
    "pdf": "http://arxiv.org/pdf/2602.22819v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.",
    "summary_cn": "研究问题：如何实现逼真的面部年龄变换，同时保留身份和视觉真实性。方法：提出一种新的方法，关注身份和视觉真实性的平衡。创新点：考虑环境因素和遗传因素。结果：实验结果表明，该方法在娱乐、法医和数字存档等领域具有潜在应用价值。"
  },
  {
    "id": "2602.22779v1",
    "title": "TrajTok: Learning Trajectory Tokens enables better Video Understanding",
    "authors": [
      "Chenhao Zheng",
      "Jieyu Zhang",
      "Jianing Zhang",
      "Weikai Huang",
      "Ashutosh Kumar",
      "Quan Kong",
      "Oncel Tuzel",
      "Chun-Liang Li",
      "Ranjay Krishna"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22779v1",
    "pdf": "http://arxiv.org/pdf/2602.22779v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.",
    "summary_cn": "研究问题：如何提高视频模型的效率和解耦视频时长与标记数量。方法：提出基于轨迹的标记器。创新点：减少冗余标记。结果：实验表明，该方法在视频效率和解耦方面取得了显著效果。"
  },
  {
    "id": "2602.22727v1",
    "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models",
    "authors": [
      "Yangguang Lin",
      "Quan Fang",
      "Yufei Li",
      "Jiachen Sun",
      "Junyu Gao",
      "Jitao Sang"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22727v1",
    "pdf": "http://arxiv.org/pdf/2602.22727v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.",
    "summary_cn": "研究问题：如何解决大型视觉语言模型（LVLMs）中的对象幻觉问题。方法：提出一种平衡效率和准确性的方法。创新点：采用静态编辑技术。结果：实验证明，该方法在效率和准确性方面均有所提升。"
  },
  {
    "id": "2602.22716v1",
    "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
    "authors": [
      "Guanting Ye",
      "Qiyan Zhao",
      "Wenhao Yu",
      "Liangyu Yuan",
      "Mingkai Li",
      "Xiaofeng Zhang",
      "Jianmin Ji",
      "Yanyong Zhang",
      "Qing Jiang",
      "Ka-Veng Yuen"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22716v1",
    "pdf": "http://arxiv.org/pdf/2602.22716v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.",
    "summary_cn": "研究问题：如何改进3D LVLMs中的位置依赖建模机制。方法：提出一种新的方法，优化RoPE。创新点：改进3D多模态理解。结果：实验表明，该方法在3D多模态理解方面取得了更好的效果。"
  },
  {
    "id": "2602.22654v1",
    "title": "Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache",
    "authors": [
      "Bowen Cui",
      "Yuanbin Wang",
      "Huajiang Xu",
      "Biaolong Chen",
      "Aixi Zhang",
      "Hao Jiang",
      "Zhengzheng Jin",
      "Xu Liu",
      "Pipei Huang"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22654v1",
    "pdf": "http://arxiv.org/pdf/2602.22654v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.",
    "summary_cn": "研究问题：如何降低扩散模型在图像和视频生成中的计算开销。方法：采用基于缓存的加速策略。创新点：无需训练。结果：实验证明，该方法在降低计算开销方面具有显著效果。"
  },
  {
    "id": "2602.22639v1",
    "title": "QuadSync: Quadrifocal Tensor Synchronization via Tucker Decomposition",
    "authors": [
      "Daniel Miao",
      "Gilad Lerman",
      "Joe Kileel"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22639v1",
    "pdf": "http://arxiv.org/pdf/2602.22639v1",
    "categories": [
      "cs.CV",
      "math.NA",
      "math.OC"
    ],
    "summary": "In structure from motion, quadrifocal tensors capture more information than their pairwise counterparts (essential matrices), yet they have often been thought of as impractical and only of theoretical interest. In this work, we challenge such beliefs by providing a new framework to recover $n$ cameras from the corresponding collection of quadrifocal tensors. We form the block quadrifocal tensor and show that it admits a Tucker decomposition whose factor matrices are the stacked camera matrices, and which thus has a multilinear rank of (4,~4,~4,~4) independent of $n$. We develop the first synchronization algorithm for quadrifocal tensors, using Tucker decomposition, alternating direction method of multipliers, and iteratively reweighted least squares. We further establish relationships between the block quadrifocal, trifocal, and bifocal tensors, and introduce an algorithm that jointly synchronizes these three entities. Numerical experiments demonstrate the effectiveness of our methods on modern datasets, indicating the potential and importance of using higher-order information in synchronization.",
    "summary_cn": "研究问题：如何提高结构从运动中的四焦点张量恢复效率。方法：提出一种新的框架。创新点：挑战传统观点。结果：实验表明，该方法在恢复四焦点张量方面具有更高的效率。"
  },
  {
    "id": "2602.22625v1",
    "title": "DiffBMP: Differentiable Rendering with Bitmap Primitives",
    "authors": [
      "Seongmin Hong",
      "Junghun James Kim",
      "Daehyeop Kim",
      "Insoo Chung",
      "Se Young Chun"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22625v1",
    "pdf": "http://arxiv.org/pdf/2602.22625v1",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "summary": "We introduce DiffBMP, a scalable and efficient differentiable rendering engine for a collection of bitmap images. Our work addresses a limitation that traditional differentiable renderers are constrained to vector graphics, given that most images in the world are bitmaps. Our core contribution is a highly parallelized rendering pipeline, featuring a custom CUDA implementation for calculating gradients. This system can, for example, optimize the position, rotation, scale, color, and opacity of thousands of bitmap primitives all in under 1 min using a consumer GPU. We employ and validate several techniques to facilitate the optimization: soft rasterization via Gaussian blur, structure-aware initialization, noisy canvas, and specialized losses/heuristics for videos or spatially constrained images. We demonstrate DiffBMP is not just an isolated tool, but a practical one designed to integrate into creative workflows. It supports exporting compositions to a native, layered file format, and the entire framework is publicly accessible via an easy-to-hack Python package.",
    "summary_cn": "研究问题：如何为位图图像提供可扩展和高效的微分渲染引擎。方法：提出DiffBMP。创新点：支持位图图像。结果：实验证明，DiffBMP在渲染效率和可扩展性方面具有优势。"
  },
  {
    "id": "2602.22620v1",
    "title": "Coded-E2LF: Coded Aperture Light Field Imaging from Events",
    "authors": [
      "Tomoya Tsuchida",
      "Keita Takahashi",
      "Chihiro Tsutake",
      "Toshiaki Fujii",
      "Hajime Nagahara"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22620v1",
    "pdf": "http://arxiv.org/pdf/2602.22620v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We propose Coded-E2LF (coded event to light field), a computational imaging method for acquiring a 4-D light field using a coded aperture and a stationary event-only camera. In a previous work, an imaging system similar to ours was adopted, but both events and intensity images were captured and used for light field reconstruction. In contrast, our method is purely event-based, which relaxes restrictions for hardware implementation. We also introduce several advancements from the previous work that enable us to theoretically support and practically improve light field reconstruction from events alone. In particular, we clarify the key role of a black pattern in aperture coding patterns. We finally implemented our method on real imaging hardware to demonstrate its effectiveness in capturing real 3-D scenes. To the best of our knowledge, we are the first to demonstrate that a 4-D light field with pixel-level accuracy can be reconstructed from events alone. Our software and supplementary video are available from our project website.",
    "summary_cn": "研究问题：如何使用编码事件和静止事件相机获取4D光场。方法：提出Coded-E2LF。创新点：采用编码孔径和事件相机。结果：实验表明，Coded-E2LF在获取4D光场方面具有可行性。"
  },
  {
    "id": "2602.22594v1",
    "title": "Causal Motion Diffusion Models for Autoregressive Motion Generation",
    "authors": [
      "Qing Yu",
      "Akihisa Watanabe",
      "Kent Fujiwara"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22594v1",
    "pdf": "http://arxiv.org/pdf/2602.22594v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.",
    "summary_cn": "研究问题：现有运动扩散模型在实时性和时间因果性方面存在限制。方法：提出一种新的自回归模型，结合时间因果性和实时性。创新点：自回归模型结合时间因果性和实时性。结果：模型在保持实时性的同时提高了运动合成的真实感。"
  },
  {
    "id": "2602.23141v1",
    "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors",
    "authors": [
      "Tao Liu",
      "Gang Wan",
      "Kan Ren",
      "Shibo Wen"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.23141v1",
    "pdf": "http://arxiv.org/pdf/2602.23141v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.",
    "summary_cn": "研究问题：现有视频稳定方法依赖深度学习，需要大量数据。方法：提出一种无监督框架，采用经典稳定流程和线程缓冲机制。创新点：无监督框架和线程缓冲机制。结果：在线视频稳定效果显著。"
  },
  {
    "id": "2602.22932v1",
    "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding",
    "authors": [
      "Wenhui Tan",
      "Xiaoyi Yu",
      "Jiaze Li",
      "Yijing Chen",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Ruihua Song",
      "Jian Luan"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22932v1",
    "pdf": "http://arxiv.org/pdf/2602.22932v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\\% accuracy gain upon the base MLLM, and 1.1\\% higher accuracy than strongest baseline method.",
    "summary_cn": "研究问题：长视频理解对多模态大型语言模型（MLLMs）是一个挑战。方法：提出MSJoE框架，联合进化MLLM和轻量级关键帧采样器。创新点：联合进化MLLM和关键帧采样器。结果：有效理解长视频。"
  },
  {
    "id": "2602.22667v1",
    "title": "Monocular Open Vocabulary Occupancy Prediction for Indoor Scenes",
    "authors": [
      "Changqing Zhou",
      "Yueru Luo",
      "Han Zhang",
      "Zeyu Jiang",
      "Changhao Chen"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22667v1",
    "pdf": "http://arxiv.org/pdf/2602.22667v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Open-vocabulary 3D occupancy is vital for embodied agents, which need to understand complex indoor environments where semantic categories are abundant and evolve beyond fixed taxonomies. While recent work has explored open-vocabulary occupancy in outdoor driving scenarios, such methods transfer poorly indoors, where geometry is denser, layouts are more intricate, and semantics are far more fine-grained. To address these challenges, we adopt a geometry-only supervision paradigm that uses only binary occupancy labels (occupied vs free). Our framework builds upon 3D Language-Embedded Gaussians, which serve as a unified intermediate representation coupling fine-grained 3D geometry with a language-aligned semantic embedding. On the geometry side, we find that existing Gaussian-to-Occupancy operators fail to converge under such weak supervision, and we introduce an opacity-aware, Poisson-based approach that stabilizes volumetric aggregation. On the semantic side, direct alignment between rendered features and open-vocabulary segmentation features suffers from feature mixing; we therefore propose a Progressive Temperature Decay schedule that gradually sharpens opacities during splatting, strengthening Gaussian-language alignment. On Occ-ScanNet, our framework achieves 59.50 IoU and 21.05 mIoU in the open-vocabulary setting, surpassing all existing occupancy methods in IoU and outperforming prior open-vocabulary approaches by a large margin in mIoU. Code will be released at https://github.com/JuIvyy/LegoOcc.",
    "summary_cn": "研究问题：开放词汇3D占用对具身智能体至关重要。方法：探索开放词汇占用在室内环境中的应用。创新点：开放词汇占用在室内环境中的应用。结果：提高智能体对复杂室内环境的理解。"
  },
  {
    "id": "2602.22601v1",
    "title": "$φ$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models",
    "authors": [
      "Thanh-Dat Truong",
      "Huu-Thien Tran",
      "Jackson Cothren",
      "Bhiksha Raj",
      "Khoa Luu"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22601v1",
    "pdf": "http://arxiv.org/pdf/2602.22601v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "summary": "Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $φ$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $φ$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $φ$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $φ$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.",
    "summary_cn": "研究问题：大型多模态模型（LMMs）的持续学习公平性是一个挑战。方法：提出公平的持续学习方法，解决数据不平衡问题。创新点：公平的持续学习方法。结果：提高LMMs在任务中的性能。"
  },
  {
    "id": "2602.22695v1",
    "title": "GFRRN: Explore the Gaps in Single Image Reflection Removal",
    "authors": [
      "Yu Chen",
      "Zewei He",
      "Xingyu Liu",
      "Zixuan Chen",
      "Zheming Lu"
    ],
    "published": "2026-02-26",
    "link": "http://arxiv.org/abs/2602.22695v1",
    "pdf": "http://arxiv.org/pdf/2602.22695v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.",
    "summary_cn": "研究问题：现有双流方法在单图像反射去除（SIRR）中存在局限性。方法：提出一种新的方法，解决语义理解差距和特征交互问题。创新点：解决语义理解差距和特征交互问题。结果：提高SIRR性能。"
  },
  {
    "id": "2602.22419v1",
    "title": "CLIP Is Shortsighted: Paying Attention Beyond the First Sentence",
    "authors": [
      "Marc-Antoine Lavoie",
      "Anas Mahmoud",
      "Aldo Zaimi",
      "Arsene Fansi Tchango",
      "Steven L. Waslander"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22419v1",
    "pdf": "http://arxiv.org/pdf/2602.22419v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.",
    "summary_cn": "研究问题：CLIP模型的预训练存在局限性。方法：提出改进的CLIP模型，解决预训练问题。创新点：改进的CLIP模型。结果：提高CLIP模型在多模态任务中的性能。"
  },
  {
    "id": "2602.22394v1",
    "title": "Vision Transformers Need More Than Registers",
    "authors": [
      "Cheng Shi",
      "Yizhou Yu",
      "Sibei Yang"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22394v1",
    "pdf": "http://arxiv.org/pdf/2602.22394v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.",
    "summary_cn": "研究问题：Vision Transformers（ViTs）存在伪影问题。方法：通过系统分析ViTs的伪影，提出解决方案。创新点：解决ViTs的伪影问题。结果：提高ViTs在下游任务中的性能。"
  },
  {
    "id": "2602.22376v1",
    "title": "AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction",
    "authors": [
      "Hanyang Liu",
      "Rongjun Qin"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22376v1",
    "pdf": "http://arxiv.org/pdf/2602.22376v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.",
    "summary_cn": "研究问题：4D场景重建在空中条件下存在局限性。方法：提出一种新的方法，解决空中条件下的重建问题。创新点：空中条件下的4D场景重建方法。结果：提高动态建模的准确性。"
  },
  {
    "id": "2602.22212v1",
    "title": "Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences",
    "authors": [
      "Julian Kaltheuner",
      "Hannah Dröge",
      "Markus Plack",
      "Patrick Stotko",
      "Reinhard Klein"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22212v1",
    "pdf": "http://arxiv.org/pdf/2602.22212v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.",
    "summary_cn": "研究问题：从无结构点云数据中重建动态3D物体表面存在挑战。方法：提出一种新的方法，解决长序列重建问题。创新点：长序列动态3D物体表面重建方法。结果：提高重建的准确性和一致性。"
  },
  {
    "id": "2602.22142v1",
    "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
    "authors": [
      "Yulin Zhang",
      "Cheng Shi",
      "Sibei Yang"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22142v1",
    "pdf": "http://arxiv.org/pdf/2602.22142v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/",
    "summary_cn": "研究问题：多模态大型语言模型在流媒体设置中的适用性问题；方法：提出了一种新的诊断方法；创新点：解决了流媒体设置中的适用性问题；结果：显著提高了视觉理解和推理能力。"
  },
  {
    "id": "2602.22140v1",
    "title": "Lumosaic: Hyperspectral Video via Active Illumination and Coded-Exposure Pixels",
    "authors": [
      "Dhruv Verma",
      "Andrew Qiu",
      "Roberto Rangel",
      "Ayandev Barman",
      "Hao Yang",
      "Chenjia Hu",
      "Fengqi Zhang",
      "Roman Genov",
      "David B. Lindell",
      "Kiriakos N. Kutulakos",
      "Alex Mariakakis"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22140v1",
    "pdf": "http://arxiv.org/pdf/2602.22140v1",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "summary": "We present Lumosaic, a compact active hyperspectral video system designed for real-time capture of dynamic scenes. Our approach combines a narrowband LED array with a coded-exposure-pixel (CEP) camera capable of high-speed, per-pixel exposure control, enabling joint encoding of scene information across space, time, and wavelength within each video frame. Unlike passive snapshot systems that divide light across multiple spectral channels simultaneously and assume no motion during a frame's exposure, Lumosaic actively synchronizes illumination and pixel-wise exposure, improving photon utilization and preserving spectral fidelity under motion. A learning-based reconstruction pipeline then recovers 31-channel hyperspectral (400-700 nm) video at 30 fps and VGA resolution, producing temporally coherent and spectrally accurate reconstructions. Experiments on synthetic and real data demonstrate that Lumosaic significantly improves reconstruction fidelity and temporal stability over existing snapshot hyperspectral imaging systems, enabling robust hyperspectral video across diverse materials and motion conditions.",
    "summary_cn": "研究问题：实时捕捉动态场景的紧凑型高光谱视频系统；方法：结合窄带LED阵列和CEP相机；创新点：实现了场景信息的联合编码；结果：提高了动态场景的实时捕捉能力。"
  },
  {
    "id": "2602.22091v1",
    "title": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos",
    "authors": [
      "Matthew Strong",
      "Wei-Jer Chang",
      "Quentin Herau",
      "Jiezhi Yang",
      "Yihan Hu",
      "Chensheng Peng",
      "Wei Zhan"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22091v1",
    "pdf": "http://arxiv.org/pdf/2602.22091v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving.",
    "summary_cn": "研究问题：自动驾驶中语义结构和3D几何学习表示的困难；方法：利用大型前馈空间模型；创新点：结合语义结构和3D几何信息；结果：提高了自动驾驶视频数据的理解能力。"
  },
  {
    "id": "2602.22059v1",
    "title": "NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training",
    "authors": [
      "Dengdi Sun",
      "Xiaoya Zhou",
      "Xiao Wang",
      "Hao Si",
      "Wanli Lyu",
      "Jin Tang",
      "Bin Luo"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22059v1",
    "pdf": "http://arxiv.org/pdf/2602.22059v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on neural operators. To address these challenges, we propose a large-scale PDE pre-trained neural operator based on a nested Mixture-of-Experts (MoE) framework. In particular, the image-level MoE is designed to capture global dependencies, while the token-level Sub-MoE focuses on local dependencies. Our model can selectively activate the most suitable expert networks for a given input, thereby enhancing generalization and transferability. We conduct large-scale pre-training on twelve PDE datasets from diverse sources and successfully transfer the model to downstream tasks. Extensive experiments demonstrate the effectiveness of our approach.",
    "summary_cn": "研究问题：神经算子解决PDEs的效率问题；方法：提出了一种新的神经算子；创新点：提高了计算效率；结果：解决了传统数值方法的局限性。"
  },
  {
    "id": "2602.21963v1",
    "title": "Global-Aware Edge Prioritization for Pose Graph Initialization",
    "authors": [
      "Tong Wei",
      "Giorgos Tolias",
      "Jiri Matas",
      "Daniel Barath"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21963v1",
    "pdf": "http://arxiv.org/pdf/2602.21963v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The pose graph is a core component of Structure-from-Motion (SfM), where images act as nodes and edges encode relative poses. Since geometric verification is expensive, SfM pipelines restrict the pose graph to a sparse set of candidate edges, making initialization critical. Existing methods rely on image retrieval to connect each image to its $k$ nearest neighbors, treating pairs independently and ignoring global consistency. We address this limitation through the concept of edge prioritization, ranking candidate edges by their utility for SfM. Our approach has three components: (1) a GNN trained with SfM-derived supervision to predict globally consistent edge reliability; (2) multi-minimal-spanning-tree-based pose graph construction guided by these ranks; and (3) connectivity-aware score modulation that reinforces weak regions and reduces graph diameter. This globally informed initialization yields more reliable and compact pose graphs, improving reconstruction accuracy in sparse and high-speed settings and outperforming SOTA retrieval methods on ambiguous scenes. The ode and trained models are available at https://github.com/weitong8591/global_edge_prior.",
    "summary_cn": "研究问题：SfM中姿态图初始化问题；方法：提出了一种新的初始化方法；创新点：提高了姿态图的准确性；结果：提高了SfM的初始化效果。"
  },
  {
    "id": "2602.21929v1",
    "title": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context",
    "authors": [
      "JiaKui Hu",
      "Jialun Liu",
      "Liying Yang",
      "Xinliang Zhang",
      "Kaiwen Li",
      "Shuang Zeng",
      "Yuanwei Li",
      "Haibin Huang",
      "Chi Zhang",
      "Yanye Lu"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21929v1",
    "pdf": "http://arxiv.org/pdf/2602.21929v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context\". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.",
    "summary_cn": "研究问题：场景一致的视频生成问题；方法：提出了一种新的视频生成模型；创新点：提高了视频生成的一致性；结果：生成了更高质量的场景一致视频。"
  },
  {
    "id": "2602.21877v1",
    "title": "How to Take a Memorable Picture? Empowering Users with Actionable Feedback",
    "authors": [
      "Francesco Laiti",
      "Davide Talon",
      "Jacopo Staiano",
      "Elisa Ricci"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21877v1",
    "pdf": "http://arxiv.org/pdf/2602.21877v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Image memorability, i.e., how likely an image is to be remembered, has traditionally been studied in computer vision either as a passive prediction task, with models regressing a scalar score, or with generative methods altering the visual input to boost the image likelihood of being remembered. Yet, none of these paradigms supports users at capture time, when the crucial question is how to improve a photo memorability. We introduce the task of Memorability Feedback (MemFeed), where an automated model should provide actionable, human-interpretable guidance to users with the goal to enhance an image future recall. We also present MemCoach, the first approach designed to provide concrete suggestions in natural language for memorability improvement (e.g., \"emphasize facial expression,\" \"bring the subject forward\"). Our method, based on Multimodal Large Language Models (MLLMs), is training-free and employs a teacher-student steering strategy, aligning the model internal activations toward more memorable patterns learned from a teacher model progressing along least-to-most memorable samples. To enable systematic evaluation on this novel task, we further introduce MemBench, a new benchmark featuring sequence-aligned photoshoots with annotated memorability scores. Our experiments, considering multiple MLLMs, demonstrate the effectiveness of MemCoach, showing consistently improved performance over several zero-shot models. The results indicate that memorability can not only be predicted but also taught and instructed, shifting the focus from mere prediction to actionable feedback for human creators.",
    "summary_cn": "研究问题：图像记忆力的研究方法；方法：提出了一种新的研究方法；创新点：从被动预测和生成方法转向主动预测；结果：提高了图像记忆力的预测能力。"
  },
  {
    "id": "2602.21864v1",
    "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
    "authors": [
      "Yanbin Wei",
      "Jiangyue Yan",
      "Chun Kang",
      "Yang Chen",
      "Hua Liu",
      "James Kwok",
      "Yu Zhang"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21864v1",
    "pdf": "http://arxiv.org/pdf/2602.21864v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.GR"
    ],
    "summary": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.",
    "summary_cn": "研究问题：VLMs在零样本问答中的挑战；方法：提出了一种新的问答方法；创新点：结合了结构化图和问答；结果：提高了VLMs的问答能力。"
  },
  {
    "id": "2602.21779v1",
    "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
    "authors": [
      "Zheyuan Gu",
      "Qingsong Zhao",
      "Yusong Wang",
      "Zhaohong Huang",
      "Xinqi Li",
      "Cheng Yuan",
      "Jiaowei Shao",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21779v1",
    "pdf": "http://arxiv.org/pdf/2602.21779v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.",
    "summary_cn": "研究问题：深伪视频检测中的时间不一致性问题；方法：提出了一种新的检测方法；创新点：结合了空间和时间信息；结果：提高了深伪视频检测的准确性。"
  },
  {
    "id": "2602.21754v1",
    "title": "LiREC-Net: A Target-Free and Learning-Based Network for LiDAR, RGB, and Event Calibration",
    "authors": [
      "Aditya Ranjan Dash",
      "Ramy Battrawy",
      "René Schuster",
      "Didier Stricker"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21754v1",
    "pdf": "http://arxiv.org/pdf/2602.21754v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Advanced autonomous systems rely on multi-sensor fusion for safer and more robust perception. To enable effective fusion, calibrating directly from natural driving scenes (i.e., target-free) with high accuracy is crucial for precise multi-sensor alignment. Existing learning-based calibration methods are typically designed for only a single pair of sensor modalities (i.e., a bi-modal setup). Unlike these methods, we propose LiREC-Net, a target-free, learning-based calibration network that jointly calibrates multiple sensor modality pairs, including LiDAR, RGB, and event data, within a unified framework. To reduce redundant computation and improve efficiency, we introduce a shared LiDAR representation that leverages features from both its 3D nature and projected depth map, ensuring better consistency across modalities. Trained and evaluated on established datasets, such as KITTI and DSEC, our LiREC-Net achieves competitive performance to bi-modal models and sets a new strong baseline for the tri-modal use case.",
    "summary_cn": "研究问题：多传感器融合中的校准问题；方法：提出了一种新的校准方法；创新点：直接从自然驾驶场景中进行校准；结果：提高了多传感器融合的精度。"
  },
  {
    "id": "2602.21698v1",
    "title": "E-comIQ-ZH: A Human-Aligned Dataset and Benchmark for Fine-Grained Evaluation of E-commerce Posters with Chain-of-Thought",
    "authors": [
      "Meiqi Sun",
      "Mingyu Li",
      "Junxiong Zhu"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21698v1",
    "pdf": "http://arxiv.org/pdf/2602.21698v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Generative AI is widely used to create commercial posters. However, rapid advances in generation have outpaced automated quality assessment. Existing models emphasize generic esthetics or low level distortions and lack the functional criteria required for e-commerce design. It is especially challenging for Chinese content, where complex characters often produce subtle but critical textual artifacts that are overlooked by existing methods. To address this, we introduce E-comIQ-ZH, a framework for evaluating Chinese e-commerce posters. We build the first dataset E-comIQ-18k to feature multi dimensional scores and expert calibrated Chain of Thought (CoT) rationales. Using this dataset, we train E-comIQ-M, a specialized evaluation model that aligns with human expert judgment. Our framework enables E-comIQ-Bench, the first automated and scalable benchmark for the generation of Chinese e-commerce posters. Extensive experiments show our E-comIQ-M aligns more closely with expert standards and enables scalable automated assessment of e-commerce posters. All datasets, models, and evaluation tools will be released to support future research in this area.Code will be available at https://github.com/4mm7/E-comIQ-ZH.",
    "summary_cn": "研究问题：生成式AI在商业海报制作中的应用，但现有模型缺乏对电子商务设计所需的功能性评估。方法：提出一种新的质量评估模型。创新点：强调功能性评估。结果：提高了海报质量评估的准确性。"
  },
  {
    "id": "2602.21655v1",
    "title": "CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning",
    "authors": [
      "Zhijiang Tang",
      "Linhua Wang",
      "Jiaxin Qi",
      "Weihao Jiang",
      "Peng Hou",
      "Anxiang Zeng",
      "Jianqiang Huang"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21655v1",
    "pdf": "http://arxiv.org/pdf/2602.21655v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, which in turn limits caption models. We argue that caption quality should be assessed by two objective aspects: completeness (does the caption cover all salient visual facts?) and correctness (are the descriptions true with respect to the image?). To this end, we introduce CCCaption: a dual-reward reinforcement learning framework with a dedicated fine-tuning corpus that explicitly optimizes these properties to generate \\textbf{C}omplete and \\textbf{C}orrect \\textbf{Captions}. For completeness, we use diverse LVLMs to disentangle the image into a set of visual queries, and reward captions that answer more of these queries, with a dynamic query sampling strategy to improve training efficiency. For correctness, we penalize captions that contain hallucinations by validating the authenticity of sub-caption queries, which are derived from the caption decomposition. Our symmetric dual-reward optimization jointly maximizes completeness and correctness, guiding models toward captions that better satisfy these objective criteria. Extensive experiments across standard captioning benchmarks show consistent improvements, offering a principled path to training caption models beyond human-annotation imitation.",
    "summary_cn": "研究问题：图像字幕生成依赖人工标注，存在主观性和不完整性。方法：提出一种基于深度学习的自动标注方法。创新点：减少人工标注依赖。结果：提高了字幕生成的准确性。"
  },
  {
    "id": "2602.21499v1",
    "title": "Easy3E: Feed-Forward 3D Asset Editing via Rectified Voxel Flow",
    "authors": [
      "Shimin Hu",
      "Yuanyi Wei",
      "Fei Zha",
      "Yudong Guo",
      "Juyong Zhang"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21499v1",
    "pdf": "http://arxiv.org/pdf/2602.21499v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Existing 3D editing methods rely on computationally intensive scene-by-scene iterative optimization and suffer from multi-view inconsistency. We propose an effective and fully feedforward 3D editing framework based on the TRELLIS generative backbone, capable of modifying 3D models from a single editing view. Our framework addresses two key issues: adapting training-free 2D editing to structured 3D representations, and overcoming the bottleneck of appearance fidelity in compressed 3D features. To ensure geometric consistency, we introduce Voxel FlowEdit, an edit-driven flow in the sparse voxel latent space that achieves globally consistent 3D deformation in a single pass. To restore high-fidelity details, we develop a normal-guided single to multi-view generation module as an external appearance prior, successfully recovering high-frequency textures. Experiments demonstrate that our method enables fast, globally consistent, and high-fidelity 3D model editing.",
    "summary_cn": "研究问题：现有3D编辑方法计算量大，存在多视图不一致问题。方法：提出基于TRELLIS生成骨干的3D编辑框架。创新点：实现单次编辑修改3D模型。结果：提高了3D编辑的效率和一致性。"
  },
  {
    "id": "2602.22150v2",
    "title": "CoLoGen: Progressive Learning of Concept-Localization Duality for Unified Image Generation",
    "authors": [
      "YuXin Song",
      "Yu Lu",
      "Haoyuan Sun",
      "Huanjin Yao",
      "Fanglong Liu",
      "Yifan Sun",
      "Haocheng Feng",
      "Hang Zhou",
      "Jingdong Wang"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22150v2",
    "pdf": "http://arxiv.org/pdf/2602.22150v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Unified conditional image generation remains difficult because different tasks depend on fundamentally different internal representations. Some require conceptual understanding for semantic synthesis, while others rely on localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept-localization representational conflict. To address this issue, we propose CoLoGen, a unified diffusion framework that progressively learns and reconciles this concept-localization duality. CoLoGen uses a staged curriculum that first builds core conceptual and localization abilities, then adapts them to diverse visual conditions, and finally refines their synergy for complex instruction-driven tasks. Central to this process is the Progressive Representation Weaving (PRW) module, which dynamically routes features to specialized experts and stably integrates their outputs across stages. Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance, offering a principled representational perspective for unified image generation.",
    "summary_cn": "研究问题：统一条件图像生成困难，不同任务依赖不同内部表示。方法：提出一种统一框架，适应不同任务。创新点：处理异构任务。结果：提高了图像生成的质量。"
  },
  {
    "id": "2602.22013v1",
    "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
    "authors": [
      "I-Hsiang Chen",
      "Yu-Wei Liu",
      "Tse-Yu Wu",
      "Yu-Chien Chiang",
      "Jen-Chien Yang",
      "Wei-Ting Chen"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22013v1",
    "pdf": "http://arxiv.org/pdf/2602.22013v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Vision-based Retrieval-Augmented Generation (VisRAG) leverages vision-language models (VLMs) to jointly retrieve relevant visual documents and generate grounded answers based on multimodal evidence. However, existing VisRAG models degrade in performance when visual inputs suffer from distortions such as blur, noise, low light, or shadow, where semantic and degradation factors become entangled within pretrained visual encoders, leading to errors in both retrieval and generation stages. To address this limitation, we introduce RobustVisRAG, a causality-guided dual-path framework that improves VisRAG robustness while preserving efficiency and zero-shot generalization. RobustVisRAG uses a non-causal path to capture degradation signals through unidirectional attention and a causal path to learn purified semantics guided by these signals. Together with the proposed Non-Causal Distortion Modeling and Causal Semantic Alignment objectives, the framework enforces a clear separation between semantics and degradations, enabling stable retrieval and generation under challenging visual conditions. To evaluate robustness under realistic conditions, we introduce the Distortion-VisRAG dataset, a large-scale benchmark containing both synthetic and real-world degraded documents across seven domains, with 12 synthetic and 5 real distortion types that comprehensively reflect practical visual degradations. Experimental results show that RobustVisRAG improves retrieval, generation, and end-to-end performance by 7.35%, 6.35%, and 12.40%, respectively, on real-world degradations, while maintaining comparable accuracy on clean inputs.",
    "summary_cn": "研究问题：视觉检索增强生成（VisRAG）模型在视觉输入扭曲时性能下降。方法：提出一种鲁棒的VisRAG模型。创新点：提高模型对视觉扭曲的鲁棒性。结果：提高了VisRAG模型的性能。"
  },
  {
    "id": "2602.21952v1",
    "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
    "authors": [
      "Lingjun Zhang",
      "Yujian Yuan",
      "Changjie Wu",
      "Xinyuan Chang",
      "Xin Cai",
      "Shuang Zeng",
      "Linzhe Shi",
      "Sijin Wang",
      "Hang Zhang",
      "Mu Xu"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21952v1",
    "pdf": "http://arxiv.org/pdf/2602.21952v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evolution. To address these, we innovatively propose MindDriver, a progressive multimodal reasoning framework that enables VLM to imitate human-like progressive thinking for autonomous driving. MindDriver presents semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning. To achieve aligned reasoning processes in MindDriver, we develop a feedback-guided automatic data annotation pipeline to generate aligned multimodal reasoning training data. Furthermore, we develop a progressive reinforcement fine-tuning method to optimize the alignment through progressive high- level reward-based learning. MindDriver demonstrates superior performance in both nuScences open-loop and Bench2Drive closed-loop evaluation. Codes are available at https://github.com/hotdogcheesewhite/MindDriver.",
    "summary_cn": "研究问题：视觉语言模型（VLM）的推理策略CoT存在语义空间和轨迹空间差距。方法：提出一种新的CoT方法。创新点：缩小语义空间和轨迹空间差距。结果：提高了VLM的推理能力。"
  },
  {
    "id": "2602.22286v1",
    "title": "OmniZip: Learning a Unified and Lightweight Lossless Compressor for Multi-Modal Data",
    "authors": [
      "Yan Zhao",
      "Zhengxue Cheng",
      "Junxuan Zhang",
      "Dajiang Zhou",
      "Qunshan Gu",
      "Qi Wang",
      "Li Song"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.22286v1",
    "pdf": "http://arxiv.org/pdf/2602.22286v1",
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "summary": "Lossless compression is essential for efficient data storage and transmission. Although learning-based lossless compressors achieve strong results, most of them are designed for a single modality, leading to redundant compressor deployments in multi-modal settings. Designing a unified multi-modal compressor is critical yet challenging, as different data types vary largely in format, dimension, and statistics. Multi-modal large language models offer a promising resolution but remain too complex for practical use. Thus, we propose \\textbf{OmniZip}, \\textbf{a unified and lightweight lossless compressor for multi-modal data (like image, text, speech, tactile, database, and gene sequence)}. Built on a lightweight backbone, OmniZip incorporates three key components to enable efficient multi-modal lossless compression: a modality-unified tokenizer that reversibly transforms diverse data into tokens, a modality-routing context learning mechanism that enables flexible multi-modal context modeling, and a modality-routing feedforward design that further enhances the model's nonlinear representation flexibility. A reparameterization training strategy is used to enhance model capacity. OmniZip outperforms or matches other state-of-the-art compressors on multiple modalities, achieving 42\\%, 57\\%, 62\\% and 42\\%, 53\\% higher compression efficiency than gzip on CLIC-M, TouchandGo, enwik9, LibriSpeech, and WikiSQL datasets, respectively. It also supports near real-time inference on resource-constrained edge devices, reaching about 1MB/s on MacBook CPUs and iPhone NPUs. Our code is released at https://github.com/adminasmi/OmniZip-CVPR2026.",
    "summary_cn": "研究问题：现有无损压缩器针对单一模态，导致多模态设置中冗余部署。方法：设计一种统一的多模态压缩器。创新点：实现多模态压缩。结果：提高了数据存储和传输效率。"
  },
  {
    "id": "2602.21736v1",
    "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
    "authors": [
      "Hao Luo",
      "Ye Wang",
      "Wanpeng Zhang",
      "Haoqi Yuan",
      "Yicheng Feng",
      "Haiweng Xu",
      "Sipeng Zheng",
      "Zongqing Lu"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21736v1",
    "pdf": "http://arxiv.org/pdf/2602.21736v1",
    "categories": [
      "cs.RO"
    ],
    "summary": "Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.",
    "summary_cn": "研究问题：Vision-Language-Action模型（VLAs）受限于大规模、多样化的机器人数据。方法：提出一种基于人类操作视频的方法。创新点：利用人类操作视频数据。结果：提高了VLAs的性能。"
  },
  {
    "id": "2602.21591v1",
    "title": "CADC: Content Adaptive Diffusion-Based Generative Image Compression",
    "authors": [
      "Xihua Sheng",
      "Lingyu Zhu",
      "Tianyu Zhang",
      "Dong Liu",
      "Shiqi Wang",
      "Jing Wang"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21591v1",
    "pdf": "http://arxiv.org/pdf/2602.21591v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Diffusion-based generative image compression has demonstrated remarkable potential for achieving realistic reconstruction at ultra-low bitrates. The key to unlocking this potential lies in making the entire compression process content-adaptive, ensuring that the encoder's representation and the decoder's generative prior are dynamically aligned with the semantic and structural characteristics of the input image. However, existing methods suffer from three critical limitations that prevent effective content adaptation. First, isotropic quantization applies a uniform quantization step, failing to adapt to the spatially varying complexity of image content and creating a misalignment with the diffusion model's noise-dependent prior. Second, the information concentration bottleneck -- arising from the dimensional mismatch between the high-dimensional noisy latent and the diffusion decoder's fixed input -- prevents the model from adaptively preserving essential semantic information in the primary channels. Third, existing textual conditioning strategies either need significant textual bitrate overhead or rely on generic, content-agnostic textual prompts, thereby failing to provide adaptive semantic guidance efficiently. To overcome these limitations, we propose a content-adaptive diffusion-based image codec with three technical innovations: 1) an Uncertainty-Guided Adaptive Quantization method that learns spatial uncertainty maps to adaptively align quantization distortion with content characteristics; 2) an Auxiliary Decoder-Guided Information Concentration method that uses a lightweight auxiliary decoder to enforce content-aware information preservation in the primary latent channels; and 3) a Bitrate-Free Adaptive Textual Conditioning method that derives content-aware textual descriptions from the auxiliary reconstructed image, enabling semantic guidance without bitrate cost.",
    "summary_cn": "研究问题：扩散式生成图像压缩在超低比特率下重建效果不佳。方法：提出一种内容自适应的压缩方法。创新点：实现内容自适应。结果：提高了压缩图像的重建质量。"
  },
  {
    "id": "2602.21552v1",
    "title": "Generalizing Visual Geometry Priors to Sparse Gaussian Occupancy Prediction",
    "authors": [
      "Changqing Zhou",
      "Yueru Luo",
      "Changhao Chen"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21552v1",
    "pdf": "http://arxiv.org/pdf/2602.21552v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Accurate 3D scene understanding is essential for embodied intelligence, with occupancy prediction emerging as a key task for reasoning about both objects and free space. Existing approaches largely rely on depth priors (e.g., DepthAnything) but make only limited use of 3D cues, restricting performance and generalization. Recently, visual geometry models such as VGGT have shown strong capability in providing rich 3D priors, but similar to monocular depth foundation models, they still operate at the level of visible surfaces rather than volumetric interiors, motivating us to explore how to more effectively leverage these increasingly powerful geometry priors for 3D occupancy prediction. We present GPOcc, a framework that leverages generalizable visual geometry priors (GPs) for monocular occupancy prediction. Our method extends surface points inward along camera rays to generate volumetric samples, which are represented as Gaussian primitives for probabilistic occupancy inference. To handle streaming input, we further design a training-free incremental update strategy that fuses per-frame Gaussians into a unified global representation. Experiments on Occ-ScanNet and EmbodiedOcc-ScanNet demonstrate significant gains: GPOcc improves mIoU by +9.99 in the monocular setting and +11.79 in the streaming setting over prior state of the art. Under the same depth prior, it achieves +6.73 mIoU while running 2.65$\\times$ faster. These results highlight that GPOcc leverages geometry priors more effectively and efficiently. Code will be released at https://github.com/JuIvyy/GPOcc.",
    "summary_cn": "研究问题：现有3D场景理解方法主要依赖深度先验，限制性能。方法：提出一种基于3D线索的方法。创新点：充分利用3D线索。结果：提高了3D场景理解的准确性。"
  },
  {
    "id": "2602.21497v1",
    "title": "See It, Say It, Sorted: An Iterative Training-Free Framework for Visually-Grounded Multimodal Reasoning in LVLMs",
    "authors": [
      "Yongchang Zhang",
      "Xianzheng Ma",
      "Tianyi Liu",
      "Guangquan Zhou",
      "Yang Chen"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21497v1",
    "pdf": "http://arxiv.org/pdf/2602.21497v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Recent large vision-language models (LVLMs) have demonstrated impressive reasoning ability by generating long chain-of-thought (CoT) responses. However, CoT reasoning in multimodal contexts is highly vulnerable to visual hallucination propagation: once an intermediate reasoning step becomes inconsistent with the visual evidence, subsequent steps-even if logically valid-can still lead to incorrect final answers. Existing solutions attempt to mitigate this issue by training models to \"think with images\" via reinforcement learning (RL). While effective, these methods are costly, model-specific, and difficult to generalize across architectures. Differently, we present a lightweight method that bypasses RL training and provides an iterative, training-free, plug-and-play framework for visually-grounded multimodal reasoning. Our key idea is to supervise each reasoning step at test time with visual evidence, ensuring that every decoded token is justified by corresponding visual cues. Concretely, we construct a textual visual-evidence pool that guides the model's reasoning generation. When existing evidence is insufficient, a visual decider module dynamically extracts additional relevant evidence from the image based on the ongoing reasoning context, expanding the pool until the model achieves sufficient visual certainty to terminate reasoning and produce the final answer. Extensive experiments on multiple LVLM backbones and benchmarks demonstrate the effectiveness of our approach. Our method achieves 16.5%-29.5% improvements on TreeBench and 13.7% RH-AUC gains on RH-Bench, substantially reducing hallucination rates while improving reasoning accuracy without additional training.",
    "summary_cn": "研究问题：多模态语境中视觉语言模型（LVLMs）的推理能力受视觉幻觉传播影响。方法：提出一种新的视觉语言模型。创新点：通过生成长链式思维（CoT）响应来增强推理能力。结果：模型在多模态语境中表现出更强的推理能力。"
  },
  {
    "id": "2602.21461v1",
    "title": "VecGlypher: Unified Vector Glyph Generation with Language Models",
    "authors": [
      "Xiaoke Huang",
      "Bhavul Gauri",
      "Kam Woh Ng",
      "Tony Ng",
      "Mengmeng Xu",
      "Zhiheng Liu",
      "Weiming Ren",
      "Zhaochong An",
      "Zijian Zhou",
      "Haonan Qiu",
      "Yuyin Zhou",
      "Sen He",
      "Ziheng Wang",
      "Tao Xiang",
      "Xiao Han"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21461v1",
    "pdf": "http://arxiv.org/pdf/2602.21461v1",
    "categories": [
      "cs.CL"
    ],
    "summary": "Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.",
    "summary_cn": "研究问题：基于向量的符号是数字印刷的原子单元，但大多数学习型管道仍依赖于精心制作的示例表和光栅到向量的后处理。方法：提出VecGlypher，一个单模态语言模型。创新点：生成高质量的向量符号。结果：提高了数字印刷的易访问性和可编辑性。"
  },
  {
    "id": "2602.21917v1",
    "title": "Scan Clusters, Not Pixels: A Cluster-Centric Paradigm for Efficient Ultra-high-definition Image Restoration",
    "authors": [
      "Chen Wu",
      "Ling Wang",
      "Zhuoran Zheng",
      "Yuning Cui",
      "Zhixiong Yang",
      "Xiangyu Chen",
      "Yue Zhang",
      "Weidong Jiang",
      "Jingyuan Xia"
    ],
    "published": "2026-02-25",
    "link": "http://arxiv.org/abs/2602.21917v1",
    "pdf": "http://arxiv.org/pdf/2602.21917v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Ultra-High-Definition (UHD) image restoration is trapped in a scalability crisis: existing models, bound to pixel-wise operations, demand unsustainable computation. While state space models (SSMs) like Mamba promise linear complexity, their pixel-serial scanning remains a fundamental bottleneck for the millions of pixels in UHD content. We ask: must we process every pixel to understand the image? This paper introduces C$^2$SSM, a visual state space model that breaks this taboo by shifting from pixel-serial to cluster-serial scanning. Our core discovery is that the rich feature distribution of a UHD image can be distilled into a sparse set of semantic centroids via a neural-parameterized mixture model. C$^2$SSM leverages this to reformulate global modeling into a novel dual-path process: it scans and reasons over a handful of cluster centers, then diffuses the global context back to all pixels through a principled similarity distribution, all while a lightweight modulator preserves fine details. This cluster-centric paradigm achieves a decisive leap in efficiency, slashing computational costs while establishing new state-of-the-art results across five UHD restoration tasks. More than a solution, C$^2$SSM charts a new course for efficient large-scale vision: scan clusters, not pixels.",
    "summary_cn": "研究问题：超高清（UHD）图像修复受可扩展性危机困扰。方法：提出一种基于状态空间模型（SSMs）的解决方案。创新点：实现线性复杂度。结果：提高了UHD图像修复的计算效率。"
  },
  {
    "id": "2602.21399v1",
    "title": "FedVG: Gradient-Guided Aggregation for Enhanced Federated Learning",
    "authors": [
      "Alina Devkota",
      "Jacob Thrasher",
      "Donald Adjeroh",
      "Binod Bhattarai",
      "Prashnna K. Gyawali"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.21399v1",
    "pdf": "http://arxiv.org/pdf/2602.21399v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "summary": "Federated Learning (FL) enables collaborative model training across multiple clients without sharing their private data. However, data heterogeneity across clients leads to client drift, which degrades the overall generalization performance of the model. This effect is further compounded by overemphasis on poorly performing clients. To address this problem, we propose FedVG, a novel gradient-based federated aggregation framework that leverages a global validation set to guide the optimization process. Such a global validation set can be established using readily available public datasets, ensuring accessibility and consistency across clients without compromising privacy. In contrast to conventional approaches that prioritize client dataset volume, FedVG assesses the generalization ability of client models by measuring the magnitude of validation gradients across layers. Specifically, we compute layerwise gradient norms to derive a client-specific score that reflects how much each client needs to adjust for improved generalization on the global validation set, thereby enabling more informed and adaptive federated aggregation. Extensive experiments on both natural and medical image benchmarking datasets, across diverse model architectures, demonstrate that FedVG consistently improves performance, particularly in highly heterogeneous settings. Moreover, FedVG is modular and can be seamlessly integrated with various state-of-the-art FL algorithms, often further improving their results. Our code is available at https://github.com/alinadevkota/FedVG.",
    "summary_cn": "研究问题：联邦学习（FL）中数据异质性导致模型泛化性能下降。方法：提出一种新的FL方法。创新点：解决数据异质性问题。结果：提高了模型的泛化性能。"
  },
  {
    "id": "2602.21395v1",
    "title": "Momentum Memory for Knowledge Distillation in Computational Pathology",
    "authors": [
      "Yongxin Guo",
      "Hao Lu",
      "Onur C. Koyun",
      "Zhengjie Zhu",
      "Muhammet Fatih Demir",
      "Metin Nafi Gurcan"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.21395v1",
    "pdf": "http://arxiv.org/pdf/2602.21395v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Multimodal learning that integrates genomics and histopathology has shown strong potential in cancer diagnosis, yet its clinical translation is hindered by the limited availability of paired histology-genomics data. Knowledge distillation (KD) offers a practical solution by transferring genomic supervision into histopathology models, enabling accurate inference using histology alone. However, existing KD methods rely on batch-local alignment, which introduces instability due to limited within-batch comparisons and ultimately degrades performance. To address these limitations, we propose Momentum Memory Knowledge Distillation (MoMKD), a cross-modal distillation framework driven by a momentum-updated memory. This memory aggregates genomic and histopathology information across batches, effectively enlarging the supervisory context available to each mini-batch. Furthermore, we decouple the gradients of the genomics and histology branches, preventing genomic signals from dominating histology feature learning during training and eliminating the modality-gap issue at inference time. Extensive experiments on the TCGA-BRCA benchmark (HER2, PR, and ODX classification tasks) and an independent in-house testing dataset demonstrate that MoMKD consistently outperforms state-of-the-art MIL and multimodal KD baselines, delivering strong performance and generalization under histology-only inference. Overall, MoMKD establishes a robust and generalizable knowledge distillation paradigm for computational pathology.",
    "summary_cn": "研究问题：多模态学习在癌症诊断中的应用受限于数据可用性。方法：提出一种基于知识蒸馏（KD）的方法。创新点：利用基因组学数据。结果：提高了癌症诊断的准确性。"
  },
  {
    "id": "2602.21172v2",
    "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
    "authors": [
      "Ishaan Rawal",
      "Shubh Gupta",
      "Yihan Hu",
      "Wei Zhan"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.21172v2",
    "pdf": "http://arxiv.org/pdf/2602.21172v2",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "summary": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with NORD (No Reasoning for Driving). Compared to existing VLAs, NORD achieves competitive performance while being fine-tuned on <60% of the data and no reasoning annotations, resulting in 3x fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. NORD overcomes this by incorporating Dr. GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, NORD achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems. Website: https://nord-vla-ai.github.io/",
    "summary_cn": "研究问题：视觉语言动作（VLA）模型在自动驾驶中的应用受限于数据收集和推理标注。方法：提出一种新的VLA模型。创新点：解决数据收集和标注问题。结果：提高了自动驾驶的性能。"
  },
  {
    "id": "2602.21105v1",
    "title": "BrepGaussian: CAD reconstruction from Multi-View Images with Gaussian Splatting",
    "authors": [
      "Jiaxing Yu",
      "Dongyang Ren",
      "Hangyu Xu",
      "Zhouyuxiao Yang",
      "Yuanqi Li",
      "Jie Guo",
      "Zhengkang Zhou",
      "Yanwen Guo"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.21105v1",
    "pdf": "http://arxiv.org/pdf/2602.21105v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes. We propose B-rep Gaussian Splatting (BrepGaussian), a novel framework that learns 3D parametric representations from 2D images. We employ a Gaussian Splatting renderer with learnable features, followed by a specific fitting strategy. To disentangle geometry reconstruction and feature learning, we introduce a two-stage learning framework that first captures geometry and edges and then refines patch features to achieve clean geometry and coherent instance representations. Extensive experiments demonstrate the superior performance of our approach to state-of-the-art methods. We will release our code and datasets upon acceptance.",
    "summary_cn": "研究问题：从非结构化数据中恢复边界表示（B-rep）是一个具有挑战性的任务。方法：提出一种基于深度学习的方法。创新点：提高了B-rep恢复的准确性。结果：提高了计算机视觉和图形的准确性。"
  },
  {
    "id": "2602.21078v1",
    "title": "ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning",
    "authors": [
      "Duowen Chen",
      "Yan Wang"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.21078v1",
    "pdf": "http://arxiv.org/pdf/2602.21078v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "summary": "Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.",
    "summary_cn": "研究问题：联邦半监督学习（FSSL）中数据异质性是一个挑战。方法：提出一种新的FSSL方法。创新点：解决数据异质性问题。结果：提高了模型的泛化性能。"
  },
  {
    "id": "2602.22025v1",
    "title": "Olbedo: An Albedo and Shading Aerial Dataset for Large-Scale Outdoor Environments",
    "authors": [
      "Shuang Song",
      "Debao Huang",
      "Deyan Deng",
      "Haolin Xiong",
      "Yang Tang",
      "Yajie Zhao",
      "Rongjun Qin"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.22025v1",
    "pdf": "http://arxiv.org/pdf/2602.22025v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Intrinsic image decomposition (IID) of outdoor scenes is crucial for relighting, editing, and understanding large-scale environments, but progress has been limited by the lack of real-world datasets with reliable albedo and shading supervision. We introduce Olbedo, a large-scale aerial dataset for outdoor albedo--shading decomposition in the wild. Olbedo contains 5,664 UAV images captured across four landscape types, multiple years, and diverse illumination conditions. Each view is accompanied by multi-view consistent albedo and shading maps, metric depth, surface normals, sun and sky shading components, camera poses, and, for recent flights, measured HDR sky domes. These annotations are derived from an inverse-rendering refinement pipeline over multi-view stereo reconstructions and calibrated sky illumination, together with per-pixel confidence masks. We demonstrate that Olbedo enables state-of-the-art diffusion-based IID models, originally trained on synthetic indoor data, to generalize to real outdoor imagery: fine-tuning on Olbedo significantly improves single-view outdoor albedo prediction on the MatrixCity benchmark. We further illustrate applications of Olbedo-trained models to multi-view consistent relighting of 3D assets, material editing, and scene change analysis for urban digital twins. We release the dataset, baseline models, and an evaluation protocol to support future research in outdoor intrinsic decomposition and illumination-aware aerial vision.",
    "summary_cn": "研究问题：室外场景的内禀图像分解（IID）受限于数据可用性。方法：提出Olbedo，一个大规模空中数据集。创新点：提供可靠的albedo和阴影监督。结果：提高了室外场景IID的准确性。"
  },
  {
    "id": "2602.20989v1",
    "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
    "authors": [
      "Zheng Gu",
      "Min Lu",
      "Zhida Sun",
      "Dani Lischinski",
      "Daniel Cohen-O",
      "Hui Huang"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20989v1",
    "pdf": "http://arxiv.org/pdf/2602.20989v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.",
    "summary_cn": "研究问题：在现实世界图像中分离视觉层是一个挑战。方法：提出一种基于上下文的图像分解框架。创新点：处理非线性全局耦合交互。结果：提高了图像分解的准确性。"
  },
  {
    "id": "2602.20985v1",
    "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer",
    "authors": [
      "Munish Monga",
      "Vishal Chudasama",
      "Pankaj Wasnik",
      "C. V. Jawahar"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20985v1",
    "pdf": "http://arxiv.org/pdf/2602.20985v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection. This framework generalises across DETR-based detectors, enabling state-of-the-art RF-DETR to operate effectively in evolving-world settings. We also introduce FOGS (Forgetting, Openness, Generalisation Score) to holistically evaluate performance across these dimensions. Extensive experiments on Pascal Series and Diverse Weather benchmarks show EW-DETR outperforms other methods, improving FOGS by 57.24%.",
    "summary_cn": "研究问题：在动态环境中进行无数据访问的实时物体检测。方法：提出EWOD，结合增量学习和领域自适应。创新点：无需访问先验数据，适应新类别和领域变化。结果：有效识别未知物体。"
  },
  {
    "id": "2602.20981v2",
    "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
    "authors": [
      "Christian Simon",
      "Masato Ishii",
      "Wei-Yao Wang",
      "Koichi Saito",
      "Akio Hayakawa",
      "Dongseok Shim",
      "Zhi Zhong",
      "Shuyang Cui",
      "Shusuke Takahashi",
      "Takashi Shibuya",
      "Yuki Mitsufuji"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20981v2",
    "pdf": "http://arxiv.org/pdf/2602.20981v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.",
    "summary_cn": "研究问题：视频与音频多模态对齐的扩展挑战。方法：研究小数据集上模型训练。创新点：探索模型在小数据集上的性能。结果：提高多模态到音频生成的扩展性。"
  },
  {
    "id": "2602.20933v1",
    "title": "Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting",
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Xuanyang Zhang",
      "Zesheng Wang",
      "Yufeng Wang",
      "Wenrui Ding",
      "Gang Yu",
      "Takeo Igarashi"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20933v1",
    "pdf": "http://arxiv.org/pdf/2602.20933v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS",
    "summary_cn": "研究问题：3D Gaussian Splatting Dropout方法中的邻域补偿效应。方法：分析Gaussian opacities的补偿效果。创新点：揭示邻域补偿效应。结果：优化3DGS Dropout方法。"
  },
  {
    "id": "2602.20913v1",
    "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding",
    "authors": [
      "Jihao Qiu",
      "Lingxi Xie",
      "Xinyue Huo",
      "Qi Tian",
      "Qixiang Ye"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20913v1",
    "pdf": "http://arxiv.org/pdf/2602.20913v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1",
    "summary_cn": "研究问题：低计算预算下的长视频理解。方法：提出LongVideo-R1，一个推理型多模态大语言模型。创新点：高效视频上下文导航。结果：避免冗余，提高理解效率。"
  },
  {
    "id": "2602.20903v3",
    "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
    "authors": [
      "Hanshen Zhu",
      "Yuliang Liu",
      "Xuecheng Wu",
      "An-Lan Wang",
      "Hao Feng",
      "Dingkang Yang",
      "Chao Feng",
      "Can Huang",
      "Jingqun Tang",
      "Xiang Bai"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20903v3",
    "pdf": "http://arxiv.org/pdf/2602.20903v3",
    "categories": [
      "cs.CV"
    ],
    "summary": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., Seedream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.",
    "summary_cn": "研究问题：视觉文本渲染中的结构异常问题。方法：分析MLLM和OCR模型在感知结构异常方面的不足。创新点：揭示模型在感知结构异常方面的局限性。结果：提出改进方法。"
  },
  {
    "id": "2602.20901v1",
    "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
    "authors": [
      "Yuechen Xie",
      "Xiaoyan Zhang",
      "Yicheng Shan",
      "Hao Zhu",
      "Rui Tang",
      "Rong Wei",
      "Mingli Song",
      "Yuanyu Wan",
      "Jie Song"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20901v1",
    "pdf": "http://arxiv.org/pdf/2602.20901v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "summary": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.",
    "summary_cn": "研究问题：视觉语言模型在复杂场景中的应用。方法：研究VLM在视觉问答和逻辑推理方面的能力。创新点：探索VLM在复杂场景中的应用潜力。结果：提高VLM在复杂场景中的性能。"
  },
  {
    "id": "2602.20880v2",
    "title": "When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance",
    "authors": [
      "Yongli Xiang",
      "Ziming Hong",
      "Zhaoqing Wang",
      "Xiangyu Zhao",
      "Bo Han",
      "Tongliang Liu"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20880v2",
    "pdf": "http://arxiv.org/pdf/2602.20880v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.",
    "summary_cn": "研究问题：文本到图像扩散模型的安全性问题。方法：提出基于安全引导的方法。创新点：引导生成过程，避免有害内容。结果：降低有害内容生成的风险。"
  },
  {
    "id": "2602.20873v1",
    "title": "MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification",
    "authors": [
      "Jiahao Xu",
      "Sheng Huang",
      "Xin Zhang",
      "Zhixiong Nan",
      "Jiajun Dong",
      "Nankun Mu"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20873v1",
    "pdf": "http://arxiv.org/pdf/2602.20873v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision. To overcome this, we propose the stochastic MUlti-view Semantic Enhancement (MUSE), a framework that first refines semantic precision via sample-wise adaptation and then enhances semantic richness through retrieval-augmented multi-view generation. Specifically, MUSE introduces Sample-wise Fine-grained Semantic Enhancement (SFSE), which yields a fine-grained semantic prior for each sample through MoE-based adaptive visual-semantic interaction. Guided by this prior, Stochastic Multi-view Model Optimization (SMMO) constructs an LLM-generated knowledge base of diverse pathological descriptions per class, then retrieves and stochastically integrates multiple matched textual views during training. These dynamically selected texts serve as enriched semantic supervisions to stochastically optimize the vision-language model, promoting robustness and mitigating overfitting. Experiments on three benchmark WSI datasets show that MUSE consistently outperforms existing vision-language baselines in few-shot settings, demonstrating that effective few-shot pathology learning requires not only richer semantic sources but also their active and sample-aware semantic optimization. Our code is available at: https://github.com/JiahaoXu-god/CVPR2026_MUSE.",
    "summary_cn": "研究问题：计算病理学中少样本全切片图像分类。方法：结合视觉语言方法。创新点：将文本描述作为动态先验。结果：提高分类准确率。"
  },
  {
    "id": "2602.20871v2",
    "title": "GeCo-SRT: Geometry-aware Continual Adaptation for Robotic Cross-Task Sim-to-Real Transfer",
    "authors": [
      "Wenbo Yu",
      "Wenke Xia",
      "Weitao Zhang",
      "Di Hu"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20871v2",
    "pdf": "http://arxiv.org/pdf/2602.20871v2",
    "categories": [
      "cs.RO"
    ],
    "summary": "Bridging the sim-to-real gap is important for applying low-cost simulation data to real-world robotic systems. However, previous methods are severely limited by treating each transfer as an isolated endeavor, demanding repeated, costly tuning and wasting prior transfer experience. To move beyond isolated sim-to-real, we build a continual cross-task sim-to-real transfer paradigm centered on knowledge accumulation across iterative transfers, thereby enabling effective and efficient adaptation to novel tasks. Thus, we propose GeCo-SRT, a geometry-aware continual adaptation method. It utilizes domain-invariant and task-invariant knowledge from local geometric features as a transferable foundation to accelerate adaptation during subsequent sim-to-real transfers. This method starts with a geometry-aware mixture-of-experts module, which dynamically activates experts to specialize in distinct geometric knowledge to bridge observation sim-to-real gap. Further, the geometry-expert-guided prioritized experience replay module preferentially samples from underutilized experts, refreshing specialized knowledge to combat forgetting and maintain robust cross-task performance. Leveraging knowledge accumulated during iterative transfer, GeCo-SRT method not only achieves 52% average performance improvement over the baseline, but also demonstrates significant data efficiency for new task adaptation with only 1/6 data. We hope this work inspires approaches for efficient, low-cost cross-task sim-to-real transfer.",
    "summary_cn": "研究问题：模拟数据到真实机器人系统的迁移。方法：提出跨域迁移方法。创新点：利用先前迁移经验。结果：提高迁移效率。"
  },
  {
    "id": "2602.20794v1",
    "title": "VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving",
    "authors": [
      "Jie Wang",
      "Guang Li",
      "Zhijian Huang",
      "Chenxu Dang",
      "Hangjun Ye",
      "Yahong Han",
      "Long Chen"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20794v1",
    "pdf": "http://arxiv.org/pdf/2602.20794v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.",
    "summary_cn": "研究问题：视觉语言模型在3D几何建模方面的不足。方法：研究VLM在3D几何建模方面的能力。创新点：构建问答系统。结果：提高VLM在3D几何建模方面的性能。"
  },
  {
    "id": "2602.20792v1",
    "title": "SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking",
    "authors": [
      "Muhammad Saif Ullah Khan",
      "Didier Stricker"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20792v1",
    "pdf": "http://arxiv.org/pdf/2602.20792v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.",
    "summary_cn": "研究问题：脊柱运动建模在计算机视觉中的挑战；方法：提出生物力学感知的关键点模拟框架；创新点：结合生物力学知识进行关键点模拟；结果：提高了脊柱运动建模的准确性。"
  },
  {
    "id": "2602.20689v1",
    "title": "MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision",
    "authors": [
      "Bedrettin Cetinkaya",
      "Sinan Kalkan",
      "Emre Akbas"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20689v1",
    "pdf": "http://arxiv.org/pdf/2602.20689v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \\MethodLPP, a lightweight, only $\\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \\MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \\MethodLPP substantially improves the performance of existing edge detection models. In particular, \\MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \\MethodLPP further boosts baseline performance by up to 20--35\\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.",
    "summary_cn": "研究问题：边缘检测中生成清晰边缘图的挑战；方法：提出基于NMS和双阈值算法的边缘细化方法；创新点：改进了边缘细化算法；结果：提高了边缘检测的清晰度。"
  },
  {
    "id": "2602.20685v2",
    "title": "RAYNOVA: Scale-Temporal Autoregressive World Modeling in Ray Space",
    "authors": [
      "Yichen Xie",
      "Chensheng Peng",
      "Mazen Abdelfattah",
      "Yihan Hu",
      "Jiezhi Yang",
      "Eric Higgins",
      "Ryan Brigden",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20685v2",
    "pdf": "http://arxiv.org/pdf/2602.20685v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-agonistic multiview world model for driving scenarios that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at https://raynova-ai.github.io/.",
    "summary_cn": "研究问题：世界基础模型在驾驶场景中的时空相关性处理；方法：提出RAYNOVA，一种几何无关的多视图世界模型；创新点：采用双因果结构；结果：提高了驾驶场景的模拟效果。"
  },
  {
    "id": "2602.20618v1",
    "title": "RecoverMark: Robust Watermarking for Localization and Recovery of Manipulated Faces",
    "authors": [
      "Haonan An",
      "Xiaohui Ye",
      "Guang Hua",
      "Yihang Tao",
      "Hangcheng Cao",
      "Xiangyu Yu",
      "Yuguang Fang"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20618v1",
    "pdf": "http://arxiv.org/pdf/2602.20618v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The proliferation of AI-generated content has facilitated sophisticated face manipulation, severely undermining visual integrity and posing unprecedented challenges to intellectual property. In response, a common proactive defense leverages fragile watermarks to detect, localize, or even recover manipulated regions. However, these methods always assume an adversary unaware of the embedded watermark, overlooking their inherent vulnerability to watermark removal attacks. Furthermore, this fragility is exacerbated in the commonly used dual-watermark strategy that adds a robust watermark for image ownership verification, where mutual interference and limited embedding capacity reduce the fragile watermark's effectiveness. To address the gap, we propose RecoverMark, a watermarking framework that achieves robust manipulation localization, content recovery, and ownership verification simultaneously. Our key insight is twofold. First, we exploit a critical real-world constraint: an adversary must preserve the background's semantic consistency to avoid visual detection, even if they apply global, imperceptible watermark removal attacks. Second, using the image's own content (face, in this paper) as the watermark enhances extraction robustness. Based on these insights, RecoverMark treats the protected face content itself as the watermark and embeds it into the surrounding background. By designing a robust two-stage training paradigm with carefully crafted distortion layers that simulate comprehensive potential attacks and a progressive training strategy, RecoverMark achieves a robust watermark embedding in no fragile manner for image manipulation localization, recovery, and image IP protection simultaneously. Extensive experiments demonstrate the proposed RecoverMark's robustness against both seen and unseen attacks and its generalizability to in-distribution and out-of-distribution data.",
    "summary_cn": "研究问题：AI生成内容对知识产权的挑战；方法：利用脆弱水印进行主动防御；创新点：提出了一种新的水印检测方法；结果：提高了知识产权保护的效果。"
  },
  {
    "id": "2602.20537v1",
    "title": "PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning",
    "authors": [
      "Xinyong Cai",
      "Changbin Sun",
      "Yong Wang",
      "Hongyu Yang",
      "Yuankai Wu"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20537v1",
    "pdf": "http://arxiv.org/pdf/2602.20537v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \\times k$ followed by $k \\times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.",
    "summary_cn": "研究问题：时空预测学习中的纯卷积模型效率问题；方法：提出了一种新的时空预测学习方法；创新点：提高了纯卷积模型的效率；结果：提高了预测的准确性。"
  },
  {
    "id": "2602.20501v1",
    "title": "Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models",
    "authors": [
      "Qing Zhang",
      "Xuesong Li",
      "Jing Zhang"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20501v1",
    "pdf": "http://arxiv.org/pdf/2602.20501v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "What does it mean for a visual system to truly understand affordance? We argue that this understanding hinges on two complementary capacities: geometric perception, which identifies the structural parts of objects that enable interaction, and interaction perception, which models how an agent's actions engage with those parts. To test this hypothesis, we conduct a systematic probing of Visual Foundation Models (VFMs). We find that models like DINO inherently encode part-level geometric structures, while generative models like Flux contain rich, verb-conditioned spatial attention maps that serve as implicit interaction priors. Crucially, we demonstrate that these two dimensions are not merely correlated but are composable elements of affordance. By simply fusing DINO's geometric prototypes with Flux's interaction maps in a training-free and zero-shot manner, we achieve affordance estimation competitive with weakly-supervised methods. This final fusion experiment confirms that geometric and interaction perception are the fundamental building blocks of affordance understanding in VFMs, providing a mechanistic account of how perception grounds action.",
    "summary_cn": "研究问题：视觉系统理解 affordance 的能力；方法：提出几何感知和交互感知两种能力；创新点：从两个角度分析了 affordance 的理解；结果：为 affordance 的理解提供了新的视角。"
  },
  {
    "id": "2602.20496v1",
    "title": "Pip-Stereo: Progressive Iterations Pruner for Iterative Optimization based Stereo Matching",
    "authors": [
      "Jintu Zheng",
      "Qizhe Liu",
      "HuangXin Xu",
      "Zhuojie Chen"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.20496v1",
    "pdf": "http://arxiv.org/pdf/2602.20496v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "While iterative stereo matching achieves high accuracy, its dependence on Recurrent Neural Networks (RNN) hinders edge deployment, a challenge underexplored in existing researches. We analyze iterative refinement and reveal that disparity updates are spatially sparse and temporally redundant. First, we introduce a progressive iteration pruning strategy that suppresses redundant update steps, effectively collapsing the recursive computation into a near-single-pass inference. Second, we propose a collaborative monocular prior transfer framework that implicitly embeds depth priors without requiring a dedicated monocular encoder, thereby eliminating its associated computational burden. Third, we develop FlashGRU, a hardware-aware RNN operator leveraging structured sparsity and I/O-conscious design, achieving a 7.28$\\times$ speedup, 76.6\\% memory peak reduction and 80.9\\% global memory requests reduction over natvie ConvGRUs under 2K resolution. Our PipStereo enables real-time, high-fidelity stereo matching on edge hardware: it processes 320$\\times$640 frames in just 75ms on an NVIDIA Jetson Orin NX (FP16) and 19ms on RTX 4090, matching the accuracy of large iterative based models, and our generalization ability and accuracy far exceeds that of existing real-time methods. Our embedded AI projects will be updated at: https://github.com/XPENG-Aridge-AI.",
    "summary_cn": "研究问题：迭代立体匹配算法的边缘部署问题；方法：分析迭代细化过程，揭示稀疏性和冗余性；创新点：提出了一种新的迭代立体匹配算法；结果：提高了算法的边缘部署能力。"
  },
  {
    "id": "2602.21273v1",
    "title": "StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives",
    "authors": [
      "Jinghao Hu",
      "Yuhe Zhang",
      "GuoHua Geng",
      "Kang Li",
      "Han Zhang"
    ],
    "published": "2026-02-24",
    "link": "http://arxiv.org/abs/2602.21273v1",
    "pdf": "http://arxiv.org/pdf/2602.21273v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes.",
    "summary_cn": "研究问题：生成多帧、动作丰富的视觉叙事的挑战；方法：提出StoryTailor，一种零样本管道；创新点：实现了动作文本、主体身份和背景的连续性；结果：提高了视觉叙事的质量。"
  },
  {
    "id": "2602.20423v1",
    "title": "MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation",
    "authors": [
      "Taha Koleilat",
      "Hojat Asgariandehkordi",
      "Omid Nejati Manzari",
      "Berardino Barile",
      "Yiming Xiao",
      "Hassan Rivaz"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20423v1",
    "pdf": "http://arxiv.org/pdf/2602.20423v1",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "summary": "Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.",
    "summary_cn": "研究问题：医学图像分割的挑战；方法：利用CLIP进行文本引导的医学图像分割；创新点：提出了一种新的医学图像分割方法；结果：提高了医学图像分割的准确性。"
  },
  {
    "id": "2602.20417v1",
    "title": "gQIR: Generative Quanta Image Reconstruction",
    "authors": [
      "Aryan Garg",
      "Sizhuo Ma",
      "Mohit Gupta"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20417v1",
    "pdf": "http://arxiv.org/pdf/2602.20417v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Capturing high-quality images from only a few detected photons is a fundamental challenge in computational imaging. Single-photon avalanche diode (SPAD) sensors promise high-quality imaging in regimes where conventional cameras fail, but raw \\emph{quanta frames} contain only sparse, noisy, binary photon detections. Recovering a coherent image from a burst of such frames requires handling alignment, denoising, and demosaicing (for color) under noise statistics far outside those assumed by standard restoration pipelines or modern generative models. We present an approach that adapts large text-to-image latent diffusion models to the photon-limited domain of quanta burst imaging. Our method leverages the structural and semantic priors of internet-scale diffusion models while introducing mechanisms to handle Bernoulli photon statistics. By integrating latent-space restoration with burst-level spatio-temporal reasoning, our approach produces reconstructions that are both photometrically faithful and perceptually pleasing, even under high-speed motion. We evaluate the method on synthetic benchmarks and new real-world datasets, including the first color SPAD burst dataset and a challenging \\textit{Deforming (XD)} video benchmark. Across all settings, the approach substantially improves perceptual quality over classical and modern learning-based baselines, demonstrating the promise of adapting large generative priors to extreme photon-limited sensing. Code at \\href{https://github.com/Aryan-Garg/gQIR}{https://github.com/Aryan-Garg/gQIR}.",
    "summary_cn": "研究问题：计算成像中捕获高质量图像的挑战；方法：利用SPAD传感器进行成像；创新点：提出了一种新的成像方法；结果：提高了成像质量。"
  },
  {
    "id": "2602.20412v1",
    "title": "SimLBR: Learning to Detect Fake Images by Learning to Detect Real Images",
    "authors": [
      "Aayush Dhakal",
      "Subash Khanal",
      "Srikumar Sastry",
      "Jacob Arndt",
      "Philipe Ambrozio Dias",
      "Dalton Lunga",
      "Nathan Jacobs"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20412v1",
    "pdf": "http://arxiv.org/pdf/2602.20412v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The rapid advancement of generative models has made the detection of AI-generated images a critical challenge for both research and society. Recent works have shown that most state-of-the-art fake image detection methods overfit to their training data and catastrophically fail when evaluated on curated hard test sets with strong distribution shifts. In this work, we argue that it is more principled to learn a tight decision boundary around the real image distribution and treat the fake category as a sink class. To this end, we propose SimLBR, a simple and efficient framework for fake image detection using Latent Blending Regularization (LBR). Our method significantly improves cross-generator generalization, achieving up to +24.85\\% accuracy and +69.62\\% recall on the challenging Chameleon benchmark. SimLBR is also highly efficient, training orders of magnitude faster than existing approaches. Furthermore, we emphasize the need for reliability-oriented evaluation in fake image detection, introducing risk-adjusted metrics and worst-case estimates to better assess model robustness. All code and models will be released on HuggingFace and GitHub.",
    "summary_cn": "研究问题：AI生成图像检测方法过拟合训练数据，在真实数据上表现不佳。方法：提出改进的检测方法。创新点：针对过拟合问题，优化模型结构。结果：检测准确率显著提高。"
  },
  {
    "id": "2602.20409v1",
    "title": "CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation",
    "authors": [
      "Mainak Singha",
      "Sarthak Mehrotra",
      "Paolo Casari",
      "Subhasis Chaudhuri",
      "Elisa Ricci",
      "Biplab Banerjee"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20409v1",
    "pdf": "http://arxiv.org/pdf/2602.20409v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.",
    "summary_cn": "研究问题：视觉-语言模型在领域迁移中脆弱，特别是从合成到真实点云。方法：提出3D领域自适应方法。创新点：结合合成和真实数据，提高模型鲁棒性。结果：模型在真实数据上表现更佳。"
  },
  {
    "id": "2602.20330v1",
    "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
    "authors": [
      "Jingcheng Yang",
      "Tianhu Xiong",
      "Shengyi Qian",
      "Klara Nahrstedt",
      "Mingyuan Wu"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20330v1",
    "pdf": "http://arxiv.org/pdf/2602.20330v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "summary": "Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.",
    "summary_cn": "研究问题：视觉-语言模型黑盒问题。方法：引入透明电路追踪框架。创新点：分析多模态推理过程。结果：揭示模型内部工作机制。"
  },
  {
    "id": "2602.20160v1",
    "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
    "authors": [
      "Chen Wang",
      "Hao Tan",
      "Wang Yifan",
      "Zhiqin Chen",
      "Yuheng Liu",
      "Kalyan Sunkavalli",
      "Sai Bi",
      "Lingjie Liu",
      "Yiwei Hu"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20160v1",
    "pdf": "http://arxiv.org/pdf/2602.20160v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
    "summary_cn": "研究问题：3D重建模型计算复杂度高。方法：提出tttLRM模型，利用TTT层实现长上下文重建。创新点：降低计算复杂度，提高模型能力。结果：重建效果显著提升。"
  },
  {
    "id": "2602.20157v1",
    "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
    "authors": [
      "Zhongxiao Cong",
      "Qitao Zhao",
      "Minsik Jeon",
      "Shubham Tulsiani"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20157v1",
    "pdf": "http://arxiv.org/pdf/2602.20157v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.",
    "summary_cn": "研究问题：3D/4D重建系统依赖密集几何和姿态监督。方法：提出Flow3r框架，结合2D对应关系作为监督。创新点：提高模型鲁棒性。结果：重建效果更佳。"
  },
  {
    "id": "2602.20089v2",
    "title": "StruXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
    "authors": [
      "Zanxi Ruan",
      "Qiuyu Kong",
      "Songqun Gao",
      "Yiming Wang",
      "Marco Cristani"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20089v2",
    "pdf": "http://arxiv.org/pdf/2602.20089v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StruXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StruXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StruXLIP.",
    "summary_cn": "研究问题：边缘表示在视觉理解中的重要性。方法：将边缘表示应用于视觉-语言对齐。创新点：提高模型微调效果。结果：模型性能提升。"
  },
  {
    "id": "2602.20053v1",
    "title": "Decoupling Defense Strategies for Robust Image Watermarking",
    "authors": [
      "Jiahui Chen",
      "Zehang Deng",
      "Zeyu Zhang",
      "Chaoyang Li",
      "Lianchen Jia",
      "Lifeng Sun"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20053v1",
    "pdf": "http://arxiv.org/pdf/2602.20053v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\\%, 33\\% and 46\\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.",
    "summary_cn": "研究问题：深度学习图像水印易受攻击。方法：提出改进的水印方法。创新点：优化编码器和解码器，提高鲁棒性。结果：水印安全性增强。"
  },
  {
    "id": "2602.19944v1",
    "title": "Discover, Segment, and Select: A Progressive Mechanism for Zero-shot Camouflaged Object Segmentation",
    "authors": [
      "Yilong Yang",
      "Jianxin Tian",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19944v1",
    "pdf": "http://arxiv.org/pdf/2602.19944v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Current zero-shot Camouflaged Object Segmentation methods typically employ a two-stage pipeline (discover-then-segment): using MLLMs to obtain visual prompts, followed by SAM segmentation. However, relying solely on MLLMs for camouflaged object discovery often leads to inaccurate localization, false positives, and missed detections. To address these issues, we propose the \\textbf{D}iscover-\\textbf{S}egment-\\textbf{S}elect (\\textbf{DSS}) mechanism, a progressive framework designed to refine segmentation step by step. The proposed method contains a Feature-coherent Object Discovery (FOD) module that leverages visual features to generate diverse object proposals, a segmentation module that refines these proposals through SAM segmentation, and a Semantic-driven Mask Selection (SMS) module that employs MLLMs to evaluate and select the optimal segmentation mask from multiple candidates. Without requiring any training or supervision, DSS achieves state-of-the-art performance on multiple COS benchmarks, especially in multiple-instance scenes.",
    "summary_cn": "研究问题：Camouflaged Object Segmentation方法定位不准确。方法：改进两阶段管道，提高定位精度。创新点：结合MLLMs和SAM分割。结果：分割效果更佳。"
  },
  {
    "id": "2602.19910v1",
    "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
    "authors": [
      "Wei He",
      "Xianghan Meng",
      "Zhiyuan Huang",
      "Xianbiao Qi",
      "Rong Xiao",
      "Chun-Guang Li"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19910v1",
    "pdf": "http://arxiv.org/pdf/2602.19910v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.",
    "summary_cn": "研究问题：Generalized Category Discovery任务中，已知和未知类别识别困难。方法：基于多模态表示学习。创新点：提高识别准确率。结果：模型性能提升。"
  },
  {
    "id": "2602.19900v1",
    "title": "ExpPortrait: Expressive Portrait Generation via Personalized Representation",
    "authors": [
      "Junyi Wang",
      "Yudong Guo",
      "Boyang Guo",
      "Shengming Yang",
      "Juyong Zhang"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19900v1",
    "pdf": "http://arxiv.org/pdf/2602.19900v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "summary": "While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.",
    "summary_cn": "研究问题：扩散模型在肖像生成中表现不佳。方法：提出改进的肖像生成方法。创新点：结合中间信号，提高生成效果。结果：生成视频更具有表现力和可控性。"
  },
  {
    "id": "2602.19863v2",
    "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation",
    "authors": [
      "Filip Wolf",
      "Blaž Rolih",
      "Luka Čehovin Zajc"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19863v2",
    "pdf": "http://arxiv.org/pdf/2602.19863v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Project page: \\textcolor{magenta}{https://wolfilip.github.io/DEO/}.",
    "summary_cn": "研究问题：地球观测（EO）中，多种传感器和模态的多样性使得单一通用模型不现实，如何实现不同模态间的高效知识迁移。方法：提出多种专业EO基础模型（EOFMs）。创新点：强调EOFMs的多样性及其在知识迁移中的重要性。结果：提高了不同模态间知识迁移的效率。"
  },
  {
    "id": "2602.20223v2",
    "title": "MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning",
    "authors": [
      "Wall Kim",
      "Chaeyoung Song",
      "Hanul Kim"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20223v2",
    "pdf": "http://arxiv.org/pdf/2602.20223v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "summary": "Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Multi-Modal Prior-data Fitted Network (MMPFN), which extends TabPFN to handle tabular and non-tabular modalities in a unified manner. MMPFN comprises per-modality encoders, modality projectors, and pre-trained foundation models. The modality projectors serve as the critical bridge, transforming non-tabular embeddings into tabular-compatible tokens for unified processing. To this end, we introduce a multi-head gated MLP and a cross-attention pooler that extract richer context from non-tabular inputs while mitigates attention imbalance issue in multimodal learning. Extensive experiments on medical and general-purpose multimodal datasets demonstrate that MMPFN consistently outperforms competitive state-of-the-art methods and effectively exploits non-tabular modalities alongside tabular features. These results highlight the promise of extending prior-data fitted networks to the multimodal setting, offering a scalable and effective framework for heterogeneous data learning. The source code is available at https://github.com/too-z/MultiModalPFN.",
    "summary_cn": "研究问题：TabPFN在表格数据基础模型中受到关注，但难以整合图像和文本等异构模态。方法：提出多模态扩展。创新点：解决TabPFN在异构模态整合方面的局限性。结果：提高了TabPFN在多模态数据中的应用能力。"
  },
  {
    "id": "2602.19753v1",
    "title": "RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing",
    "authors": [
      "Kaifa Yang",
      "Qi Yang",
      "Yiling Xu",
      "Zhu Li"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19753v1",
    "pdf": "http://arxiv.org/pdf/2602.19753v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.",
    "summary_cn": "研究问题：3D Gaussian Splatting（3DGS）在3D场景重建中表现良好，但迭代优化过程产生大量原始数据，影响重建质量。方法：优化3DGS算法。创新点：减少原始数据数量，提高重建质量。结果：提高了3D场景重建的质量。"
  },
  {
    "id": "2602.19715v1",
    "title": "Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision",
    "authors": [
      "Kartik Kuckreja",
      "Parul Gupta",
      "Muhammad Haris Khan",
      "Abhinav Dhall"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19715v1",
    "pdf": "http://arxiv.org/pdf/2602.19715v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framework for scalable reasoning supervision and evaluation, that integrates an out-of-distribution benchmark containing recent generative and editing forgeries, a human-annotated subset with visual reasoning labels, and a suite of evaluation models, that specialize in evaluating reasoning rationales without the need for explicit ground truth reasoning rationales. The Judge is optimized through a bootstrapped generator-evaluator process that scales human feedback into structured reasoning supervision and supports both pointwise and pairwise evaluation. On the proposed meta-evaluation benchmark, our reasoning-bootstrapped model achieves an accuracy of 96.2\\%, outperforming \\texttt{30x} larger baselines. The reasoning judge attains very high correlation with human ratings and 98.9\\% percent pairwise agreement on the human-annotated meta-evaluation subset. These results establish reasoning fidelity as a quantifiable dimension of deepfake detection and demonstrate scalable supervision for interpretable deepfake reasoning. Our user study shows that participants preferred the reasonings generated by our framework 70\\% of the time, in terms of faithfulness, groundedness, and usefulness, compared to those produced by other models and datasets. All of our datasets, models, and codebase are \\href{https://github.com/KjAeRsTuIsK/DeepfakeJudge}{open-sourced}.",
    "summary_cn": "研究问题：深度伪造检测模型在生成自然语言解释时，其推理缺乏视觉证据支持，影响可靠性。方法：提出DeepfakeJudge框架。创新点：关注推理的可靠性。结果：提高了深度伪造检测模型的可靠性。"
  },
  {
    "id": "2602.19679v1",
    "title": "TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures",
    "authors": [
      "Hyeongjin Nam",
      "Daniel Sungho Jung",
      "Kyoung Mu Lee"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19679v1",
    "pdf": "http://arxiv.org/pdf/2602.19679v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.",
    "summary_cn": "研究问题：从单张图像中联合重建3D人类和物体，在机器人学和数字内容创作中具有广泛应用。方法：提出改进的重建方法。创新点：解决现有方法的局限性。结果：提高了3D重建的准确性。"
  },
  {
    "id": "2602.19615v1",
    "title": "Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness",
    "authors": [
      "Xin Hu",
      "Haomiao Ni",
      "Yunbei Zhang",
      "Jihun Hamm",
      "Zechen Li",
      "Zhengming Ding"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19615v1",
    "pdf": "http://arxiv.org/pdf/2602.19615v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.",
    "summary_cn": "研究问题：视觉语言模型（VLMs）在广泛视觉理解方面取得成功，但在处理罕见物体时存在困难。方法：提出基于数据检索的解决方案。创新点：缓解罕见物体推理问题。结果：提高了VLMs在罕见物体推理方面的性能。"
  },
  {
    "id": "2602.19611v1",
    "title": "RAID: Retrieval-Augmented Anomaly Detection",
    "authors": [
      "Mingxiu Cai",
      "Zhe Zhang",
      "Gaochang Wu",
      "Tianyou Chai",
      "Xiatian Zhu"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19611v1",
    "pdf": "http://arxiv.org/pdf/2602.19611v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \\textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \\href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.",
    "summary_cn": "研究问题：无监督异常检测（UAD）中，现有方法在匹配测试图像和正常模板时面临挑战。方法：提出基于深度学习的解决方案。创新点：提高匹配精度。结果：提高了UAD的准确性。"
  },
  {
    "id": "2602.19605v1",
    "title": "CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning",
    "authors": [
      "Chunlei Meng",
      "Guanhong Huang",
      "Rong Fu",
      "Runmin Jian",
      "Zhongxue Gan",
      "Chun Ouyang"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19605v1",
    "pdf": "http://arxiv.org/pdf/2602.19605v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "summary": "Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.",
    "summary_cn": "研究问题：多模态学习在融合多模态数据时，忽视异步、多级语义结构。方法：提出新的多模态学习方法。创新点：关注多模态数据的语义结构。结果：提高了多模态学习的性能。"
  },
  {
    "id": "2602.19585v1",
    "title": "Tri-Subspaces Disentanglement for Multimodal Sentiment Analysis",
    "authors": [
      "Chunlei Meng",
      "Jiabin Luo",
      "Zhenglin Yan",
      "Zhenyu Yu",
      "Rong Fu",
      "Zhongxue Gan",
      "Chun Ouyang"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19585v1",
    "pdf": "http://arxiv.org/pdf/2602.19585v1",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "summary": "Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic modalities to infer human sentiment. Most existing methods either focus on globally shared representations or modality-specific features, while overlooking signals that are shared only by certain modality pairs. This limits the expressiveness and discriminative power of multimodal representations. To address this limitation, we propose a Tri-Subspace Disentanglement (TSD) framework that explicitly factorizes features into three complementary subspaces: a common subspace capturing global consistency, submodally-shared subspaces modeling pairwise cross-modal synergies, and private subspaces preserving modality-specific cues. To keep these subspaces pure and independent, we introduce a decoupling supervisor together with structured regularization losses. We further design a Subspace-Aware Cross-Attention (SACA) fusion module that adaptively models and integrates information from the three subspaces to obtain richer and more robust representations. Experiments on CMU-MOSI and CMU-MOSEI demonstrate that TSD achieves state-of-the-art performance across all key metrics, reaching 0.691 MAE on CMU-MOSI and 54.9% ACC-7 on CMU-MOSEI, and also transfers well to multimodal intent recognition tasks. Ablation studies confirm that tri-subspace disentanglement and SACA jointly enhance the modeling of multi-granular cross-modal sentiment cues.",
    "summary_cn": "研究问题：多模态情感分析（MSA）在处理特定模态对共享信号时存在局限性。方法：提出新的MSA方法。创新点：关注特定模态对共享信号。结果：提高了MSA的准确性。"
  },
  {
    "id": "2602.19575v1",
    "title": "ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization",
    "authors": [
      "Minseo Kim",
      "Minchan Kwon",
      "Dongyeun Lee",
      "Yunho Jeon",
      "Junmo Kim"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19575v1",
    "pdf": "http://arxiv.org/pdf/2602.19575v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.",
    "summary_cn": "研究问题：个性化文本到图像生成中，概念纠缠导致概念保真度和文本对齐之间的权衡。方法：提出基于解纠缠的方法。创新点：利用手动指导解决概念纠缠问题。结果：提高了个性化文本到图像生成的质量。"
  },
  {
    "id": "2602.19542v1",
    "title": "Vinedresser3D: Agentic Text-guided 3D Editing",
    "authors": [
      "Yankuan Chi",
      "Xiang Li",
      "Zixuan Huang",
      "James M. Rehg"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19542v1",
    "pdf": "http://arxiv.org/pdf/2602.19542v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Text-guided 3D editing aims to modify existing 3D assets using natural-language instructions. Current methods struggle to jointly understand complex prompts, automatically localize edits in 3D, and preserve unedited content. We introduce Vinedresser3D, an agentic framework for high-quality text-guided 3D editing that operates directly in the latent space of a native 3D generative model. Given a 3D asset and an editing prompt, Vinedresser3D uses a multimodal large language model to infer rich descriptions of the original asset, identify the edit region and edit type (addition, modification, deletion), and generate decomposed structural and appearance-level text guidance. The agent then selects an informative view and applies an image editing model to obtain visual guidance. Finally, an inversion-based rectified-flow inpainting pipeline with an interleaved sampling module performs editing in the 3D latent space, enforcing prompt alignment while maintaining 3D coherence and unedited regions. Experiments on diverse 3D edits demonstrate that Vinedresser3D outperforms prior baselines in both automatic metrics and human preference studies, while enabling precise, coherent, and mask-free 3D editing.",
    "summary_cn": "研究问题：3D编辑中理解复杂指令、自动定位编辑和保留未编辑内容的挑战。方法：提出Vinedresser3D，一个用于高质量文本引导的代理框架。创新点：结合自然语言处理和3D编辑技术。结果：有效提高了3D编辑的准确性和效率。"
  },
  {
    "id": "2602.20208v1",
    "title": "Model Merging in the Essential Subspace",
    "authors": [
      "Longhua Li",
      "Lei Qi",
      "Qi Tian",
      "Xin Geng"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20208v1",
    "pdf": "http://arxiv.org/pdf/2602.20208v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "summary": "Model merging aims to integrate multiple task-specific fine-tuned models derived from a shared pre-trained checkpoint into a single multi-task model without additional training. Despite extensive research, task interference remains a major obstacle that often undermines the performance of merged models. In this paper, we propose ESM (Essential Subspace Merging) , a robust framework for effective model merging. We begin by performing Principal Component Analysis (PCA) on feature shifts induced by parameter updates. The resulting principal directions span an essential subspace that dominantly influences feature representations. Each task's parameter update matrix is projected onto its respective essential subspace for low-rank decomposition before merging. This methodology mitigates inter-task interference while preserving core task-specific functionality. Furthermore, we introduce a multi-level polarized scaling strategy that amplifies parameters containing critical knowledge and suppresses redundant ones, preventing essential knowledge from being overwhelmed during fusion. Extensive experiments across multiple task sets and model scales demonstrate that our method achieves state-of-the-art performance in multi-task model merging.",
    "summary_cn": "研究问题：模型合并中任务干扰问题。方法：提出一种无需额外训练的模型合并方法。创新点：解决任务干扰，提高合并模型性能。结果：合并模型在多个任务上均表现出色。"
  },
  {
    "id": "2602.20309v2",
    "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
    "authors": [
      "Jingxuan Zhang",
      "Yunta Hsieh",
      "Zhongwei Wan",
      "Haokun Lin",
      "Xin Wang",
      "Ziqi Wang",
      "Yingtie Lei",
      "Mi Zhang"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20309v2",
    "pdf": "http://arxiv.org/pdf/2602.20309v2",
    "categories": [
      "cs.LG"
    ],
    "summary": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.",
    "summary_cn": "研究问题：VLA模型在实际部署中计算和内存需求过高。方法：提出一种降低计算和内存需求的解决方案。创新点：优化模型结构和算法。结果：提高了VLA模型的实际应用可行性。"
  },
  {
    "id": "2602.19870v1",
    "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
    "authors": [
      "Qiankun Ma",
      "Ziyao Zhang",
      "Haofei Wang",
      "Jie Chen",
      "Zhen Song",
      "Hairong Zheng"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19870v1",
    "pdf": "http://arxiv.org/pdf/2602.19870v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.",
    "summary_cn": "研究问题：VLMs中冗余视觉token导致计算开销大。方法：提出一种基于[CLS]注意力或文本-视觉交叉注意力的优化方法。创新点：减少冗余token，提高推理效率。结果：提高了VLMs的推理速度。"
  },
  {
    "id": "2602.19516v1",
    "title": "Pixel2Phys: Distilling Governing Laws from Visual Dynamics",
    "authors": [
      "Ruikun Li",
      "Jun Yao",
      "Yingfan Hua",
      "Shixiang Tang",
      "Biqing Qi",
      "Bin Liu",
      "Wanli Ouyang",
      "Yan Lu"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19516v1",
    "pdf": "http://arxiv.org/pdf/2602.19516v1",
    "categories": [
      "cs.CE"
    ],
    "summary": "Discovering physical laws directly from high-dimensional visual data is a long-standing human pursuit but remains a formidable challenge for machines, representing a fundamental goal of scientific intelligence. This task is inherently difficult because physical knowledge is low-dimensional and structured, whereas raw video observations are high-dimensional and redundant, with most pixels carrying little or no physical meaning. Extracting concise, physically relevant variables from such noisy data remains a key obstacle. To address this, we propose Pixel2Phys, a collaborative multi-agent framework adaptable to any Multimodal Large Language Model (MLLM). It emulates human scientific reasoning by employing a structured workflow to extract formalized physical knowledge through iterative hypothesis generation, validation, and refinement. By repeatedly formulating, and refining candidate equations on high-dimensional data, it identifies the most concise representations that best capture the underlying physical evolution. This automated exploration mimics the iterative workflow of human scientists, enabling AI to reveal interpretable governing equations directly from raw observations. Across diverse simulated and real-world physics videos, Pixel2Phys discovers accurate, interpretable governing equations and maintaining stable long-term extrapolation where baselines rapidly diverge.",
    "summary_cn": "研究问题：从高维视觉数据中发现物理定律的挑战。方法：提出一种基于低维物理知识结构的解决方案。创新点：利用物理知识结构提高机器学习模型的性能。结果：提高了机器学习模型在物理定律发现任务上的准确率。"
  },
  {
    "id": "2602.19497v1",
    "title": "MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models",
    "authors": [
      "Mingrui Wu",
      "Hang Liu",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19497v1",
    "pdf": "http://arxiv.org/pdf/2602.19497v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks. In this work, we introduce \\textbf{MICON-Bench}, a comprehensive benchmark covering six tasks that evaluate cross-image composition, contextual reasoning, and identity preservation. We further propose an MLLM-driven Evaluation-by-Checkpoint framework for automatic verification of semantic and visual consistency, where multimodal large language model (MLLM) serves as a verifier. Additionally, we present \\textbf{Dynamic Attention Rebalancing (DAR)}, a training-free, plug-and-play mechanism that dynamically adjusts attention during inference to enhance coherence and reduce hallucinations. Extensive experiments on various state-of-the-art open-source models demonstrate both the rigor of MICON-Bench in exposing multi-image reasoning challenges and the efficacy of DAR in improving generation quality and cross-image coherence. Github: https://github.com/Angusliuuu/MICON-Bench.",
    "summary_cn": "研究问题：UMMs在多图像推理上的挑战。方法：提出一种针对多图像推理的解决方案。创新点：优化模型结构和算法。结果：提高了UMMs在多图像推理任务上的性能。"
  },
  {
    "id": "2602.19449v1",
    "title": "Decoupling Vision and Language: Codebook Anchored Visual Adaptation",
    "authors": [
      "Jason Wu",
      "Tianchen Zhao",
      "Chang Liu",
      "Jiarui Cai",
      "Zheng Zhang",
      "Zhuowei Li",
      "Aaditya Singh",
      "Xiang Xu",
      "Mani Srivastava",
      "Jonathan Wu"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19449v1",
    "pdf": "http://arxiv.org/pdf/2602.19449v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Large Vision-Language Models (LVLMs) use their vision encoders to translate images into representations for downstream reasoning, but the encoders often underperform in domain-specific visual tasks such as medical image diagnosis or fine-grained classification, where representation errors can cascade through the language model, leading to incorrect responses. Existing adaptation methods modify the continuous feature interface between encoder and language model through projector tuning or other parameter-efficient updates, which still couples the two components and requires re-alignment whenever the encoder changes. We introduce CRAFT (Codebook RegulAted Fine-Tuning), a lightweight method that fine-tunes the encoder using a discrete codebook that anchors visual representations to a stable token space, achieving domain adaptation without modifying other parts of the model. This decoupled design allows the adapted encoder to seamlessly boost the performance of LVLMs with different language architectures, as long as they share the same codebook. Empirically, CRAFT achieves an average gain of 13.51% across 10 domain-specific benchmarks such as VQARAD and PlantVillage, while preserving the LLM's linguistic capabilities and outperforming peer methods that operate on continuous tokens.",
    "summary_cn": "研究问题：LVLMs在特定领域视觉任务中的性能问题。方法：提出一种针对特定领域视觉任务的解决方案。创新点：优化视觉编码器，提高模型性能。结果：提高了LVLMs在特定领域视觉任务上的准确率。"
  },
  {
    "id": "2602.19596v1",
    "title": "Learning Mutual View Information Graph for Adaptive Adversarial Collaborative Perception",
    "authors": [
      "Yihang Tao",
      "Senkang Hu",
      "Haonan An",
      "Zhengru Fang",
      "Hangcheng Cao",
      "Yuguang Fang"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.19596v1",
    "pdf": "http://arxiv.org/pdf/2602.19596v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Collaborative perception (CP) enables data sharing among connected and autonomous vehicles (CAVs) to enhance driving safety. However, CP systems are vulnerable to adversarial attacks where malicious agents forge false objects via feature-level perturbations. Current defensive systems use threshold-based consensus verification by comparing collaborative and ego detection results. Yet, these defenses remain vulnerable to more sophisticated attack strategies that could exploit two critical weaknesses: (i) lack of robustness against attacks with systematic timing and target region optimization, and (ii) inadvertent disclosure of vulnerability knowledge through implicit confidence information in shared collaboration data. In this paper, we propose MVIG attack, a novel adaptive adversarial CP framework learning to capture vulnerability knowledge disclosed by different defensive CP systems from a unified mutual view information graph (MVIG) representation. Our approach combines MVIG representation with temporal graph learning to generate evolving fabrication risk maps and employs entropy-aware vulnerability search to optimize attack location, timing and persistence, enabling adaptive attacks with generalizability across various defensive configurations. Extensive evaluations on OPV2V and Adv-OPV2V datasets demonstrate that MVIG attack reduces defense success rates by up to 62\\% against state-of-the-art defenses while achieving 47\\% lower detection for persistent attacks at 29.9 FPS, exposing critical security gaps in CP systems. Code will be released at https://github.com/yihangtao/MVIG.git",
    "summary_cn": "研究问题：CP系统易受对抗攻击。方法：提出一种基于特征级扰动检测的防御系统。创新点：提高CP系统的安全性。结果：有效防御了对抗攻击。"
  },
  {
    "id": "2602.20328v1",
    "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems",
    "authors": [
      "Romario Gualdrón-Hurtado",
      "Roman Jacome",
      "Rafael S. Suarez",
      "Henry Arguello"
    ],
    "published": "2026-02-23",
    "link": "http://arxiv.org/abs/2602.20328v1",
    "pdf": "http://arxiv.org/pdf/2602.20328v1",
    "categories": [
      "cs.CV",
      "eess.IV",
      "math.OC"
    ],
    "summary": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.",
    "summary_cn": "研究问题：成像中逆问题求解的挑战。方法：提出一种基于图像先验的解决方案。创新点：利用图像先验提高逆问题求解的准确性。结果：提高了成像逆问题求解的准确性。"
  },
  {
    "id": "2602.19285v1",
    "title": "MRI Contrast Enhancement Kinetics World Model",
    "authors": [
      "Jindi Kong",
      "Yuting He",
      "Cong Xia",
      "Rongjun Ge",
      "Shuo Li"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19285v1",
    "pdf": "http://arxiv.org/pdf/2602.19285v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Clinical MRI contrast acquisition suffers from inefficient information yield, which presents as a mismatch between the risky and costly acquisition protocol and the fixed and sparse acquisition sequence. Applying world models to simulate the contrast enhancement kinetics in the human body enables continuous contrast-free dynamics. However, the low temporal resolution in MRI acquisition restricts the training of world models, leading to a sparsely sampled dataset. Directly training a generative model to capture the kinetics leads to two limitations: (a) Due to the absence of data on missing time, the model tends to overfit to irrelevant features, leading to content distortion. (b) Due to the lack of continuous temporal supervision, the model fails to learn the continuous kinetics law over time, causing temporal discontinuities. For the first time, we propose MRI Contrast Enhancement Kinetics World model (MRI CEKWorld) with SpatioTemporal Consistency Learning (STCL). For (a), guided by the spatial law that patient-level structures remain consistent during enhancement, we propose Latent Alignment Learning (LAL) that constructs a patient-specific template to constrain contents to align with this template. For (b), guided by the temporal law that the kinetics follow a consistent smooth trend, we propose Latent Difference Learning (LDL) which extends the unobserved intervals by interpolation and constrains smooth variations in the latent space among interpolated sequences. Extensive experiments on two datasets show our MRI CEKWorld achieves better realistic contents and kinetics. Codes will be available at https://github.com/DD0922/MRI-Contrast-Enhancement-Kinetics-World-Model.",
    "summary_cn": "研究问题：临床MRI对比度采集效率低。方法：提出一种基于世界模型的解决方案。创新点：模拟人体内对比度增强动力学。结果：提高了临床MRI对比度采集的效率。"
  },
  {
    "id": "2602.19248v1",
    "title": "No Need For Real Anomaly: MLLM Empowered Zero-Shot Video Anomaly Detection",
    "authors": [
      "Zunkai Dai",
      "Ke Li",
      "Jiajia Liu",
      "Jie Yang",
      "Yuanyuan Qiao"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19248v1",
    "pdf": "http://arxiv.org/pdf/2602.19248v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "The collection and detection of video anomaly data has long been a challenging problem due to its rare occurrence and spatio-temporal scarcity. Existing video anomaly detection (VAD) methods under perform in open-world scenarios. Key contributing factors include limited dataset diversity, and inadequate understanding of context-dependent anomalous semantics. To address these issues, i) we propose LAVIDA, an end-to-end zero-shot video anomaly detection framework. ii) LAVIDA employs an Anomaly Exposure Sampler that transforms segmented objects into pseudo-anomalies to enhance model adaptability to unseen anomaly categories. It further integrates a Multimodal Large Language Model (MLLM) to bolster semantic comprehension capabilities. Additionally, iii) we design a token compression approach based on reverse attention to handle the spatio-temporal scarcity of anomalous patterns and decrease computational cost. The training process is conducted solely on pseudo anomalies without any VAD data. Evaluations across four benchmark VAD datasets demonstrate that LAVIDA achieves SOTA performance in both frame-level and pixel-level anomaly detection under the zero-shot setting. Our code is available in https://github.com/VitaminCreed/LAVIDA.",
    "summary_cn": "研究问题：视频异常检测在开放场景下的性能不足，原因包括数据集多样性有限和时空稀缺性。方法：提出了一种新的视频异常检测方法。创新点：通过引入更多样化的数据集和时空信息，提高了检测性能。结果：在开放场景下，该方法显著提高了异常检测的准确性。"
  },
  {
    "id": "2602.19206v2",
    "title": "GS-CLIP: Zero-shot 3D Anomaly Detection by Geometry-Aware Prompt and Synergistic View Representation Learning",
    "authors": [
      "Zehao Deng",
      "An Liu",
      "Yan Wang"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19206v2",
    "pdf": "http://arxiv.org/pdf/2602.19206v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Zero-shot 3D Anomaly Detection is an emerging task that aims to detect anomalies in a target dataset without any target training data, which is particularly important in scenarios constrained by sample scarcity and data privacy concerns. While current methods adapt CLIP by projecting 3D point clouds into 2D representations, they face challenges. The projection inherently loses some geometric details, and the reliance on a single 2D modality provides an incomplete visual understanding, limiting their ability to detect diverse anomaly types. To address these limitations, we propose the Geometry-Aware Prompt and Synergistic View Representation Learning (GS-CLIP) framework, which enables the model to identify geometric anomalies through a two-stage learning process. In stage 1, we dynamically generate text prompts embedded with 3D geometric priors. These prompts contain global shape context and local defect information distilled by our Geometric Defect Distillation Module (GDDM). In stage 2, we introduce Synergistic View Representation Learning architecture that processes rendered and depth images in parallel. A Synergistic Refinement Module (SRM) subsequently fuses the features of both streams, capitalizing on their complementary strengths. Comprehensive experimental results on four large-scale public datasets show that GS-CLIP achieves superior performance in detection. Code can be available at https://github.com/zhushengxinyue/GS-CLIP.",
    "summary_cn": "研究问题：零样本3D异常检测在样本稀缺和数据隐私受限场景下的重要性。方法：通过将CLIP模型应用于3D点云投影，实现异常检测。创新点：提出了一种新的3D异常检测方法，无需目标训练数据。结果：在零样本场景下，该方法有效提高了异常检测的准确性。"
  },
  {
    "id": "2602.19180v1",
    "title": "VLM-Guided Group Preference Alignment for Diffusion-based Human Mesh Recovery",
    "authors": [
      "Wenhao Shen",
      "Hao Wang",
      "Wanqi Yin",
      "Fayao Liu",
      "Xulei Yang",
      "Chao Liang",
      "Zhongang Cai",
      "Guosheng Lin"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19180v1",
    "pdf": "http://arxiv.org/pdf/2602.19180v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Human mesh recovery (HMR) from a single RGB image is inherently ambiguous, as multiple 3D poses can correspond to the same 2D observation. Recent diffusion-based methods tackle this by generating various hypotheses, but often sacrifice accuracy. They yield predictions that are either physically implausible or drift from the input image, especially under occlusion or in cluttered, in-the-wild scenes. To address this, we introduce a dual-memory augmented HMR critique agent with self-reflection to produce context-aware quality scores for predicted meshes. These scores distill fine-grained cues about 3D human motion structure, physical feasibility, and alignment with the input image. We use these scores to build a group-wise HMR preference dataset. Leveraging this dataset, we propose a group preference alignment framework for finetuning diffusion-based HMR models. This process injects the rich preference signals into the model, guiding it to generate more physically plausible and image-consistent human meshes. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches.",
    "summary_cn": "研究问题：从单张RGB图像中恢复人体网格的模糊性。方法：利用扩散模型生成多种假设，提高准确性。创新点：提出了一种新的扩散模型，在保持准确性的同时，避免了物理不真实预测。结果：该方法在人体网格恢复任务中取得了较好的效果。"
  },
  {
    "id": "2602.19170v1",
    "title": "BriMA: Bridged Modality Adaptation for Multi-Modal Continual Action Quality Assessment",
    "authors": [
      "Kanglei Zhou",
      "Chang Li",
      "Qingyi Pan",
      "Liyuan Wang"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19170v1",
    "pdf": "http://arxiv.org/pdf/2602.19170v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Action Quality Assessment (AQA) aims to score how well an action is performed and is widely used in sports analysis, rehabilitation assessment, and human skill evaluation. Multi-modal AQA has recently achieved strong progress by leveraging complementary visual and kinematic cues, yet real-world deployments often suffer from non-stationary modality imbalance, where certain modalities become missing or intermittently available due to sensor failures or annotation gaps. Existing continual AQA methods overlook this issue and assume that all modalities remain complete and stable throughout training, which restricts their practicality. To address this challenge, we introduce Bridged Modality Adaptation (BriMA), an innovative approach to multi-modal continual AQA under modality-missing conditions. BriMA consists of a memory-guided bridging imputation module that reconstructs missing modalities using both task-agnostic and task-specific representations, and a modality-aware replay mechanism that prioritizes informative samples based on modality distortion and distribution drift. Experiments on three representative multi-modal AQA datasets (RG, Fis-V, and FS1000) show that BriMA consistently improves performance under different modality-missing conditions, achieving 6--8\\% higher correlation and 12--15\\% lower error on average. These results demonstrate a step toward robust multi-modal AQA systems under real-world deployment constraints.",
    "summary_cn": "研究问题：动作质量评估（AQA）在多模态融合中的挑战。方法：利用视觉和运动学线索进行多模态AQA。创新点：提出了一种新的多模态AQA方法，提高了评估的准确性。结果：在真实场景中，该方法有效提高了动作质量评估的准确性。"
  },
  {
    "id": "2602.19140v1",
    "title": "CaReFlow: Cyclic Adaptive Rectified Flow for Multimodal Fusion",
    "authors": [
      "Sijie Mai",
      "Shiqin Han"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19140v1",
    "pdf": "http://arxiv.org/pdf/2602.19140v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "summary": "Modality gap significantly restricts the effectiveness of multimodal fusion. Previous methods often use techniques such as diffusion models and adversarial learning to reduce the modality gap, but they typically focus on one-to-one alignment without exposing the data points of the source modality to the global distribution information of the target modality. To this end, leveraging the characteristic of rectified flow that can map one distribution to another via a straight trajectory, we extend rectified flow for modality distribution mapping. Specifically, we leverage the `one-to-many mapping' strategy in rectified flow that allows each data point of the source modality to observe the overall target distribution. This also alleviates the issue of insufficient paired data within each sample, enabling a more robust distribution transformation. Moreover, to achieve more accurate distribution mapping and address the ambiguous flow directions in one-to-many mapping, we design `adaptive relaxed alignment', enforcing stricter alignment for modality pairs belonging to the same sample, while applying relaxed mapping for pairs not belonging to the same sample or category. Additionally, to prevent information loss during distribution mapping, we introduce `cyclic rectified flow' to ensure the transferred features can be translated back to the original features, allowing multimodal representations to learn sufficient modality-specific information. After distribution alignment, our approach achieves very competitive results on multiple tasks of multimodal affective computing even with a simple fusion method, and visualizations verify that it can effectively reduce the modality gap.",
    "summary_cn": "研究问题：模态差距对多模态融合的影响。方法：提出了一种新的方法，通过扩散模型和对抗学习减少模态差距。创新点：该方法关注一对多对齐，同时揭示了源模态的数据点。结果：在多模态融合任务中，该方法提高了融合效果。"
  },
  {
    "id": "2602.19112v2",
    "title": "Universal 3D Shape Matching via Coarse-to-Fine Language Guidance",
    "authors": [
      "Qinfeng Xiao",
      "Guofeng Mei",
      "Bo Yang",
      "Liying Zhang",
      "Jian Zhang",
      "Kit-lun Yick"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19112v2",
    "pdf": "http://arxiv.org/pdf/2602.19112v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Establishing dense correspondences between shapes is a crucial task in computer vision and graphics, while prior approaches depend on near-isometric assumptions and homogeneous subject types (i.e., only operate for human shapes). However, building semantic correspondences for cross-category objects remains challenging and has received relatively little attention. To achieve this, we propose UniMatch, a semantic-aware, coarse-to-fine framework for constructing dense semantic correspondences between strongly non-isometric shapes without restricting object categories. The key insight is to lift \"coarse\" semantic cues into \"fine\" correspondence, which is achieved through two stages. In the \"coarse\" stage, we perform class-agnostic 3D segmentation to obtain non-overlapping semantic parts and prompt multimodal large language models (MLLMs) to identify part names. Then, we employ pretrained vision language models (VLMs) to extract text embeddings, enabling the construction of matched semantic parts. In the \"fine\" stage, we leverage these coarse correspondences to guide the learning of dense correspondences through a dedicated rank-based contrastive scheme. Thanks to class-agnostic segmentation, language guiding, and rank-based contrastive learning, our method is versatile for universal object categories and requires no predefined part proposals, enabling universal matching for inter-class and non-isometric shapes. Extensive experiments demonstrate UniMatch consistently outperforms competing methods in various challenging scenarios.",
    "summary_cn": "研究问题：形状之间建立密集对应关系。方法：提出了一种新的方法，适用于不同类别对象。创新点：该方法不依赖于近等距假设和同质主体类型。结果：在形状对应关系任务中，该方法取得了较好的效果。"
  },
  {
    "id": "2602.19096v1",
    "title": "The Power of Decaying Steps: Enhancing Attack Stability and Transferability for Sign-based Optimizers",
    "authors": [
      "Wei Tao",
      "Yang Dai",
      "Jincai Huang",
      "Qing Tao"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19096v1",
    "pdf": "http://arxiv.org/pdf/2602.19096v1",
    "categories": [
      "cs.LG"
    ],
    "summary": "Crafting adversarial examples can be formulated as an optimization problem. While sign-based optimizers such as I-FGSM and MI-FGSM have become the de facto standard for the induced optimization problems, there still exist several unsolved problems in theoretical grounding and practical reliability especially in non-convergence and instability, which inevitably influences their transferability. Contrary to the expectation, we observe that the attack success rate may degrade sharply when more number of iterations are conducted. In this paper, we address these issues from an optimization perspective. By reformulating the sign-based optimizer as a specific coordinate-wise gradient descent, we argue that one cause for non-convergence and instability is their non-decaying step-size scheduling. Based upon this viewpoint, we propose a series of new attack algorithms that enforce Monotonically Decreasing Coordinate-wise Step-sizes (MDCS) within sign-based optimizers. Typically, we further provide theoretical guarantees proving that MDCS-MI attains an optimal convergence rate of $O(1/\\sqrt{T})$, where $T$ is the number of iterations. Extensive experiments on image classification and cross-modal retrieval tasks demonstrate that our approach not only significantly improves transferability but also enhances attack stability compared to state-of-the-art sign-based methods.",
    "summary_cn": "研究问题：基于优化的对抗样本生成。方法：提出了一种新的优化方法，用于生成对抗样本。创新点：该方法在理论基础和实际可靠性方面取得了进展。结果：在对抗样本生成任务中，该方法提高了生成样本的质量。"
  },
  {
    "id": "2602.19089v1",
    "title": "Ani3DHuman: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling",
    "authors": [
      "Qi Sun",
      "Can Wang",
      "Jiaxiang Shang",
      "Yingchun Liu",
      "Jing Liao"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19089v1",
    "pdf": "http://arxiv.org/pdf/2602.19089v1",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "summary": "Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non-rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present Ani3DHuman, a framework that marries kinematics-based animation with video diffusion priors. We first introduce a layered motion representation that disentangles rigid motion from residual non-rigid motion. Rigid motion is generated by a kinematic method, which then produces a coarse rendering to guide the video diffusion model in generating video sequences that restore the residual non-rigid motion. However, this restoration task, based on diffusion sampling, is highly challenging, as the initial renderings are out-of-distribution, causing standard deterministic ODE samplers to fail. Therefore, we propose a novel self-guided stochastic sampling method, which effectively addresses the out-of-distribution problem by combining stochastic sampling (for photorealistic quality) with self-guidance (for identity fidelity). These restored videos provide high-quality supervision, enabling the optimization of the residual non-rigid motion field. Extensive experiments demonstrate that \\MethodName can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman.",
    "summary_cn": "研究问题：3D人体动画的逼真度问题。方法：提出了一种新的方法，结合非刚性动力学和视频扩散先验。创新点：该方法在保持非刚性运动的同时，避免了质量伪影和身份损失。结果：在3D人体动画任务中，该方法提高了动画的逼真度。"
  },
  {
    "id": "2602.19083v1",
    "title": "ChordEdit: One-Step Low-Energy Transport for Image Editing",
    "authors": [
      "Liangsi Lu",
      "Xuhang Chen",
      "Minzhe Guo",
      "Shichu Li",
      "Jingchao Wang",
      "Yang Shi"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19083v1",
    "pdf": "http://arxiv.org/pdf/2602.19083v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The advent of one-step text-to-image (T2I) models offers unprecedented synthesis speed. However, their application to text-guided image editing remains severely hampered, as forcing existing training-free editors into a single inference step fails. This failure manifests as severe object distortion and a critical loss of consistency in non-edited regions, resulting from the high-energy, erratic trajectories produced by naive vector arithmetic on the models' structured fields. To address this problem, we introduce ChordEdit, a model agnostic, training-free, and inversion-free method that facilitates high-fidelity one-step editing. We recast editing as a transport problem between the source and target distributions defined by the source and target text prompts. Leveraging dynamic optimal transport theory, we derive a principled, low-energy control strategy. This strategy yields a smoothed, variance-reduced editing field that is inherently stable, facilitating the field to be traversed in a single, large integration step. A theoretically grounded and experimentally validated approach allows ChordEdit to deliver fast, lightweight and precise edits, finally achieving true real-time editing on these challenging models.",
    "summary_cn": "研究问题：一步式文本到图像（T2I）模型在文本引导图像编辑中的应用。方法：提出了一种新的方法，将T2I模型应用于文本引导图像编辑。创新点：该方法解决了现有训练自由编辑器在单步推理中的问题。结果：在文本引导图像编辑任务中，该方法提高了编辑效果。"
  },
  {
    "id": "2602.19064v1",
    "title": "L3DR: 3D-aware LiDAR Diffusion and Rectification",
    "authors": [
      "Quan Liu",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19064v1",
    "pdf": "http://arxiv.org/pdf/2602.19064v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Range-view (RV) based LiDAR diffusion has recently made huge strides towards 2D photo-realism. However, it neglects 3D geometry realism and often generates various RV artifacts such as depth bleeding and wavy surfaces. We design L3DR, a 3D-aware LiDAR Diffusion and Rectification framework that can regress and cancel RV artifacts in 3D space and restore local geometry accurately. Our theoretical and empirical analysis reveals that 3D models are inherently superior to 2D models in generating sharp and authentic boundaries. Leveraging such analysis, we design a 3D residual regression network that rectifies RV artifacts and achieves superb geometry realism by predicting point-level offsets in 3D space. On top of that, we design a Welsch Loss that helps focus on local geometry and ignore anomalous regions effectively. Extensive experiments over multiple benchmarks including KITTI, KITTI360, nuScenes and Waymo show that the proposed L3DR achieves state-of-the-art generation and superior geometry-realism consistently. In addition, L3DR is generally applicable to different LiDAR diffusion models with little computational overhead.",
    "summary_cn": "研究问题：基于范围视图（RV）的LiDAR扩散在2D照片真实感方面的局限性。方法：设计了一种新的3D感知LiDAR扩散和校正框架。创新点：该方法关注3D几何真实感，并解决了深度渗透和波浪状表面等RV伪影问题。结果：在LiDAR扩散任务中，该方法提高了2D照片的真实感。"
  },
  {
    "id": "2602.19063v1",
    "title": "Direction-aware 3D Large Multimodal Models",
    "authors": [
      "Quan Liu",
      "Weihao Xuan",
      "Junjue Wang",
      "Naoto Yokoya",
      "Ling Shao",
      "Shijian Lu"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19063v1",
    "pdf": "http://arxiv.org/pdf/2602.19063v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "3D large multimodal models (3D LMMs) rely heavily on ego poses for enabling directional question-answering and spatial reasoning. However, most existing point cloud benchmarks contain rich directional queries but lack the corresponding ego poses, making them inherently ill-posed in 3D large multimodal modelling. In this work, we redefine a new and rigorous paradigm that enables direction-aware 3D LMMs by identifying and supplementing ego poses into point cloud benchmarks and transforming the corresponding point cloud data according to the identified ego poses. We enable direction-aware 3D LMMs with two novel designs. The first is PoseRecover, a fully automatic pose recovery pipeline that matches questions with ego poses from RGB-D video extrinsics via object-frustum intersection and visibility check with Z-buffers. The second is PoseAlign that transforms the point cloud data to be aligned with the identified ego poses instead of either injecting ego poses into textual prompts or introducing pose-encoded features in the projection layers. Extensive experiments show that our designs yield consistent improvements across multiple 3D LMM backbones such as LL3DA, LL3DA-SONATA, Chat-Scene, and 3D-LLAVA, improving ScanRefer mIoU by 30.0% and Scan2Cap LLM-as-judge accuracy by 11.7%. In addition, our approach is simple, generic, and training-efficient, requiring only instruction tuning while establishing a strong baseline for direction-aware 3D-LMMs.",
    "summary_cn": "研究问题：3D LMMs在缺乏ego poses的情况下难以进行方向问答和空间推理。方法：构建包含ego poses的点云基准。创新点：提出包含ego poses的点云基准。结果：提高了3D LMMs的问答和推理能力。"
  },
  {
    "id": "2602.19053v1",
    "title": "TeFlow: Enabling Multi-frame Supervision for Self-Supervised Feed-forward Scene Flow Estimation",
    "authors": [
      "Qingwen Zhang",
      "Chenhan Jiang",
      "Xiaomeng Zhu",
      "Yunqi Miao",
      "Yushan Zhang",
      "Olov Andersson",
      "Patric Jensfelt"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19053v1",
    "pdf": "http://arxiv.org/pdf/2602.19053v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "summary": "Self-supervised feed-forward methods for scene flow estimation offer real-time efficiency, but their supervision from two-frame point correspondences is unreliable and often breaks down under occlusions. Multi-frame supervision has the potential to provide more stable guidance by incorporating motion cues from past frames, yet naive extensions of two-frame objectives are ineffective because point correspondences vary abruptly across frames, producing inconsistent signals. In the paper, we present TeFlow, enabling multi-frame supervision for feed-forward models by mining temporally consistent supervision. TeFlow introduces a temporal ensembling strategy that forms reliable supervisory signals by aggregating the most temporally consistent motion cues from a candidate pool built across multiple frames. Extensive evaluations demonstrate that TeFlow establishes a new state-of-the-art for self-supervised feed-forward methods, achieving performance gains of up to 33\\% on the challenging Argoverse 2 and nuScenes datasets. Our method performs on par with leading optimization-based methods, yet speeds up 150 times. The code is open-sourced at https://github.com/KTH-RPL/OpenSceneFlow along with trained model weights.",
    "summary_cn": "研究问题：自监督场景流估计方法在遮挡情况下可靠性差。方法：引入多帧监督。创新点：多帧监督提高稳定性。结果：提高了场景流估计的可靠性。"
  },
  {
    "id": "2602.19035v1",
    "title": "OpenVO: Open-World Visual Odometry with Temporal Dynamics Awareness",
    "authors": [
      "Phuc D. A. Nguyen",
      "Anh N. Nhu",
      "Ming C. Lin"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19035v1",
    "pdf": "http://arxiv.org/pdf/2602.19035v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We introduce OpenVO, a novel framework for Open-world Visual Odometry (VO) with temporal awareness under limited input conditions. OpenVO effectively estimates real-world-scale ego-motion from monocular dashcam footage with varying observation rates and uncalibrated cameras, enabling robust trajectory dataset construction from rare driving events recorded in dashcam. Existing VO methods are trained on fixed observation frequency (e.g., 10Hz or 12Hz), completely overlooking temporal dynamics information. Many prior methods also require calibrated cameras with known intrinsic parameters. Consequently, their performance degrades when (1) deployed under unseen observation frequencies or (2) applied to uncalibrated cameras. These significantly limit their generalizability to many downstream tasks, such as extracting trajectories from dashcam footage. To address these challenges, OpenVO (1) explicitly encodes temporal dynamics information within a two-frame pose regression framework and (2) leverages 3D geometric priors derived from foundation models. We validate our method on three major autonomous-driving benchmarks - KITTI, nuScenes, and Argoverse 2 - achieving more than 20 performance improvement over state-of-the-art approaches. Under varying observation rate settings, our method is significantly more robust, achieving 46%-92% lower errors across all metrics. These results demonstrate the versatility of OpenVO for real-world 3D reconstruction and diverse downstream applications.",
    "summary_cn": "研究问题：在有限输入条件下，如何进行Open-world Visual Odometry。方法：提出OpenVO框架。创新点：基于时间感知的Open-world VO。结果：实现了从单目监控视频中的鲁棒轨迹估计。"
  },
  {
    "id": "2602.19024v1",
    "title": "Towards Calibrating Prompt Tuning of Vision-Language Models",
    "authors": [
      "Ashshak Sharifdeen",
      "Fahad Shamshad",
      "Muhammad Akhtar Munir",
      "Abhishek Basu",
      "Mohamed Insaf Ismithdeen",
      "Jeyapriyan Jeyamohan",
      "Chathurika Sewwandi Silva",
      "Karthik Nandakumar",
      "Muhammad Haris Khan"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19024v1",
    "pdf": "http://arxiv.org/pdf/2602.19024v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Prompt tuning of large-scale vision-language models such as CLIP enables efficient task adaptation without updating model weights. However, it often leads to poor confidence calibration and unreliable predictive uncertainty. We address this problem by proposing a calibration framework that enhances predictive reliability while preserving the geometry of the pretrained CLIP embedding space, which is required for robust generalization. Our approach extends the standard cross-entropy loss with two complementary regularizers: (1) a mean-variance margin penalty that stabilizes inter-class logit margins by maximizing their average while minimizing dispersion, mitigating underconfidence and overconfidence spikes; and (2) a text moment-matching loss that aligns the first and second moments of tuned text embeddings with their frozen CLIP counterparts, preserving semantic dispersion crucial for generalization. Through extensive experiments across 7 prompt-tuning methods and 11 diverse datasets, we demonstrate that our approach significantly reduces the Expected Calibration Error (ECE) compared to competitive calibration techniques on both base and novel classes",
    "summary_cn": "研究问题：Prompt tuning大视觉语言模型时，如何提高置信度校准和预测不确定性。方法：提出校准框架。创新点：增强置信度校准和预测不确定性。结果：提高了模型的可信度。"
  },
  {
    "id": "2602.18996v1",
    "title": "Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction",
    "authors": [
      "Shannan Yan",
      "Leqi Zheng",
      "Keyu Lv",
      "Jingchen Ni",
      "Hongyang Wei",
      "Jiajun Zhang",
      "Guangting Wang",
      "Jing Lyu",
      "Chun Yuan",
      "Fengyun Rao"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.18996v1",
    "pdf": "http://arxiv.org/pdf/2602.18996v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We study the task of establishing object-level visual correspondence across different viewpoints in videos, focusing on the challenging egocentric-to-exocentric and exocentric-to-egocentric scenarios. We propose a simple yet effective framework based on conditional binary segmentation, where an object query mask is encoded into a latent representation to guide the localization of the corresponding object in a target video. To encourage robust, view-invariant representations, we introduce a cycle-consistency training objective: the predicted mask in the target view is projected back to the source view to reconstruct the original query mask. This bidirectional constraint provides a strong self-supervisory signal without requiring ground-truth annotations and enables test-time training (TTT) at inference. Experiments on the Ego-Exo4D and HANDAL-X benchmarks demonstrate the effectiveness of our optimization objective and TTT strategy, achieving state-of-the-art performance. The code is available at https://github.com/shannany0606/CCMP.",
    "summary_cn": "研究问题：在视频中进行不同视角下的物体级视觉对应。方法：提出基于条件二值分割的框架。创新点：简单有效的对应方法。结果：提高了不同视角下的视觉对应精度。"
  },
  {
    "id": "2602.18993v1",
    "title": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models",
    "authors": [
      "Jiwoo Chung",
      "Sangeek Hyun",
      "MinKyu Lee",
      "Byeongju Han",
      "Geonho Cha",
      "Dongyoon Wee",
      "Youngjun Hong",
      "Jae-Pil Heo"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.18993v1",
    "pdf": "http://arxiv.org/pdf/2602.18993v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.",
    "summary_cn": "研究问题：加速扩散模型采样。方法：基于特征距离的缓存和重用。创新点：改进缓存策略。结果：提高了扩散模型采样的速度。"
  },
  {
    "id": "2602.20205v2",
    "title": "OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport",
    "authors": [
      "Xiwen Chen",
      "Wenhui Zhu",
      "Gen Li",
      "Xuanzhao Dong",
      "Yujian Xiong",
      "Hao Wang",
      "Peijie Qiu",
      "Qingquan Song",
      "Zhipeng Wang",
      "Shao Tang",
      "Yalin Wang",
      "Abolfazl Razi"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.20205v2",
    "pdf": "http://arxiv.org/pdf/2602.20205v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.",
    "summary_cn": "研究问题：降低多模态大语言模型的推理成本。方法：提出视觉token剪枝。创新点：考虑分布结构。结果：降低了推理成本。"
  },
  {
    "id": "2602.19004v1",
    "title": "MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment",
    "authors": [
      "Duc Duy Nguyen",
      "Tat-Jun Chin",
      "Minh Hoai"
    ],
    "published": "2026-02-22",
    "link": "http://arxiv.org/abs/2602.19004v1",
    "pdf": "http://arxiv.org/pdf/2602.19004v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind.",
    "summary_cn": "研究问题：学习IMU信号和2D姿态序列的联合表示。方法：提出MoBind框架。创新点：跨模态检索和动作识别。结果：提高了跨模态检索和动作识别的准确性。"
  },
  {
    "id": "2602.18977v1",
    "title": "Frame2Freq: Spectral Adapters for Fine-Grained Video Understanding",
    "authors": [
      "Thinesh Thiyakesan Ponbagavathi",
      "Constantin Seibold",
      "Alina Roitberg"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18977v1",
    "pdf": "http://arxiv.org/pdf/2602.18977v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Adapting image-pretrained backbones to video typically relies on time-domain adapters tuned to a single temporal scale. Our experiments show that these modules pick up static image cues and very fast flicker changes, while overlooking medium-speed motion. Capturing dynamics across multiple time-scales is, however, crucial for fine-grained temporal analysis (i.e., opening vs. closing bottle). To address this, we introduce Frame2Freq -- a family of frequency-aware adapters that perform spectral encoding during image-to-video adaptation of pretrained Vision Foundation Models (VFMs), improving fine-grained action recognition. Frame2Freq uses Fast Fourier Transform (FFT) along time and learns frequency-band specific embeddings that adaptively highlight the most discriminative frequency ranges. Across five fine-grained activity recognition datasets, Frame2Freq outperforms prior PEFT methods and even surpasses fully fine-tuned models on four of them. These results provide encouraging evidence that frequency analysis methods are a powerful tool for modeling temporal dynamics in image-to-video transfer. Code is available at https://github.com/th-nesh/Frame2Freq.",
    "summary_cn": "研究问题：将图像预训练的backbone应用于视频。方法：引入时间域适配器。创新点：捕捉多时间尺度的动态。结果：提高了视频处理的效果。"
  },
  {
    "id": "2602.18887v1",
    "title": "SafeDrive: Fine-Grained Safety Reasoning for End-to-End Driving in a Sparse World",
    "authors": [
      "Jungho Kim",
      "Jiyong Oh",
      "Seunghoon Yu",
      "Hongjae Shin",
      "Donghyuk Kwak",
      "Jun Won Choi"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18887v1",
    "pdf": "http://arxiv.org/pdf/2602.18887v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The end-to-end (E2E) paradigm, which maps sensor inputs directly to driving decisions, has recently attracted significant attention due to its unified modeling capability and scalability. However, ensuring safety in this unified framework remains one of the most critical challenges. In this work, we propose SafeDrive, an E2E planning framework designed to perform explicit and interpretable safety reasoning through a trajectory-conditioned Sparse World Model. SafeDrive comprises two complementary networks: the Sparse World Network (SWNet) and the Fine-grained Reasoning Network (FRNet). SWNet constructs trajectory-conditioned sparse worlds that simulate the future behaviors of critical dynamic agents and road entities, providing interaction-centric representations for downstream reasoning. FRNet then evaluates agent-specific collision risks and temporal adherence to drivable regions, enabling precise identification of safety-critical events across future timesteps. SafeDrive achieves state-of-the-art performance on both open-loop and closed-loop benchmarks. On NAVSIM, it records a PDMS of 91.6 and an EPDMS of 87.5, with only 61 collisions out of 12,146 scenarios (0.5%). On Bench2Drive, SafeDrive attains a 66.8% driving score.",
    "summary_cn": "研究问题：确保端到端驾驶决策框架的安全性。方法：提出安全保证方法。创新点：统一建模和安全保证。结果：提高了驾驶决策框架的安全性。"
  },
  {
    "id": "2602.18873v1",
    "title": "BiMotion: B-spline Motion for Text-guided Dynamic 3D Character Generation",
    "authors": [
      "Miaowei Wang",
      "Qingxuan Yan",
      "Zhi Cao",
      "Yayuan Li",
      "Oisin Mac Aodha",
      "Jason J. Corso",
      "Amir Vaxman"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18873v1",
    "pdf": "http://arxiv.org/pdf/2602.18873v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Text-guided dynamic 3D character generation has advanced rapidly, yet producing high-quality motion that faithfully reflects rich textual descriptions remains challenging. Existing methods tend to generate limited sub-actions or incoherent motion due to fixed-length temporal inputs and discrete frame-wise representations that fail to capture rich motion semantics. We address these limitations by representing motion with continuous differentiable B-spline curves, enabling more effective motion generation without modifying the capabilities of the underlying generative model. Specifically, our closed-form, Laplacian-regularized B-spline solver efficiently compresses variable-length motion sequences into compact representations with a fixed number of control points. Further, we introduce a normal-fusion strategy for input shape adherence along with correspondence-aware and local-rigidity losses for motion-restoration quality. To train our model, we collate BIMO, a new dataset containing diverse variable-length 3D motion sequences with rich, high-quality text annotations. Extensive evaluations show that our feed-forward framework BiMotion generates more expressive, higher-quality, and better prompt-aligned motions than existing state-of-the-art methods, while also achieving faster generation. Our project page is at: https://wangmiaowei.github.io/BiMotion.github.io/.",
    "summary_cn": "研究问题：如何生成高质量、符合丰富文本描述的运动。方法：提出动态3D角色生成方法，解决固定长度时间输入和离散帧的问题。创新点：动态调整动作序列，实现连贯运动。结果：生成运动更符合文本描述，质量更高。"
  },
  {
    "id": "2602.18867v1",
    "title": "Similarity-as-Evidence: Calibrating Overconfident VLMs for Interpretable and Label-Efficient Medical Active Learning",
    "authors": [
      "Zhuofan Xie",
      "Zishan Lin",
      "Jinliang Lin",
      "Jie Qi",
      "Shaohua Hong",
      "Shuo Li"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18867v1",
    "pdf": "http://arxiv.org/pdf/2602.18867v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Active Learning (AL) reduces annotation costs in medical imaging by selecting only the most informative samples for labeling, but suffers from cold-start when labeled data are scarce. Vision-Language Models (VLMs) address the cold-start problem via zero-shot predictions, yet their temperature-scaled softmax outputs treat text-image similarities as deterministic scores while ignoring inherent uncertainty, leading to overconfidence. This overconfidence misleads sample selection, wasting annotation budgets on uninformative cases. To overcome these limitations, the Similarity-as-Evidence (SaE) framework calibrates text-image similarities by introducing a Similarity Evidence Head (SEH), which reinterprets the similarity vector as evidence and parameterizes a Dirichlet distribution over labels. In contrast to a standard softmax that enforces confident predictions even under weak signals, the Dirichlet formulation explicitly quantifies lack of evidence (vacuity) and conflicting evidence (dissonance), thereby mitigating overconfidence caused by rigid softmax normalization. Building on this, SaE employs a dual-factor acquisition strategy: high-vacuity samples (e.g., rare diseases) are prioritized in early rounds to ensure coverage, while high-dissonance samples (e.g., ambiguous diagnoses) are prioritized later to refine boundaries, providing clinically interpretable selection rationales. Experiments on ten public medical imaging datasets with a 20% label budget show that SaE attains state-of-the-art macro-averaged accuracy of 82.57%. On the representative BTMRI dataset, SaE also achieves superior calibration, with a negative log-likelihood (NLL) of 0.425.",
    "summary_cn": "研究问题：如何解决医学图像标注中的冷启动问题。方法：结合主动学习和视觉语言模型，实现零样本预测。创新点：利用未标记数据提高标注效率。结果：降低标注成本，提高标注质量。"
  },
  {
    "id": "2602.18863v1",
    "title": "TIACam: Text-Anchored Invariant Feature Learning with Auto-Augmentation for Camera-Robust Zero-Watermarking",
    "authors": [
      "Abdullah All Tanvir",
      "Agnibh Dasgupta",
      "Xin Zhong"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18863v1",
    "pdf": "http://arxiv.org/pdf/2602.18863v1",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "summary": "Camera recapture introduces complex optical degradations, such as perspective warping, illumination shifts, and Moiré interference, that remain challenging for deep watermarking systems. We present TIACam, a text-anchored invariant feature learning framework with auto-augmentation for camera-robust zero-watermarking. The method integrates three key innovations: (1) a learnable auto-augmentor that discovers camera-like distortions through differentiable geometric, photometric, and Moiré operators; (2) a text-anchored invariant feature learner that enforces semantic consistency via cross-modal adversarial alignment between image and text; and (3) a zero-watermarking head that binds binary messages in the invariant feature space without modifying image pixels. This unified formulation jointly optimizes invariance, semantic alignment, and watermark recoverability. Extensive experiments on both synthetic and real-world camera captures demonstrate that TIACam achieves state-of-the-art feature stability and watermark extraction accuracy, establishing a principled bridge between multimodal invariance learning and physically robust zero-watermarking.",
    "summary_cn": "研究问题：如何提高相机鲁棒性，应对复杂光学退化。方法：提出TIACam，结合文本锚定和自动增强。创新点：学习相机鲁棒特征。结果：提高水印系统鲁棒性。"
  },
  {
    "id": "2602.18858v2",
    "title": "Hyperbolic Busemann Neural Networks",
    "authors": [
      "Ziheng Chen",
      "Bernhard Schölkopf",
      "Nicu Sebe"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18858v2",
    "pdf": "http://arxiv.org/pdf/2602.18858v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "summary": "Hyperbolic spaces provide a natural geometry for representing hierarchical and tree-structured data due to their exponential volume growth. To leverage these benefits, neural networks require intrinsic and efficient components that operate directly in hyperbolic space. In this work, we lift two core components of neural networks, Multinomial Logistic Regression (MLR) and Fully Connected (FC) layers, into hyperbolic space via Busemann functions, resulting in Busemann MLR (BMLR) and Busemann FC (BFC) layers with a unified mathematical interpretation. BMLR provides compact parameters, a point-to-horosphere distance interpretation, batch-efficient computation, and a Euclidean limit, while BFC generalizes FC and activation layers with comparable complexity. Experiments on image classification, genome sequence learning, node classification, and link prediction demonstrate improvements in effectiveness and efficiency over prior hyperbolic layers. The code is available at https://github.com/GitZH-Chen/HBNN.",
    "summary_cn": "研究问题：如何利用双曲空间表示树状结构数据。方法：提出在双曲空间中操作的神经网络组件。创新点：实现高效的双曲空间计算。结果：提高神经网络性能。"
  },
  {
    "id": "2602.18846v1",
    "title": "DUET-VLM: Dual stage Unified Efficient Token reduction for VLM Training and Inference",
    "authors": [
      "Aditya Kumar Singh",
      "Hitesh Kandala",
      "Pratik Prabhanjan Brahma",
      "Zicheng Liu",
      "Emad Barsoum"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18846v1",
    "pdf": "http://arxiv.org/pdf/2602.18846v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Vision-language models (VLMs) have achieved remarkable multimodal understanding and reasoning capabilities, yet remain computationally expensive due to dense visual tokenization. Existing efficiency approaches either merge redundant visual tokens or drop them progressively in language backbone, often trading accuracy for speed. In this work, we propose DUET-VLM, a versatile plug-and-play dual compression framework that consists of (a) vision-only redundancy aware compression of vision encoder's output into information-preserving tokens, followed by (b) layer-wise, salient text-guided dropping of visual tokens within the language backbone to progressively prune less informative tokens. This coordinated token management enables aggressive compression while retaining critical semantics. On LLaVA-1.5-7B, our approach maintains over 99% of baseline accuracy with 67% fewer tokens, and still retains >97% even at 89% reduction. With this dual-stage compression during training, it achieves 99.7% accuracy at 67% and 97.6% at 89%, surpassing prior SoTA visual token reduction methods across multiple benchmarks. When integrated into Video-LLaVA-7B, it even surpasses the baseline -- achieving >100% accuracy with a substantial 53.1% token reduction and retaining 97.6% accuracy under an extreme 93.4% setting. These results highlight end-to-end training with DUET-VLM, enabling robust adaptation to reduced visual (image/video) input without sacrificing accuracy, producing compact yet semantically rich representations within the same computational budget. Our code is available at https://github.com/AMD-AGI/DUET-VLM.",
    "summary_cn": "研究问题：如何降低视觉语言模型的计算成本。方法：提出基于视觉语言模型的效率方法，减少冗余视觉标记。创新点：优化语言骨干网络。结果：降低计算成本，提高模型效率。"
  },
  {
    "id": "2602.18845v1",
    "title": "Echoes of Ownership: Adversarial-Guided Dual Injection for Copyright Protection in MLLMs",
    "authors": [
      "Chengwei Xia",
      "Fan Ma",
      "Ruijie Quan",
      "Yunqiu Xu",
      "Kun Zhan",
      "Yi Yang"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18845v1",
    "pdf": "http://arxiv.org/pdf/2602.18845v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "With the rapid deployment and widespread adoption of multimodal large language models (MLLMs), disputes regarding model version attribution and ownership have become increasingly frequent, raising significant concerns about intellectual property protection. In this paper, we propose a framework for generating copyright triggers for MLLMs, enabling model publishers to embed verifiable ownership information into the model. The goal is to construct trigger images that elicit ownership-related textual responses exclusively in fine-tuned derivatives of the original model, while remaining inert in other non-derivative models. Our method constructs a tracking trigger image by treating the image as a learnable tensor, performing adversarial optimization with dual-injection of ownership-relevant semantic information. The first injection is achieved by enforcing textual consistency between the output of an auxiliary MLLM and a predefined ownership-relevant target text; the consistency loss is backpropagated to inject this ownership-related information into the image. The second injection is performed at the semantic-level by minimizing the distance between the CLIP features of the image and those of the target text. Furthermore, we introduce an additional adversarial training stage involving the auxiliary model derived from the original model itself. This auxiliary model is specifically trained to resist generating ownership-relevant target text, thereby enhancing robustness in heavily fine-tuned derivative models. Extensive experiments demonstrate the effectiveness of our dual-injection approach in tracking model lineage under various fine-tuning and domain-shift scenarios.",
    "summary_cn": "研究问题：如何解决多模态大型语言模型版本归属和所有权问题。方法：提出框架，实现模型版本追踪。创新点：保护知识产权。结果：提高知识产权保护水平。"
  },
  {
    "id": "2602.18842v1",
    "title": "Detecting AI-Generated Forgeries via Iterative Manifold Deviation Amplification",
    "authors": [
      "Jiangling Zhang",
      "Shuxuan Gao",
      "Bofan Liu",
      "Siqiang Feng",
      "Jirui Huang",
      "Yaxiong Chen",
      "Ziyu Chen"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18842v1",
    "pdf": "http://arxiv.org/pdf/2602.18842v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "The proliferation of highly realistic AI-generated images poses critical challenges for digital forensics, demanding precise pixel-level localization of manipulated regions. Existing methods predominantly learn discriminative patterns of specific forgeries and often struggle with novel manipulations as editing techniques continue to evolve. We propose the Iterative Forgery Amplifier Network (IFA-Net), which shifts from learning \"what is fake\" to modeling \"what is real\". Grounded in the principle that all manipulations deviate from the natural image manifold, IFA-Net leverages a frozen Masked Autoencoder (MAE) pretrained on real images as a universal realness prior. Our framework operates through a two-stage closed-loop process: an initial Dual-Stream Segmentation Network (DSSN) fuses the original image with MAE reconstruction residuals for coarse localization, followed by a Task-Adaptive Prior Injection (TAPI) module that converts this coarse prediction into guiding prompts to steer the MAE decoder and amplify reconstruction failures in suspicious regions for precise refinement. Extensive experiments on four diffusion-based inpainting benchmarks show that IFA-Net achieves an average improvement of 6.5% in IoU and 8.1% in F1-score over the second-best method, while demonstrating strong generalization to traditional manipulation types.",
    "summary_cn": "研究问题：如何精确定位AI生成图像中的篡改区域。方法：学习特定伪造的判别模式。创新点：应对新型篡改。结果：提高图像篡改检测精度。"
  },
  {
    "id": "2602.18831v1",
    "title": "IDperturb: Enhancing Variation in Synthetic Face Generation via Angular Perturbation",
    "authors": [
      "Fadi Boutros",
      "Eduarda Caldeira",
      "Tahar Chettaoui",
      "Naser Damer"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18831v1",
    "pdf": "http://arxiv.org/pdf/2602.18831v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Synthetic data has emerged as a practical alternative to authentic face datasets for training face recognition (FR) systems, especially as privacy and legal concerns increasingly restrict the use of real biometric data. Recent advances in identity-conditional diffusion models have enabled the generation of photorealistic and identity-consistent face images. However, many of these models suffer from limited intra-class variation, an essential property for training robust and generalizable FR models. In this work, we propose IDPERTURB, a simple yet effective geometric-driven sampling strategy to enhance diversity in synthetic face generation. IDPERTURB perturbs identity embeddings within a constrained angular region of the unit hyper-sphere, producing a diverse set of embeddings without modifying the underlying generative model. Each perturbed embedding serves as a conditioning vector for a pre-trained diffusion model, enabling the synthesis of visually varied yet identity-coherent face images suitable for training generalizable FR systems. Empirical results demonstrate that training FR on datasets generated using IDPERTURB yields improved performance across multiple FR benchmarks, compared to existing synthetic data generation approaches.",
    "summary_cn": "研究问题：如何利用合成数据训练人脸识别系统。方法：提出基于身份条件的扩散模型。创新点：生成高质量人脸图像。结果：提高人脸识别系统性能。"
  },
  {
    "id": "2602.18811v1",
    "title": "Learning Multi-Modal Prototypes for Cross-Domain Few-Shot Object Detection",
    "authors": [
      "Wanqi Wang",
      "Jingcai Guo",
      "Yuxiang Cai",
      "Zhi Chen"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18811v1",
    "pdf": "http://arxiv.org/pdf/2602.18811v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel classes in unseen target domains given only a few labeled examples. While open-vocabulary detectors built on vision-language models (VLMs) transfer well, they depend almost entirely on text prompts, which encode domain-invariant semantics but miss domain-specific visual information needed for precise localization under few-shot supervision. We propose a dual-branch detector that Learns Multi-modal Prototypes, dubbed LMP, by coupling textual guidance with visual exemplars drawn from the target domain. A Visual Prototype Construction module aggregates class-level prototypes from support RoIs and dynamically generates hard-negative prototypes in query images via jittered boxes, capturing distractors and visually similar backgrounds. In the visual-guided branch, we inject these prototypes into the detection pipeline with components mirrored from the text branch as the starting point for training, while a parallel text-guided branch preserves open-vocabulary semantics. The branches are trained jointly and ensembled at inference by combining semantic abstraction with domain-adaptive details. On six cross-domain benchmark datasets and standard 1/5/10-shot settings, our method achieves state-of-the-art or highly competitive mAP.",
    "summary_cn": "研究问题：如何实现跨域少量样本目标检测。方法：基于视觉语言模型的开放词汇检测器。创新点：利用文本提示编码域不变信息。结果：提高跨域目标检测性能。"
  },
  {
    "id": "2602.18853v1",
    "title": "Open-Vocabulary Domain Generalization in Urban-Scene Segmentation",
    "authors": [
      "Dong Zhao",
      "Qi Zang",
      "Nan Pu",
      "Wenjing Li",
      "Nicu Sebe",
      "Zhun Zhong"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18853v1",
    "pdf": "http://arxiv.org/pdf/2602.18853v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Domain Generalization in Semantic Segmentation (DG-SS) aims to enable segmentation models to perform robustly in unseen environments. However, conventional DG-SS methods are restricted to a fixed set of known categories, limiting their applicability in open-world scenarios. Recent progress in Vision-Language Models (VLMs) has advanced Open-Vocabulary Semantic Segmentation (OV-SS) by enabling models to recognize a broader range of concepts. Yet, these models remain sensitive to domain shifts and struggle to maintain robustness when deployed in unseen environments, a challenge that is particularly severe in urban-driving scenarios. To bridge this gap, we introduce Open-Vocabulary Domain Generalization in Semantic Segmentation (OVDG-SS), a new setting that jointly addresses unseen domains and unseen categories. We introduce the first benchmark for OVDG-SS in autonomous driving, addressing a previously unexplored problem and covering both synthetic-to-real and real-to-real generalization across diverse unseen domains and unseen categories. In OVDG-SS, we observe that domain shifts often distort text-image correlations in pre-trained VLMs, which hinders the performance of OV-SS models. To tackle this challenge, we propose S2-Corr, a state-space-driven text-image correlation refinement mechanism that mitigates domain-induced distortions and produces more consistent text-image correlations under distribution changes. Extensive experiments on our constructed benchmark demonstrate that the proposed method achieves superior cross-domain performance and efficiency compared to existing OV-SS approaches.",
    "summary_cn": "研究问题：如何实现语义分割的域泛化。方法：提出基于视觉的域泛化方法。创新点：提高模型在未知环境中的鲁棒性。结果：提高语义分割模型性能。"
  },
  {
    "id": "2602.18792v1",
    "title": "MaskDiME: Adaptive Masked Diffusion for Precise and Efficient Visual Counterfactual Explanations",
    "authors": [
      "Changlu Guo",
      "Anders Nymark Christensen",
      "Anders Bjorholm Dahl",
      "Morten Rieger Hannemose"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18792v1",
    "pdf": "http://arxiv.org/pdf/2602.18792v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Visual counterfactual explanations aim to reveal the minimal semantic modifications that can alter a model's prediction, providing causal and interpretable insights into deep neural networks. However, existing diffusion-based counterfactual generation methods are often computationally expensive, slow to sample, and imprecise in localizing the modified regions. To address these limitations, we propose MaskDiME, a simple, fast, and effective diffusion framework that unifies semantic consistency and spatial precision through localized sampling. Our approach adaptively focuses on decision-relevant regions to achieve localized and semantically consistent counterfactual generation while preserving high image fidelity. Our training-free framework, MaskDiME, achieves over 30x faster inference than the baseline method and achieves comparable or state-of-the-art performance across five benchmark datasets spanning diverse visual domains, establishing a practical and generalizable solution for efficient counterfactual explanation.",
    "summary_cn": "研究问题：现有基于扩散的对抗生成方法计算成本高。方法：提出一种新的视觉对抗解释方法。创新点：降低计算成本，提供因果和可解释的洞察。结果：有效揭示模型预测的微小语义修改。"
  },
  {
    "id": "2602.18735v1",
    "title": "LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency",
    "authors": [
      "Weilong Yan",
      "Haipeng Li",
      "Hao Xu",
      "Nianjin Ye",
      "Yihao Ai",
      "Shuaicheng Liu",
      "Jingyu Hu"
    ],
    "published": "2026-02-21",
    "link": "http://arxiv.org/abs/2602.18735v1",
    "pdf": "http://arxiv.org/pdf/2602.18735v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "summary": "This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First, \\ourname{} harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at \\href{https://github.com/DavidYan2001/LaS-Comp}{LaS-Comp}.",
    "summary_cn": "研究问题：3D形状补全在多样化部分观察中存在挑战。方法：引入LaS-Comp，利用3D基础模型的几何先验。创新点：零样本和类别无关，实现多样化类型部分观察的3D形状补全。结果：提高3D形状补全的准确性和效率。"
  },
  {
    "id": "2602.16412v2",
    "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
    "authors": [
      "Daichi Yashima",
      "Shuhei Kurita",
      "Yusuke Oda",
      "Komei Sugiura"
    ],
    "published": "2026-02-18",
    "link": "http://arxiv.org/abs/2602.16412v2",
    "pdf": "http://arxiv.org/pdf/2602.16412v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.",
    "summary_cn": "研究问题：长视频理解是MLLMs的挑战。方法：关注视频理解。创新点：处理RGB帧流，实现视频理解。结果：提高MLLMs在视频理解任务上的性能。"
  },
  {
    "id": "2602.05449v2",
    "title": "DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching",
    "authors": [
      "Chang Zou",
      "Changlin Li",
      "Yang Li",
      "Patrol Li",
      "Jianbing Wu",
      "Xiao He",
      "Songtao Liu",
      "Zhao Zhong",
      "Kailin Huang",
      "Linfeng Zhang"
    ],
    "published": "2026-02-05",
    "link": "http://arxiv.org/abs/2602.05449v2",
    "pdf": "http://arxiv.org/pdf/2602.05449v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code will be made publicly available soon.",
    "summary_cn": "研究问题：扩散模型计算负担重。方法：采用特征缓存加速方法。创新点：无需训练，显著提高速度。结果：有效降低视频生成模型的计算成本。"
  },
  {
    "id": "2601.16788v1",
    "title": "REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion",
    "authors": [
      "Xuewei Li",
      "Xinghan Bao",
      "Zhimin Chen",
      "Xi Li"
    ],
    "published": "2026-01-23",
    "link": "http://arxiv.org/abs/2601.16788v1",
    "pdf": "http://arxiv.org/pdf/2601.16788v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.",
    "summary_cn": "研究问题：全景语义分割方法存在局限性。方法：提出一种新的PASS方法。创新点：考虑球形几何和深度信息。结果：提高全景语义分割的准确性和鲁棒性。"
  },
  {
    "id": "2601.16672v1",
    "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
    "authors": [
      "Ming Li",
      "Hui Shan",
      "Kai Zheng",
      "Chentao Shen",
      "Siyu Liu",
      "Yanwei Fu",
      "Zhen Chen",
      "Xiangru Huang"
    ],
    "published": "2026-01-23",
    "link": "http://arxiv.org/abs/2601.16672v1",
    "pdf": "http://arxiv.org/pdf/2601.16672v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.",
    "summary_cn": "研究问题：高质量3D服装重建存在挑战。方法：提出基于3D Gaussian Splats的重建方法。创新点：提高重建质量，减少sim-to-real差距。结果：实现高质量的3D服装重建。"
  },
  {
    "id": "2601.13401v1",
    "title": "Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics",
    "authors": [
      "Peter A. Massih",
      "Eric Cosatto"
    ],
    "published": "2026-01-19",
    "link": "http://arxiv.org/abs/2601.13401v1",
    "pdf": "http://arxiv.org/pdf/2601.13401v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.",
    "summary_cn": "研究问题：VLMs在定量空间推理方面存在缺陷。方法：改进VLMs架构。创新点：保留像素级信息，实现计数和测量。结果：提高VLMs在定量空间推理任务上的性能。"
  },
  {
    "id": "2601.09823v2",
    "title": "NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration",
    "authors": [
      "Subhajit Sanyal",
      "Srinivas Soumitri Miriyala",
      "Akshay Janardan Bankar",
      "Manjunath Arveti",
      "Sowmya Vajrala",
      "Shreyas Pandith",
      "Sravanth Kodavanti",
      "Abhishek Ameta",
      "Harshit",
      "Amit Satish Unde"
    ],
    "published": "2026-01-14",
    "link": "http://arxiv.org/abs/2601.09823v2",
    "pdf": "http://arxiv.org/pdf/2601.09823v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.",
    "summary_cn": "研究问题：Latent扩散模型在边缘设备上部署困难。方法：提出轻量级变体。创新点：压缩去噪U-Net，降低计算成本。结果：提高Latent扩散模型在边缘设备上的部署效率。"
  },
  {
    "id": "2601.09708v2",
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "authors": [
      "Chi-Pin Huang",
      "Yunze Man",
      "Zhiding Yu",
      "Min-Hung Chen",
      "Jan Kautz",
      "Yu-Chiang Frank Wang",
      "Fu-En Yang"
    ],
    "published": "2026-01-14",
    "link": "http://arxiv.org/abs/2601.09708v2",
    "pdf": "http://arxiv.org/pdf/2601.09708v2",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
    "summary_cn": "研究问题：VLA推理存在延迟问题。方法：引入Object-WIPER框架。创新点：去除动态物体，实现视频修复。结果：提高VLA推理的准确性和效率。"
  },
  {
    "id": "2601.06391v2",
    "title": "Object-WIPER : Training-Free Object and Associated Effect Removal in Videos",
    "authors": [
      "Saksham Singh Kushwaha",
      "Sayan Nag",
      "Yapeng Tian",
      "Kuldeep Kulkarni"
    ],
    "published": "2026-01-10",
    "link": "http://arxiv.org/abs/2601.06391v2",
    "pdf": "http://arxiv.org/pdf/2601.06391v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "In this paper, we introduce Object-WIPER, a training-free framework for removing dynamic objects and their associated visual effects from videos, and inpainting them with semantically consistent and temporally coherent content. Our approach leverages a pre-trained text-to-video diffusion transformer (DiT). Given an input video, a user-provided object mask, and query tokens describing the target object and its effects, we localize relevant visual tokens via visual-text cross-attention and visual self-attention. This produces an intermediate effect mask that we fuse with the user mask to obtain a final foreground token mask to replace. We first invert the video through the DiT to obtain structured noise, then reinitialize the masked tokens with Gaussian noise while preserving background tokens. During denoising, we copy values for the background tokens saved during inversion to maintain scene fidelity. To address the lack of suitable evaluation, we introduce a new object removal metric that rewards temporal consistency among foreground tokens across consecutive frames, coherence between foreground and background tokens within each frame, and dissimilarity between the input and output foreground tokens. Experiments on DAVIS and a newly curated real-world associated effect benchmark (WIPER-Bench) show that Object-WIPER surpasses both training-based and training-free baselines in terms of the metric, achieving clean removal and temporally stable reconstruction without any retraining. Our new benchmark, source code, and pre-trained models will be publicly available.",
    "summary_cn": "研究问题：动态视频处理存在挑战。方法：提出Object-WIPER框架。创新点：去除动态物体，实现视频修复。结果：提高视频处理的质量和效率。"
  },
  {
    "id": "2601.01695v1",
    "title": "Learnability-Driven Submodular Optimization for Active Roadside 3D Detection",
    "authors": [
      "Ruiyu Mao",
      "Baoming Zhang",
      "Nicholas Ruozzi",
      "Yunhui Guo"
    ],
    "published": "2026-01-04",
    "link": "http://arxiv.org/abs/2601.01695v1",
    "pdf": "http://arxiv.org/pdf/2601.01695v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.",
    "summary_cn": "研究问题：路边感知数据集的标注困难；方法：提出路边数据集的标注方法；创新点：解决硬件和隐私限制下的标注问题；结果：提高标注准确性。"
  },
  {
    "id": "2601.00991v1",
    "title": "UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data",
    "authors": [
      "Joshua Kawaguchi",
      "Saad Manzur",
      "Emily Gao Wang",
      "Maitreyi Sinha",
      "Bryan Vela",
      "Yunxi Wang",
      "Brandon Vela",
      "Wayne B. Hayes"
    ],
    "published": "2026-01-02",
    "link": "http://arxiv.org/abs/2601.00991v1",
    "pdf": "http://arxiv.org/pdf/2601.00991v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted \"coherent\" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.",
    "summary_cn": "研究问题：高质量3D人体姿态数据获取困难；方法：构建UnrealPose-Gen渲染管线；创新点：实现高质量离线渲染；结果：生成包含3D关节的高质量帧。"
  },
  {
    "id": "2512.21058v2",
    "title": "Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control",
    "authors": [
      "Minghao Han",
      "Yichen Liu",
      "Yizhou Liu",
      "Zizhi Chen",
      "Jingqun Tang",
      "Xuecheng Wu",
      "Dingkang Yang",
      "Lihua Zhang"
    ],
    "published": "2025-12-24",
    "link": "http://arxiv.org/abs/2512.21058v2",
    "pdf": "http://arxiv.org/pdf/2512.21058v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The dataset and code can be obtained from https://github.com/Hanminghao/UniPath.",
    "summary_cn": "研究问题：计算病理学中理解和生成模型的进展不平衡；方法：提出解决三个耦合因素的方法；创新点：提高大尺寸、高质量数据集的获取；结果：提升诊断水平。"
  },
  {
    "id": "2512.20770v1",
    "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective",
    "authors": [
      "Markus Gross",
      "Sai B. Matha",
      "Aya Fahmy",
      "Rui Song",
      "Daniel Cremers",
      "Henri Meess"
    ],
    "published": "2025-12-23",
    "link": "http://arxiv.org/abs/2512.20770v1",
    "pdf": "http://arxiv.org/pdf/2512.20770v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.",
    "summary_cn": "研究问题：语义场景补全在移动机器人中的应用；方法：提出基于SSC的3D感知方法；创新点：联合估计体积占用和像素级语义；结果：实现整体场景理解。"
  },
  {
    "id": "2512.20340v2",
    "title": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
    "authors": [
      "Qingdong He",
      "Xueqin Chen",
      "Yanjie Pan",
      "Peng Tang",
      "Pengcheng Xu",
      "Zhenye Gan",
      "Chengjie Wang",
      "Xiaobin Hu",
      "Jiangning Zhang",
      "Yabiao Wang"
    ],
    "published": "2025-12-23",
    "link": "http://arxiv.org/abs/2512.20340v2",
    "pdf": "http://arxiv.org/pdf/2512.20340v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.",
    "summary_cn": "研究问题：基于扩散变换的视频虚拟试穿方法存在局限性；方法：提出改进的VVT方法；创新点：捕捉服装动态和保持背景完整性；结果：降低计算成本。"
  },
  {
    "id": "2512.17514v3",
    "title": "Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection",
    "authors": [
      "Sairam VCR",
      "Rishabh Lalla",
      "Aveen Dayal",
      "Tejal Kulkarni",
      "Anuj Lalla",
      "Vineeth N Balasubramanian",
      "Muhammad Haris Khan"
    ],
    "published": "2025-12-19",
    "link": "http://arxiv.org/abs/2512.17514v3",
    "pdf": "http://arxiv.org/pdf/2512.17514v3",
    "categories": [
      "cs.CV"
    ],
    "summary": "Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.",
    "summary_cn": "研究问题：源无对象检测方法在域偏移下的性能下降；方法：提出改进的SFOD方法；创新点：提高对象聚焦表示能力；结果：降低背景杂波中的高置信度激活。"
  },
  {
    "id": "2512.16397v1",
    "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
    "authors": [
      "Haodi He",
      "Jihun Yu",
      "Ronald Fedkiw"
    ],
    "published": "2025-12-18",
    "link": "http://arxiv.org/abs/2512.16397v1",
    "pdf": "http://arxiv.org/pdf/2512.16397v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "summary": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
    "summary_cn": "研究问题：构建人脸图像的统一解释；方法：利用高斯分层；创新点：提高约束的明确性和适用性；结果：实现人脸图像的统一解释。"
  },
  {
    "id": "2512.05016v2",
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "authors": [
      "Qi Mao",
      "Hao Cheng",
      "Tinghan Yang",
      "Libiao Jin",
      "Siwei Ma"
    ],
    "published": "2025-12-04",
    "link": "http://arxiv.org/abs/2512.05016v2",
    "pdf": "http://arxiv.org/pdf/2512.05016v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
    "summary_cn": "研究问题：基于扩散变换的视频压缩框架；方法：提出GNVC-VD框架；创新点：统一时空潜压缩和序列级生成优化；结果：提高视频压缩性能。"
  },
  {
    "id": "2512.04421v1",
    "title": "UTrice: Unifying Primitives in Differentiable Ray Tracing and Rasterization via Triangles for Particle-Based 3D Scenes",
    "authors": [
      "Changhe Liu",
      "Ehsan Javanmardi",
      "Naren Bao",
      "Alex Orsholits",
      "Manabu Tsukada"
    ],
    "published": "2025-12-04",
    "link": "http://arxiv.org/abs/2512.04421v1",
    "pdf": "http://arxiv.org/pdf/2512.04421v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "summary": "Ray tracing 3D Gaussian particles enables realistic effects such as depth of field, refractions, and flexible camera modeling for novel-view synthesis. However, existing methods trace Gaussians through proxy geometry, which requires constructing complex intermediate meshes and performing costly intersection tests. This limitation arises because Gaussian-based particles are not well suited as unified primitives for both ray tracing and rasterization. In this work, we propose a differentiable triangle-based ray tracing pipeline that directly treats triangles as rendering primitives without relying on any proxy geometry. Our results show that the proposed method achieves significantly higher rendering quality than existing ray tracing approaches while maintaining real-time rendering performance. Moreover, our pipeline can directly render triangles optimized by the rasterization-based method Triangle Splatting, thus unifying the primitives used in novel-view synthesis.",
    "summary_cn": "研究问题：基于光线追踪的3D高斯粒子方法存在局限性；方法：提出改进的光线追踪方法；创新点：无需构建复杂中间网格；结果：提高合成新视角的效果。"
  },
  {
    "id": "2512.04309v1",
    "title": "Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction",
    "authors": [
      "Rui Fonseca",
      "Bruno Martins",
      "Gil Rocha"
    ],
    "published": "2025-12-03",
    "link": "http://arxiv.org/abs/2512.04309v1",
    "pdf": "http://arxiv.org/pdf/2512.04309v1",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "summary": "Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.",
    "summary_cn": "研究问题：图像字幕生成对标注数据的依赖；方法：提出无标注数据训练方法；创新点：降低对标注数据的依赖；结果：提高图像字幕生成的准确性。"
  },
  {
    "id": "2512.02700v4",
    "title": "VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm",
    "authors": [
      "Zhenkai Wu",
      "Xiaowen Ma",
      "Zhenliang Ni",
      "Dengming Zhang",
      "Han Shu",
      "Xin Jiang",
      "Xinghao Chen"
    ],
    "published": "2025-12-02",
    "link": "http://arxiv.org/abs/2512.02700v4",
    "pdf": "http://arxiv.org/pdf/2512.02700v4",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "summary": "Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup. The code is available at https://github.com/Casey-bit/VLMPruner.",
    "summary_cn": "研究问题：视觉语言模型在移动设备上的部署受限于大量视觉标记的计算成本。方法：提出一种考虑标记间冗余的剪枝方法。创新点：通过剪枝减少视觉标记数量，降低计算成本。结果：在移动设备上实现了高效的视觉语言模型部署。"
  },
  {
    "id": "2512.02686v2",
    "title": "ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data",
    "authors": [
      "Yuxing Liu",
      "Zheng Li",
      "Huanhuan Liang",
      "Ji Zhang",
      "Zeyu Sun",
      "Yong Liu"
    ],
    "published": "2025-12-02",
    "link": "http://arxiv.org/abs/2512.02686v2",
    "pdf": "http://arxiv.org/pdf/2512.02686v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.",
    "summary_cn": "研究问题：异常分割在自动驾驶中检测和定位未知或异常物体。方法：提出一种基于异常数据的模型泛化方法。创新点：通过引入异常数据增强模型泛化能力。结果：提高了异常分割的准确性和鲁棒性。"
  },
  {
    "id": "2512.02172v1",
    "title": "SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting",
    "authors": [
      "Pranav Asthana",
      "Alex Hanson",
      "Allen Tu",
      "Tom Goldstein",
      "Matthias Zwicker",
      "Amitabh Varshney"
    ],
    "published": "2025-12-01",
    "link": "http://arxiv.org/abs/2512.02172v1",
    "pdf": "http://arxiv.org/pdf/2512.02172v1",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "summary": "3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.",
    "summary_cn": "研究问题：3D高斯分层（3DGS）在生成高质量新视图方面具有潜力。方法：将超分辨率（SR）应用于低分辨率（LR）输入视图。创新点：通过SR提高渲染分辨率。结果：实现了更高分辨率的渲染效果。"
  },
  {
    "id": "2511.23055v2",
    "title": "MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents",
    "authors": [
      "Ruoxuan Zhang",
      "Qiyun Zheng",
      "Zhiyu Zhou",
      "Ziqi Liao",
      "Siyu Wu",
      "Jian-Yu Jiang-Lin",
      "Bin Wen",
      "Hongxia Xie",
      "Jianlong Fu",
      "Wen-Huang Cheng"
    ],
    "published": "2025-11-28",
    "link": "http://arxiv.org/abs/2511.23055v2",
    "pdf": "http://arxiv.org/pdf/2511.23055v2",
    "categories": [
      "cs.AI"
    ],
    "summary": "Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.",
    "summary_cn": "研究问题：视觉语言具身代理缺乏基于心智理论的决策能力。方法：提出一种基于心智理论的决策方法。创新点：结合代理视角和人类心智状态进行决策。结果：提高了代理的决策能力。"
  },
  {
    "id": "2511.22860v1",
    "title": "MARVO: Marine-Adaptive Radiance-aware Visual Odometry",
    "authors": [
      "Sacchin Sundar",
      "Atman Kikani",
      "Aaliya Alam",
      "Sumukh Shrote",
      "A. Nayeemulla Khan",
      "A. Shahina"
    ],
    "published": "2025-11-28",
    "link": "http://arxiv.org/abs/2511.22860v1",
    "pdf": "http://arxiv.org/pdf/2511.22860v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "summary": "Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).",
    "summary_cn": "研究问题：水下视觉定位受波长依赖衰减、纹理差和非高斯传感器噪声影响。方法：提出一种基于物理感知、学习融合的里程计框架。创新点：融合水下图像形成建模、可微分匹配和强化学习。结果：提高了水下视觉定位的精度。"
  },
  {
    "id": "2511.20629v4",
    "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
    "authors": [
      "Chieh-Yun Chen",
      "Zhonghao Wang",
      "Qi Chen",
      "Zhifan Ye",
      "Min Shi",
      "Yue Zhao",
      "Yinan Zhao",
      "Hui Qu",
      "Wei-An Lin",
      "Yiru Shen",
      "Ajinkya Kale",
      "Irfan Essa",
      "Humphrey Shi"
    ],
    "published": "2025-11-25",
    "link": "http://arxiv.org/abs/2511.20629v4",
    "pdf": "http://arxiv.org/pdf/2511.20629v4",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "summary": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
    "summary_cn": "研究问题：联合优化多个奖励模型存在对齐税。方法：提出一种平衡优化方法。创新点：降低对齐税，提高模型性能。结果：实现了更好的模型对齐效果。"
  },
  {
    "id": "2511.18570v1",
    "title": "PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation",
    "authors": [
      "Samarth Chopra",
      "Jing Liang",
      "Gershom Seneviratne",
      "Dinesh Manocha"
    ],
    "published": "2025-11-23",
    "link": "http://arxiv.org/abs/2511.18570v1",
    "pdf": "http://arxiv.org/pdf/2511.18570v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "summary": "Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.",
    "summary_cn": "研究问题：现有3D重建方法无法推断物理属性。方法：提出一种基于物理属性的3D重建方法。创新点：结合几何和外观信息推断物理属性。结果：提高了3D重建的准确性。"
  },
  {
    "id": "2511.18507v2",
    "title": "Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives",
    "authors": [
      "Kai Jiang",
      "Siqi Huang",
      "Xiangyu Chen",
      "Jiawei Shao",
      "Hongyuan Zhang",
      "Xuelong Li"
    ],
    "published": "2025-11-23",
    "link": "http://arxiv.org/abs/2511.18507v2",
    "pdf": "http://arxiv.org/pdf/2511.18507v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.",
    "summary_cn": "研究问题：多模态大型语言模型（MLLMs）在视觉理解中的持续学习。方法：提出一种针对MLLMs的持续学习方法。创新点：解决灾难性遗忘问题。结果：提高了MLLMs在动态场景下的适应性。"
  },
  {
    "id": "2511.18281v1",
    "title": "Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation",
    "authors": [
      "Yara Bahram",
      "Melodie Desbos",
      "Mohammadhadi Shateri",
      "Eric Granger"
    ],
    "published": "2025-11-23",
    "link": "http://arxiv.org/abs/2511.18281v1",
    "pdf": "http://arxiv.org/pdf/2511.18281v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.",
    "summary_cn": "研究问题：扩散模型（DMs）在适应新领域时采样成本高。方法：提出一种两阶段训练管道。创新点：实现快速且高质量的生成。结果：提高了DMs在新型领域的生成效果。"
  },
  {
    "id": "2511.16156v2",
    "title": "Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers",
    "authors": [
      "Jian Ma",
      "Qirong Peng",
      "Xujie Zhu",
      "Peixing Xie",
      "Chen Chen",
      "Haonan Lu"
    ],
    "published": "2025-11-20",
    "link": "http://arxiv.org/abs/2511.16156v2",
    "pdf": "http://arxiv.org/pdf/2511.16156v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\\% reduction in parameter count compared to the full model, with less than 3\\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.",
    "summary_cn": "研究问题：扩散变换器（DiTs）参数量大，计算成本高。方法：提出一种可插拔剪枝和连续层蒸馏（PPCL）方法。创新点：降低DiTs的计算成本。结果：提高了DiTs在资源受限环境下的部署效率。"
  },
  {
    "id": "2511.15487v2",
    "title": "NTK-Guided Implicit Neural Teaching",
    "authors": [
      "Chen Zhang",
      "Wei Zuo",
      "Bingyang Cheng",
      "Yikun Wang",
      "Wei-Bin Kou",
      "Yik Chung WU",
      "Ngai Wong"
    ],
    "published": "2025-11-19",
    "link": "http://arxiv.org/abs/2511.15487v2",
    "pdf": "http://arxiv.org/pdf/2511.15487v2",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "summary": "Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.",
    "summary_cn": "研究问题：如何优化高分辨率信号的隐式神经网络表示（INRs）参数化。方法：通过多层感知器（MLPs）参数化连续信号。创新点：提出了一种新的优化方法，减少优化坐标数量。结果：提高了高分辨率信号建模的效率和准确性。"
  },
  {
    "id": "2511.13135v2",
    "title": "MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation",
    "authors": [
      "Junjie Yang",
      "Yuhao Yan",
      "Gang Wu",
      "Yuxuan Wang",
      "Ruoyu Liang",
      "Xinjie Jiang",
      "Xiang Wan",
      "Fenglei Fan",
      "Yongquan Zhang",
      "Feiwei Qin",
      "Changmiao Wang"
    ],
    "published": "2025-11-17",
    "link": "http://arxiv.org/abs/2511.13135v2",
    "pdf": "http://arxiv.org/pdf/2511.13135v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce MedGEN-Bench, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.",
    "summary_cn": "研究问题：如何使AI系统在医疗应用中生成与文本诊断相对应的医学图像。方法：开发了一种视觉-语言模型（VLM）。创新点：将VLM应用于医学图像生成。结果：生成的医学图像与临床工作流程无缝集成。"
  },
  {
    "id": "2511.12554v1",
    "title": "EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis",
    "authors": [
      "Yijie Guo",
      "Dexiang Hong",
      "Weidong Chen",
      "Zihan She",
      "Cheng Ye",
      "Xiaojun Chang",
      "Zhendong Mao"
    ],
    "published": "2025-11-16",
    "link": "http://arxiv.org/abs/2511.12554v1",
    "pdf": "http://arxiv.org/pdf/2511.12554v1",
    "categories": [
      "cs.CV"
    ],
    "summary": "Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.",
    "summary_cn": "研究问题：如何提高视觉情感分析（VEA）的准确性和可解释性。方法：构建了一个开源、可解释的数据集。创新点：提出了一种新的情感标签分配方法。结果：提高了VEA的准确性和可解释性。"
  },
  {
    "id": "2511.12370v3",
    "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion",
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Jason Lai",
      "Lloyd Windrim",
      "Donald Dansereau",
      "Niko Sünderhauf",
      "Dimity Miller"
    ],
    "published": "2025-11-15",
    "link": "http://arxiv.org/abs/2511.12370v3",
    "pdf": "http://arxiv.org/pdf/2511.12370v3",
    "categories": [
      "cs.CV"
    ],
    "summary": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.",
    "summary_cn": "研究问题：如何提高在线场景变化检测（SCD）的准确性。方法：提出了一种新的在线SCD方法。创新点：首次实现了在线SCD。结果：提高了SCD的准确性。"
  },
  {
    "id": "2511.11851v2",
    "title": "Defending Unauthorized Model Merging via Dual-Stage Weight Protection",
    "authors": [
      "Wei-Jia Chen",
      "Min-Yen Tsai",
      "Cheng-Yi Lee",
      "Chia-Mu Yu"
    ],
    "published": "2025-11-14",
    "link": "http://arxiv.org/abs/2511.11851v2",
    "pdf": "http://arxiv.org/pdf/2511.11851v2",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "summary": "The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.",
    "summary_cn": "研究问题：如何防止未经授权的模型合并。方法：开发了一种检测未经授权模型合并的方法。创新点：提出了一种新的检测算法。结果：有效防止了未经授权的模型合并。"
  },
  {
    "id": "2511.10376v3",
    "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation",
    "authors": [
      "Xun Huang",
      "Shijia Zhao",
      "Yunxiang Wang",
      "Xin Lu",
      "Wanfa Zhang",
      "Rongsheng Qu",
      "Weixin Li",
      "Yunhong Wang",
      "Chenglu Wen"
    ],
    "published": "2025-11-13",
    "link": "http://arxiv.org/abs/2511.10376v3",
    "pdf": "http://arxiv.org/pdf/2511.10376v3",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "summary": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relation",
    "summary_cn": "研究问题：如何实现机器人导航的零样本泛化。方法：提出了一种基于零样本的导航方法。创新点：利用了3D场景的隐式表示。结果：提高了机器人导航的泛化能力。"
  },
  {
    "id": "2511.01425v1",
    "title": "Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis",
    "authors": [
      "Yuhang Huang",
      "Zekai Lin",
      "Fan Zhong",
      "Lei Liu"
    ],
    "published": "2025-11-03",
    "link": "http://arxiv.org/abs/2511.01425v1",
    "pdf": "http://arxiv.org/pdf/2511.01425v1",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "summary": "Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18\\% compared to a non-interactive baseline. To validate the faithfulness of the agent's explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($Δ$Brier=+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.",
    "summary_cn": "研究问题：如何提高AI模型在医学领域的可解释性。方法：开发了一种交互式解释代理。创新点：通过可审计的动作序列生成解释。结果：提高了AI模型在医学领域的可解释性和可信度。"
  },
  {
    "id": "2510.08318v3",
    "title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation",
    "authors": [
      "Yushi Huang",
      "Xingtong Ge",
      "Ruihao Gong",
      "Chengtao Lv",
      "Jun Zhang"
    ],
    "published": "2025-10-09",
    "link": "http://arxiv.org/abs/2510.08318v3",
    "pdf": "http://arxiv.org/pdf/2510.08318v3",
    "categories": [
      "cs.CV"
    ],
    "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.",
    "summary_cn": "研究问题：如何降低视频扩散模型（DMs）的计算成本。方法：提出了一种基于线性注意力的DMs。创新点：降低了计算复杂度。结果：降低了DMs的计算成本。"
  },
  {
    "id": "2509.25210v2",
    "title": "STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting",
    "authors": [
      "Hao Chen",
      "Tao Han",
      "Jie Zhang",
      "Song Guo",
      "Lei Bai"
    ],
    "published": "2025-09-21",
    "link": "http://arxiv.org/abs/2509.25210v2",
    "pdf": "http://arxiv.org/pdf/2509.25210v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "summary": "To gain finer regional forecasts, many works have explored the regional integration from the global atmosphere, e.g., by solving boundary equations in physics-based methods or cropping regions from global forecasts in data-driven methods. However, the effectiveness of these methods is often constrained by static and imprecise regional boundaries, resulting in poor generalization ability. To address this issue, we propose Spatial-Temporal Weather Forecasting (STCast), a novel AI-driven framework for adaptive regional boundary optimization and dynamic monthly forecast allocation. Specifically, our approach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns global and regional spatial distributions to initialize boundaries and adaptively refines them based on attention-derived alignment patterns. Furthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where atmospheric variables from distinct months are dynamically routed to specialized experts using a discrete Gaussian distribution, enhancing the model's ability to capture temporal patterns. Beyond global and regional forecasting, we evaluate our STCast on extreme event prediction and ensemble forecasting. Experimental results demonstrate consistent superiority over state-of-the-art methods across all four tasks.",
    "summary_cn": "研究问题：如何提高区域天气预报的准确性。方法：提出了一种基于区域整合的方法。创新点：结合了物理和数据驱动方法。结果：提高了区域天气预报的准确性。"
  },
  {
    "id": "2509.14544v2",
    "title": "Association and Consolidation: Evolutionary Memory-Enhanced Incremental Multi-View Clustering",
    "authors": [
      "Zisen Kong",
      "Bo Zhong",
      "Pengyuan Li",
      "Dongxia Chang",
      "Yiming Wang",
      "Yongyong Chen"
    ],
    "published": "2025-09-18",
    "link": "http://arxiv.org/abs/2509.14544v2",
    "pdf": "http://arxiv.org/pdf/2509.14544v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Incremental multi-view clustering aims to achieve stable clustering results while addressing the stability-plasticity dilemma (SPD) in view-incremental scenarios. The core challenge is that the model must have enough plasticity to quickly adapt to new data, while maintaining sufficient stability to consolidate long-term knowledge. To address this challenge, we propose a novel Evolutionary Memory-Enhanced Incremental Multi-View Clustering (EMIMC), inspired by the memory regulation mechanisms of the human brain. Specifically, we design a rapid association module to establish connections between new and historical views, thereby ensuring the plasticity required for learning new knowledge. Second, a cognitive forgetting module with a decay mechanism is introduced. By dynamically adjusting the contribution of the historical view to optimize knowledge integration. Finally, we propose a knowledge consolidation module to progressively refine short-term knowledge into stable long-term memory using temporal tensors, thereby ensuring model stability. By integrating these modules, EMIMC achieves strong knowledge retention capabilities in scenarios with growing views. Extensive experiments demonstrate that EMIMC exhibits remarkable advantages over existing state-of-the-art methods.",
    "summary_cn": "研究问题：如何解决增量多视图聚类中的稳定性-塑性困境（SPD）。方法：提出了一种新的增量多视图聚类方法。创新点：平衡了模型的可塑性和稳定性。结果：实现了稳定的聚类结果。"
  },
  {
    "id": "2509.01552v2",
    "title": "Variation-aware Vision Token Dropping for Faster Large Vision-Language Models",
    "authors": [
      "Junjie Chen",
      "Xuyang Liu",
      "Zichen Wen",
      "Yiyu Wang",
      "Siteng Huang",
      "Honggang Chen"
    ],
    "published": "2025-09-01",
    "link": "http://arxiv.org/abs/2509.01552v2",
    "pdf": "http://arxiv.org/pdf/2509.01552v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Large vision-language models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding tasks. However, the increasing demand for high-resolution image and long-video understanding results in substantial token counts, consequently leading to reduced inference efficiency. Token compression offers a direct solution by reducing the number of tokens to be processed, thereby improving computational efficiency without architectural changes. Through extensive analysis, we identify two critical limitations in existing inner-LLM token compression methods: positional bias and incompatibility with efficient operators, which critically hinder their practical deployment for LVLM acceleration. This paper presents the first approach from a dynamic token variation perspective, revealing that visual token variations within LLMs exhibit task-agnostic properties. We propose Variation-aware Vision Token Dropping (\\textit{i.e.}, \\textbf{V$^2$Drop}), which progressively removes visual tokens with minimal variation during LVLM inference, thereby enhancing computational efficiency. Extensive experiments across multiple models and benchmarks consistently demonstrate that V$^2$Drop maintains \\textbf{94.0\\%} and \\textbf{98.6\\%} of the original performance for image and video understanding tasks respectively, while reducing LLM generation latency by \\textbf{31.5\\%} and \\textbf{74.2\\%}.",
    "summary_cn": "研究问题：大视觉语言模型在多模态理解任务中的效率问题。方法：提出一种基于token压缩的优化方法。创新点：通过减少token数量提高推理效率。结果：显著提升了推理速度。"
  },
  {
    "id": "2508.02291v2",
    "title": "FAIR-Pruner: Leveraging Tolerance of Difference for Flexible Automatic Layer-Wise Neural Network Pruning",
    "authors": [
      "Chenqing Lin",
      "Mostafa Hussien",
      "Chengyao Yu",
      "Bingyi Jing",
      "Mohamed Cheriet",
      "Osama Abdelrahman",
      "Ruixing Ming"
    ],
    "published": "2025-08-04",
    "link": "http://arxiv.org/abs/2508.02291v2",
    "pdf": "http://arxiv.org/pdf/2508.02291v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "summary": "Neural network pruning has been widely adopted to reduce the parameter scale of complex neural networks, enabling efficient deployment on resource-limited edge devices. Mainstream pruning methods typically adopt uniform pruning strategies, which tend to cause a substantial performance degradation under high sparsity levels. Recent studies focus on non-uniform layer-wise pruning, but such approaches typically depend on global architecture optimization, which is computational expensive and lacks flexibility. To address these limitations, this paper proposes a novel method named Flexible Automatic Identification and Removal (FAIR)-Pruner, which adaptively determines the sparsity levels of each layer and identifies the units to be pruned. The core of FAIR-Pruner lies in the introduction of a novel indicator, Tolerance of Differences (ToD), designed to balance the importance scores obtained from two complementary perspectives: the architecture-level (Utilization Score) and the task-level (Reconstruction Score). By controlling ToD at preset levels, FAIR-Pruner determines layer-specific thresholds and removes units whose Utilization Scores fall below the corresponding thresholds. Furthermore, by decoupling threshold determination from importance estimation, FAIR-Pruner allows users to flexibly obtain pruned models under varying pruning ratios. Extensive experiments demonstrate that FAIR-Pruner achieves state-of-the-art performance, maintaining higher accuracy even at high compression ratios. Moreover, the ToD based layer-wise pruning ratios can be directly applied to existing powerful importance measurements, thereby improving the performance under uniform-pruning.",
    "summary_cn": "研究问题：神经网络剪枝方法导致性能下降。方法：提出一种非均匀剪枝策略。创新点：根据重要性调整剪枝力度。结果：在保持性能的同时降低了参数规模。"
  },
  {
    "id": "2507.10065v2",
    "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second",
    "authors": [
      "Chenguo Lin",
      "Yuchen Lin",
      "Panwang Pan",
      "Yifan Yu",
      "Tao Hu",
      "Honglei Yan",
      "Katerina Fragkiadaki",
      "Yadong Mu"
    ],
    "published": "2025-07-14",
    "link": "http://arxiv.org/abs/2507.10065v2",
    "pdf": "http://arxiv.org/pdf/2507.10065v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "We present MoVieS, a Motion-aware View Synthesis model that reconstructs 4D dynamic scenes from monocular videos in one second. It represents dynamic 3D scenes with pixel-aligned Gaussian primitives and explicitly supervises their time-varying motions. This allows, for the first time, the unified modeling of appearance, geometry and motion from monocular videos, and enables reconstruction, view synthesis and 3D point tracking within a single learning-based framework. By bridging view synthesis with geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups.",
    "summary_cn": "研究问题：从单目视频中重建4D动态场景。方法：提出MoVieS模型，使用高斯原语表示动态场景。创新点：首次实现统一运动感知。结果：重建效果显著。"
  },
  {
    "id": "2506.09217v2",
    "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule",
    "authors": [
      "Boyu Jiang",
      "Liang Shi",
      "Zhengzhi Lin",
      "Lanxin Xiang",
      "Loren Stowe",
      "Feng Guo"
    ],
    "published": "2025-06-10",
    "link": "http://arxiv.org/abs/2506.09217v2",
    "pdf": "http://arxiv.org/pdf/2506.09217v2",
    "categories": [
      "cs.RO",
      "cs.CV",
      "stat.AP"
    ],
    "summary": "The safety of autonomous driving systems (ADS) depends on accurate perception across distance and driving conditions. The outputs of AI perception algorithms are stochastic, which have a major impact on decision making and safety outcomes, including time-to-collision estimation. However, current perception evaluation metrics do not reflect the stochastic nature of perception algorithms. We introduce the Perception Characteristics Distance (PCD), a novel metric incorporating model output uncertainty as represented by the farthest distance at which an object can be reliably detected. To represent a system's overall perception capability in terms of reliable detection distance, we average PCD values across multiple detection quality and probabilistic thresholds to produce the average PCD (aPCD). For empirical validation, we present the SensorRainFall dataset, collected on the Virginia Smart Road using a sensor-equipped vehicle (cameras, radar, and LiDAR) under different weather (clear and rainy) and illumination conditions (daylight, streetlight, and nighttime). The dataset includes ground-truth distances, bounding boxes, and segmentation masks for target objects. Experiments with state-of-the-art models show that aPCD captures meaningful differences across weather, daylight, and illumination conditions, which traditional evaluation metrics fail to reflect. PCD provides an uncertainty-aware measure of perception performance, supporting safer and more robust ADS operation, while the SensorRainFall dataset offers a valuable benchmark for evaluation. The SensorRainFall dataset is publicly available at https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the evaluation code is available at https://github.com/datadrivenwheels/PCD_Python.",
    "summary_cn": "研究问题：自动驾驶系统中感知算法的随机性对安全的影响。方法：提出一种基于时间碰撞估计的改进方法。创新点：提高感知算法的准确性。结果：提升了自动驾驶系统的安全性。"
  },
  {
    "id": "2506.07917v3",
    "title": "SpeeDe3DGS: Speedy Deformable 3D Gaussian Splatting with Temporal Pruning and Motion Grouping",
    "authors": [
      "Allen Tu",
      "Haiyang Ying",
      "Alex Hanson",
      "Yonghan Lee",
      "Tom Goldstein",
      "Matthias Zwicker"
    ],
    "published": "2025-06-09",
    "link": "http://arxiv.org/abs/2506.07917v3",
    "pdf": "http://arxiv.org/pdf/2506.07917v3",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "summary": "Dynamic extensions of 3D Gaussian Splatting (3DGS) achieve high-quality reconstructions through neural motion fields, but per-Gaussian neural inference makes these models computationally expensive. Building on DeformableGS, we introduce Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), which bridges this efficiency-fidelity gap through three complementary modules: Temporal Sensitivity Pruning (TSP) removes low-impact Gaussians via temporally aggregated sensitivity analysis, Temporal Sensitivity Sampling (TSS) perturbs timestamps to suppress floaters and improve temporal coherence, and GroupFlow distills the learned deformation field into shared SE(3) transformations for efficient groupwise motion. On the 50 dynamic scenes in MonoDyGauBench, integrating TSP and TSS into DeformableGS accelerates rendering by 6.78$\\times$ on average while maintaining neural-field fidelity and using 10$\\times$ fewer primitives. Adding GroupFlow culminates in 13.71$\\times$ faster rendering and 2.53$\\times$ shorter training, surpassing all baselines in speed while preserving superior image quality.",
    "summary_cn": "研究问题：3D高斯分层（3DGS）模型的计算成本。方法：提出Speedy Deformable 3D Gaussian Splatting（SpeeDe3DGS）模型。创新点：降低计算成本。结果：提高了重建速度。"
  },
  {
    "id": "2506.01783v2",
    "title": "Harnessing Chain-of-Thought Reasoning in Multimodal Large Language Models for Face Anti-Spoofing",
    "authors": [
      "Honglu Zhang",
      "Zhiqin Fang",
      "Ningning Zhao",
      "Saihui Hou",
      "Long Ma",
      "Renwang Pei",
      "Zhaofeng He"
    ],
    "published": "2025-06-02",
    "link": "http://arxiv.org/abs/2506.01783v2",
    "pdf": "http://arxiv.org/pdf/2506.01783v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "Face Anti-Spoofing (FAS) typically depends on a single visual modality when defending against presentation attacks such as print attacks, screen replays, and 3D masks, resulting in limited generalization across devices, environments, and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have recently achieved breakthroughs in image-text understanding and semantic reasoning, suggesting that integrating visual and linguistic co-inference into FAS can substantially improve both robustness and interpretability. However, the lack of a high-quality vision-language multimodal dataset has been a critical bottleneck. To address this, we introduce FaceCoT (Face Chain-of-Thought), the first large-scale Visual Question Answering (VQA) dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches model learning with high-quality CoT VQA annotations. Meanwhile, we develop a caption model refined via reinforcement learning to expand the dataset and enhance annotation quality. Furthermore, we introduce a CoT-Enhanced Progressive Learning (CEPL) strategy to better leverage the CoT data and boost model performance on FAS tasks. Extensive experiments demonstrate that models trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple benchmark datasets.",
    "summary_cn": "研究问题：人脸反欺骗（FAS）的泛化能力。方法：提出一种多模态方法。创新点：结合多种视觉模态提高泛化能力。结果：提高了FAS的鲁棒性。"
  },
  {
    "id": "2506.01085v2",
    "title": "Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection",
    "authors": [
      "Shivam Chandhok",
      "Qian Yang",
      "Oscar Manas",
      "Kanishk Jain",
      "Leonid Sigal",
      "Aishwarya Agrawal"
    ],
    "published": "2025-06-01",
    "link": "http://arxiv.org/abs/2506.01085v2",
    "pdf": "http://arxiv.org/pdf/2506.01085v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "summary": "Instruction tuning has been central to the success of recent vision-language models (VLMs), but it remains expensive-requiring large-scale datasets, high-quality annotations, and large compute budgets. We propose PRioritized cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data- and compute-efficient framework that enables VLMs to dynamically select what to learn next based on their evolving needs during training. At each stage, the model tracks its learning progress across skills and selects the most informative samples-those it has not already mastered and that are not too difficult to learn at the current stage of training. This strategy effectively controls skill acquisition and the order in which skills are learned. Specifically, we sample from skills showing the highest learning progress, prioritizing those with the most rapid improvement. Unlike prior methods, PROGRESS requires no upfront answer annotations, queries answers only on a need basis, avoids reliance on additional supervision from auxiliary VLMs, and does not require compute-heavy gradient computations for data selection. Experiments across multiple instruction-tuning datasets of varying scales demonstrate that PROGRESS consistently outperforms state-of-the-art baselines with much less data and supervision. Additionally, we show strong cross-architecture generalization and transferability to larger models, validating PROGRESS as a scalable solution for efficient learning.",
    "summary_cn": "研究问题：视觉语言模型（VLM）的指令微调成本高。方法：提出PROGRESS方法。创新点：通过相对误差驱动样本选择降低成本。结果：提高了指令微调的效率。"
  },
  {
    "id": "2505.22499v3",
    "title": "SABER: Spatially Consistent 3D Universal Adversarial Objects for BEV Detectors",
    "authors": [
      "Aixuan Li",
      "Mochu Xiang",
      "Bosen Hou",
      "Zhexiong Wan",
      "Jing Zhang",
      "Yuchao Dai"
    ],
    "published": "2025-05-28",
    "link": "http://arxiv.org/abs/2505.22499v3",
    "pdf": "http://arxiv.org/pdf/2505.22499v3",
    "categories": [
      "cs.CV"
    ],
    "summary": "Adversarial robustness of BEV 3D object detectors is critical for autonomous driving (AD). Existing invasive attacks require altering the target vehicle itself (e.g. attaching patches), making them unrealistic and impractical for real-world evaluation. While non-invasive attacks that place adversarial objects in the environment are more practical, current methods still lack the multi-view and temporal consistency needed for physically plausible threats. In this paper, we present the first framework for generating universal, non-invasive, and 3D-consistent adversarial objects that expose fundamental vulnerabilities for BEV 3D object detectors. Instead of modifying target vehicles, our method inserts rendered objects into scenes with an occlusion-aware module that enforces physical plausibility across views and time. To maintain attack effectiveness across views and frames, we optimize adversarial object appearance using a BEV spatial feature-guided optimization strategy that attacks the detector's internal representations. Extensive experiments demonstrate that our learned universal adversarial objects can consistently degrade multiple BEV detectors from various viewpoints and distances. More importantly, the new environment-manipulation attack paradigm exposes models' over-reliance on contextual cues and provides a practical pipeline for robustness evaluation in AD systems.",
    "summary_cn": "研究问题：BEV 3D目标检测器的对抗鲁棒性。方法：提出一种非侵入式攻击方法。创新点：无需修改目标车辆。结果：提高了检测器的鲁棒性。"
  },
  {
    "id": "2504.18594v2",
    "title": "RaPA: Enhancing Transferable Targeted Attacks via Random Parameter Pruning",
    "authors": [
      "Tongrui Su",
      "Qingbin Li",
      "Shengyu Zhu",
      "Wei Chen",
      "Xueqi Cheng"
    ],
    "published": "2025-04-24",
    "link": "http://arxiv.org/abs/2504.18594v2",
    "pdf": "http://arxiv.org/pdf/2504.18594v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "summary": "Compared to untargeted attacks, targeted transfer-based attack is still suffering from much lower Attack Success Rates (ASRs), although significant improvements have been achieved by kinds of methods, such as diversifying input, stabilizing the gradient, and re-training surrogate models. In this paper, we find that adversarial examples generated by existing methods rely heavily on a small subset of surrogate model parameters, which in turn limits their transferability to unseen target models. Inspired by this, we propose the Random Parameter Pruning Attack (RaPA), which introduces parameter-level randomization during the attack process. At each optimization step, RaPA randomly prunes model parameters to generate diverse yet semantically consistent surrogate variants.We show this parameter-level randomization is equivalent to adding an importance-equalization regularizer, thereby alleviating the over-reliance issue. Extensive experiments across both CNN and Transformer architectures demonstrate that RaPA substantially enhances transferability. In the challenging case of transferring from CNN-based to Transformer-based models, RaPA achieves up to 11.7% higher average ASRs than state-of-the-art baselines(with 33.3% ASRs), while being training-free, cross-architecture efficient, and easily integrated into existing attack frameworks. Code is available in https://github.com/molarsu/RaPA.",
    "summary_cn": "研究问题：针对转移攻击的攻击成功率低。方法：提出一种基于多样化输入、稳定梯度和重新训练代理模型的方法。创新点：提高攻击成功率。结果：提高了攻击成功率。"
  },
  {
    "id": "2504.11434v2",
    "title": "Enhancing Out-of-Distribution Detection with Extended Logit Normalization",
    "authors": [
      "Yifan Ding",
      "Xixi Liu",
      "Jonas Unger",
      "Gabriel Eilertsen"
    ],
    "published": "2025-04-15",
    "link": "http://arxiv.org/abs/2504.11434v2",
    "pdf": "http://arxiv.org/pdf/2504.11434v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "\\noindent Out-of-distribution (OOD) detection is essential for the safe deployment of machine learning models. Extensive work has focused on devising various scoring functions for detecting OOD samples, while only a few studies focus on training neural networks using certain model calibration objectives, which often lead to a compromise in predictive accuracy and support only limited choices of scoring functions. In this work, we first identify the feature collapse phenomena in Logit Normalization (LogitNorm), then propose a novel hyperparameter-free formulation that significantly benefits a wide range of post-hoc detection methods. To be specific, we devise a feature distance-awareness loss term in addition to LogitNorm, termed $\\textbf{ELogitNorm}$, which enables improved OOD detection and in-distribution (ID) confidence calibration. Extensive experiments across standard benchmarks demonstrate that our approach outperforms state-of-the-art training-time methods in OOD detection while maintaining strong ID classification accuracy. Our code is available on: https://github.com/limchaos/ElogitNorm.",
    "summary_cn": "研究问题：模型校准在分布外（OOD）检测中的应用。方法：提出一种基于模型校准的OOD检测方法。创新点：提高检测准确性。结果：提高了OOD检测的准确性。"
  },
  {
    "id": "2503.10981v4",
    "title": "CLIP-Free, Label Free, Unsupervised Concept Bottleneck Models",
    "authors": [
      "Fawaz Sammani",
      "Jonas Fischer",
      "Nikos Deligiannis"
    ],
    "published": "2025-03-14",
    "link": "http://arxiv.org/abs/2503.10981v4",
    "pdf": "http://arxiv.org/pdf/2503.10981v4",
    "categories": [
      "cs.CV"
    ],
    "summary": "Concept Bottleneck Models (CBMs) map dense feature representations into human-interpretable concepts which are then combined linearly to make a prediction. However, modern CBMs rely on the CLIP model to obtain image-concept annotations, and it remains unclear how to design CBMs without the CLIP bottleneck. Methods that do not use CLIP instead require manual, labor intensive annotation to associate feature representations with concepts. Furthermore, all CBMs necessitate training a linear classifier to map the extracted concepts to class labels. In this work, we lift all three limitations simultaneously by proposing a method that converts any frozen visual classifier into a CBM without requiring image-concept labels (label-free), without relying on the CLIP model (CLIP-free), and by deriving the linear classifier in an unsupervised manner. Our method is formulated by aligning the original classifier's distribution (over discrete class indices) with its corresponding vision-language counterpart distribution derived from textual class names, while preserving the classifier's performance. The approach requires no ground-truth image-class annotations, and is highly data-efficient and preserves the classifier's reasoning process. Applied and tested on over 40 visual classifiers, our resulting unsupervised, label-free and CLIP-free CBM (U-F$^2$-CBM) sets a new state of the art, surpassing even supervised CLIP-based CBMs. We also show that our method can be used for zero-shot image captioning, outperforming existing methods based on CLIP, and achieving state-of-art.",
    "summary_cn": "研究问题：如何设计不依赖CLIP模型的Concept Bottleneck Models (CBMs)。方法：提出一种新的CBMs设计方法。创新点：无需CLIP模型，直接从图像中提取概念。结果：实验表明，该方法在预测准确率上与依赖CLIP模型的CBMs相当。"
  },
  {
    "id": "2503.08049v3",
    "title": "SphOR: A Representation Learning Perspective on Open-set Recognition for Identifying Unknown Classes in Deep Learning Models",
    "authors": [
      "Nadarasar Bahavan",
      "Sachith Seneviratne",
      "Saman Halgamuge"
    ],
    "published": "2025-03-11",
    "link": "http://arxiv.org/abs/2503.08049v3",
    "pdf": "http://arxiv.org/pdf/2503.08049v3",
    "categories": [
      "cs.CV"
    ],
    "summary": "The reliance on Deep Neural Network (DNN)-based classifiers in safety-critical and real-world applications necessitates Open-Set Recognition (OSR). OSR enables the identification of input data from classes unknown during training as unknown, as opposed to misclassifying them as belonging to a known class. DNNs consist of a feature extraction backbone and classifier head; however, most OSR methods typically train both components jointly, often yielding feature representations that adapt poorly to unknown data. Other approaches employ off-the-shelf objectives, such as supervised contrastive learning, which are not specifically designed for OSR. To address these limitations, we propose SpHOR, which explicitly shapes the feature space via supervised representation learning, before training a classifier. Instead of relying on generic feature learning, SpHOR custom-designs representation learning for OSR through three key innovations: (1) enforcing discriminative class-specific features via orthogonal label embeddings, ensuring clearer separation between classes. (2) imposing a spherical constraint, modeling representations as a mixture of von Mises-Fisher distributions. (3) integrating Mixup and Label Smoothing (LS) directly into the representation learning stage. To quantify how these techniques enhance representations for OSR, we introduce two metrics: the Angular Separability (AS) and Norm Separability (NS). Combining all three innovations, SpHOR achieves state-of-the-art results (in AUROC and OSCR) across various coarse-grained and fine-grained open-set benchmarks, particularly excelling on the Semantic Shift Benchmark with improvements up to 5.1\\%. Code at https://github.com/nadarasarbahavan/SpHOR",
    "summary_cn": "研究问题：如何提高基于DNN分类器的Open-Set Recognition (OSR)能力。方法：提出一种新的OSR方法。创新点：通过识别未知类别数据，避免错误分类。结果：实验证明，该方法在OSR任务中具有显著优势。"
  },
  {
    "id": "2503.07853v2",
    "title": "Hier-COS: Making Deep Features Hierarchy-aware via Composition of Orthogonal Subspaces",
    "authors": [
      "Depanshu Sani",
      "Saket Anand"
    ],
    "published": "2025-03-10",
    "link": "http://arxiv.org/abs/2503.07853v2",
    "pdf": "http://arxiv.org/pdf/2503.07853v2",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "summary": "Traditional classifiers treat all labels as mutually independent, thereby considering all negative classes to be equally incorrect. This approach fails severely in many real-world scenarios, where a known semantic hierarchy defines a partial order of preferences over negative classes. While hierarchy-aware feature representations have shown promise in mitigating this problem, their performance is typically assessed using metrics like MS and AHD. In this paper, we highlight important shortcomings in existing hierarchical evaluation metrics, demonstrating that they are often incapable of measuring true hierarchical performance. Our analysis reveals that existing methods learn sub-optimal hierarchical representations, despite competitive MS and AHD scores. To counter these issues, we introduce Hier-COS, a novel framework for unified hierarchy-aware fine-grained and hierarchical multi-level classification. We show that Hier-COS is theoretically guaranteed to be consistent with the given hierarchy tree. Furthermore, our framework implicitly adapts the learning capacity for different classes based on their position within the hierarchy tree-a vital property absent in existing methods. Finally, to address the limitations of evaluation metrics, we propose HOPS, a ranking-based metric that demonstrably overcomes the deficiencies of current evaluation standards. We benchmark Hier-COS on four challenging datasets, including the deep and imbalanced tieredImageNet-H and iNaturalist-19. Through extensive experiments, we demonstrate that Hier-COS achieves SOTA across all hierarchical metrics for every dataset, while simultaneously beating the top-1 accuracy in all but one case. Lastly, we show that Hier-COS can effectively learn to transform the frozen features extracted from a pretrained backbone (ViT) to be hierarchy-aware, yielding substantial benefits for hierarchical classification performance.",
    "summary_cn": "研究问题：如何改进传统分类器在具有语义层次结构场景下的性能。方法：提出一种考虑语义层次结构的分类器。创新点：根据层次结构调整负类权重。结果：实验表明，该方法在多个场景中优于传统分类器。"
  },
  {
    "id": "2411.16758v2",
    "title": "Motion-Aware Animatable Gaussian Avatars Deblurring",
    "authors": [
      "Muyao Niu",
      "Yifan Zhan",
      "Qingtian Zhu",
      "Zhuoxiao Li",
      "Wei Wang",
      "Zhihang Zhong",
      "Xiao Sun",
      "Yinqiang Zheng"
    ],
    "published": "2024-11-24",
    "link": "http://arxiv.org/abs/2411.16758v2",
    "pdf": "http://arxiv.org/pdf/2411.16758v2",
    "categories": [
      "cs.CV"
    ],
    "summary": "The creation of 3D human avatars from multi-view videos is a significant yet challenging task in computer vision. However, existing techniques rely on high-quality, sharp images as input, which are often impractical to obtain in real-world scenarios due to variations in human motion speed and intensity. This paper introduces a novel method for directly reconstructing sharp 3D human Gaussian avatars from blurry videos. The proposed approach incorporates a 3D-aware, physics-based model of blur formation caused by human motion, together with a 3D human motion model designed to resolve ambiguities in motion-induced blur. This framework enables the joint optimization of the avatar representation and motion parameters from a coarse initialization. Comprehensive benchmarks are established using both a synthetic dataset and a real-world dataset captured with a 360-degree synchronous hybrid-exposure camera system. Extensive evaluations demonstrate the effectiveness and robustness of the model across diverse conditions.",
    "summary_cn": "研究问题：如何从多视角视频中创建3D人像。方法：提出一种基于多视角视频的3D人像生成方法。创新点：无需高质量图像，适应实际场景。结果：实验证明，该方法在3D人像生成任务中具有较高准确率。"
  }
]